














import requests
import uuid
import os
import tempfile
import hashlib
import dataclasses

import sentencepiece as spm
import functools
from sentencepiece import sentencepiece_model_pb2


@dataclasses.dataclass(frozen=True)
class _TokenizerConfig:
    model_url: str
    model_hash: str


_GEMMA_TOKENIZER = "google/gemma"


_GEMINI_MODEL_NAMES = ["gemini-1.0-pro", "gemini-1.5-pro", "gemini-1.5-flash"]
_GEMINI_STABLE_MODEL_NAMES = [
    "gemini-1.0-pro-001",
    "gemini-1.0-pro-002",
    "gemini-1.5-pro-001",
    "gemini-1.5-flash-001",
    "gemini-1.5-flash-002",
    "gemini-1.5-pro-002",
]

_TOKENIZERS = {
    _GEMMA_TOKENIZER: _TokenizerConfig(
        model_url="https://raw.githubusercontent.com/google/gemma_pytorch/33b652c465537c6158f9a472ea5700e5e770ad3f/tokenizer/tokenizer.model",
        model_hash="61a7b147390c64585d6c3543dd6fc636906c9af3865a5548f27f31aee1d4c8e2",
    )
}


def _load_file(file_url_path: str) -> bytes:
    
    resp = requests.get(file_url_path)
    resp.raise_for_status()
    return resp.content


def _is_valid_model(*, model_data: bytes, expected_hash: str) -> bool:
    
    if not expected_hash:
        raise ValueError("expected_hash is required")
    return hashlib.sha256(model_data).hexdigest() == expected_hash


def _maybe_remove_file(file_path: str) -> None:
    
    if not os.path.exists(file_path):
        return
    try:
        os.remove(file_path)
    except OSError:
        
        pass


def _maybe_load_from_cache(*, file_path: str, expected_hash: str) -> bytes:
    
    if not os.path.exists(file_path):
        return
    with open(file_path, "rb") as f:
        content = f.read()
    if _is_valid_model(model_data=content, expected_hash=expected_hash):
        return content

    
    _maybe_remove_file(file_path)


def _maybe_save_to_cache(*, cache_dir: str, cache_path: str, content: bytes) -> None:
    
    try:
        os.makedirs(cache_dir, exist_ok=True)
        tmp_path = cache_dir + "." + str(uuid.uuid4()) + ".tmp"
        with open(tmp_path, "wb") as f:
            f.write(content)
        os.rename(tmp_path, cache_path)
    except OSError:
        
        pass


def _load_from_url(*, file_url: str, expected_hash: str) -> bytes:
    
    content = _load_file(file_url)
    if not _is_valid_model(model_data=content, expected_hash=expected_hash):
        actual_hash = hashlib.sha256(content).hexdigest()
        raise ValueError(
            f"Downloaded model file is corrupted."
            f" Expected hash {expected_hash}. Got file hash {actual_hash}."
        )
    return content


def _load(*, file_url: str, expected_hash: str) -> bytes:
    
    model_dir = os.path.join(tempfile.gettempdir(), "vertexai_tokenizer_model")
    filename = hashlib.sha1(file_url.encode()).hexdigest()
    model_path = os.path.join(model_dir, filename)

    model_data = _maybe_load_from_cache(
        file_path=model_path, expected_hash=expected_hash
    )
    if not model_data:
        model_data = _load_from_url(file_url=file_url, expected_hash=expected_hash)

    _maybe_save_to_cache(cache_dir=model_dir, cache_path=model_path, content=model_data)
    return model_data


def _load_model_proto_bytes(tokenizer_name: str) -> bytes:
    
    if tokenizer_name not in _TOKENIZERS:
        raise ValueError(
            f"Tokenizer {tokenizer_name} is not supported."
            f"Supported tokenizers: {list(_TOKENIZERS.keys())}"
        )
    return _load(
        file_url=_TOKENIZERS[tokenizer_name].model_url,
        expected_hash=_TOKENIZERS[tokenizer_name].model_hash,
    )


@functools.lru_cache()
def load_model_proto(tokenizer_name) -> sentencepiece_model_pb2.ModelProto:
    
    model_proto = sentencepiece_model_pb2.ModelProto()
    model_proto.ParseFromString(_load_model_proto_bytes(tokenizer_name))
    return model_proto


def get_tokenizer_name(model_name: str):
    
    if model_name in _GEMINI_MODEL_NAMES:
        return _GEMMA_TOKENIZER
    if model_name in _GEMINI_STABLE_MODEL_NAMES:
        return _GEMMA_TOKENIZER
    raise ValueError(
        f"Model {model_name} is not supported. Supported models: {', '.join(_GEMINI_STABLE_MODEL_NAMES)}.\n"  
    )


@functools.lru_cache()
def get_sentencepiece(tokenizer_name: str) -> spm.SentencePieceProcessor:
    
    processor = spm.SentencePieceProcessor()
    processor.LoadFromSerializedProto(_load_model_proto_bytes(tokenizer_name))
    return processor
