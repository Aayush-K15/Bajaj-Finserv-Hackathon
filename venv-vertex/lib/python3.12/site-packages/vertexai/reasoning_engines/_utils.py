














import dataclasses
import inspect
import json
import types
import typing
from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Sequence, Union

import proto

from google.cloud.aiplatform import base
from google.api import httpbody_pb2
from google.protobuf import struct_pb2
from google.protobuf import json_format

try:
    
    
    
    import langchain_core.runnables.config

    RunnableConfig = langchain_core.runnables.config.RunnableConfig
except ImportError:
    RunnableConfig = Any

try:
    from llama_index.core.base.response import schema as llama_index_schema
    from llama_index.core.base.llms import types as llama_index_types

    LlamaIndexResponse = llama_index_schema.Response
    LlamaIndexBaseModel = llama_index_schema.BaseModel
    LlamaIndexChatResponse = llama_index_types.ChatResponse
except ImportError:
    LlamaIndexResponse = Any
    LlamaIndexBaseModel = Any
    LlamaIndexChatResponse = Any

JsonDict = Dict[str, Any]

_LOGGER = base.Logger(__name__)


def to_proto(
    obj: Union[JsonDict, proto.Message],
    message: Optional[proto.Message] = None,
) -> proto.Message:
    
    if message is None:
        message = struct_pb2.Struct()
    if isinstance(obj, (proto.Message, struct_pb2.Struct)):
        return obj
    try:
        json_format.ParseDict(obj, message._pb)
    except AttributeError:
        json_format.ParseDict(obj, message)
    return message


def to_dict(message: proto.Message) -> JsonDict:
    
    try:
        
        result: JsonDict = json.loads(json_format.MessageToJson(message._pb))
    except AttributeError:
        result: JsonDict = json.loads(json_format.MessageToJson(message))
    return result


def dataclass_to_dict(obj: dataclasses.dataclass) -> Any:
    
    return json.loads(json.dumps(dataclasses.asdict(obj)))


def _llama_index_response_to_dict(obj: LlamaIndexResponse) -> Any:
    response = {}
    if hasattr(obj, "response"):
        response["response"] = obj.response
    if hasattr(obj, "source_nodes"):
        response["source_nodes"] = [node.model_dump_json() for node in obj.source_nodes]
    if hasattr(obj, "metadata"):
        response["metadata"] = obj.metadata

    return json.loads(json.dumps(response))


def _llama_index_chat_response_to_dict(obj: LlamaIndexChatResponse) -> Any:
    return json.loads(obj.message.model_dump_json())


def _llama_index_base_model_to_dict(obj: LlamaIndexBaseModel) -> Any:
    return json.loads(obj.model_dump_json())


def to_json_serializable_llama_index_object(
    obj: Union[
        LlamaIndexResponse,
        LlamaIndexBaseModel,
        LlamaIndexChatResponse,
        Sequence[LlamaIndexBaseModel],
    ]
) -> Union[str, Dict[str, Any], Sequence[Union[str, Dict[str, Any]]]]:
    
    if isinstance(obj, LlamaIndexResponse):
        return _llama_index_response_to_dict(obj)
    if isinstance(obj, LlamaIndexChatResponse):
        return _llama_index_chat_response_to_dict(obj)
    if isinstance(obj, Sequence):
        seq_result = []
        for item in obj:
            if isinstance(item, LlamaIndexBaseModel):
                seq_result.append(_llama_index_base_model_to_dict(item))
                continue
            seq_result.append(str(item))
        return seq_result
    if isinstance(obj, LlamaIndexBaseModel):
        return _llama_index_base_model_to_dict(obj)
    return str(obj)


def yield_parsed_json(body: httpbody_pb2.HttpBody) -> Iterable[Any]:
    
    content_type = getattr(body, "content_type", None)
    data = getattr(body, "data", None)

    if content_type is None or data is None or "application/json" not in content_type:
        yield body
        return

    try:
        utf8_data = data.decode("utf-8")
    except Exception as e:
        _LOGGER.warning(f"Failed to decode data: {data}. Exception: {e}")
        yield body
        return

    if not utf8_data:
        yield None
        return

    
    for line in utf8_data.split("\n"):
        if line:
            try:
                line = json.loads(line)
            except Exception as e:
                _LOGGER.warning(f"failed to parse json: {line}. Exception: {e}")
            yield line


def generate_schema(
    f: Callable[..., Any],
    *,
    schema_name: Optional[str] = None,
    descriptions: Mapping[str, str] = {},
    required: Sequence[str] = [],
) -> JsonDict:
    
    pydantic = _import_pydantic_or_raise()
    defaults = dict(inspect.signature(f).parameters)
    fields_dict = {
        name: (
            
            
            (param.annotation if param.annotation != inspect.Parameter.empty else Any),
            pydantic.Field(
                
                
                
                
                
                
                description=descriptions.get(name, None),
            ),
        )
        for name, param in defaults.items()
        
        if param.kind
        in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
            inspect.Parameter.POSITIONAL_ONLY,
        )
    }
    parameters = pydantic.create_model(f.__name__, **fields_dict).schema()
    
    
    
    
    parameters.pop("title", "")
    for name, function_arg in parameters.get("properties", {}).items():
        function_arg.pop("title", "")
        annotation = defaults[name].annotation
        
        
        
        
        if typing.get_origin(annotation) is Union and type(None) in typing.get_args(
            annotation
        ):
            
            
            
            
            for schema in function_arg.pop("anyOf", []):
                schema_type = schema.get("type")
                if schema_type and schema_type != "null":
                    function_arg["type"] = schema_type
                    break
            function_arg["nullable"] = True
    
    if required:
        
        parameters["required"] = required
    else:
        
        parameters["required"] = [
            k
            for k in defaults
            if (
                defaults[k].default == inspect.Parameter.empty
                and defaults[k].kind
                in (
                    inspect.Parameter.POSITIONAL_OR_KEYWORD,
                    inspect.Parameter.KEYWORD_ONLY,
                    inspect.Parameter.POSITIONAL_ONLY,
                )
            )
        ]
    schema = dict(name=f.__name__, description=f.__doc__, parameters=parameters)
    if schema_name:
        schema["name"] = schema_name
    return schema


def is_noop_or_proxy_tracer_provider(tracer_provider) -> bool:
    
    opentelemetry = _import_opentelemetry_or_warn()
    ProxyTracerProvider = opentelemetry.trace.ProxyTracerProvider
    NoOpTracerProvider = opentelemetry.trace.NoOpTracerProvider
    return isinstance(tracer_provider, (NoOpTracerProvider, ProxyTracerProvider))


def _import_cloud_storage_or_raise() -> types.ModuleType:
    
    try:
        from google.cloud import storage
    except ImportError as e:
        raise ImportError(
            "Cloud Storage is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        ) from e
    return storage


def _import_cloudpickle_or_raise() -> types.ModuleType:
    
    try:
        import cloudpickle  
    except ImportError as e:
        raise ImportError(
            "cloudpickle is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        ) from e
    return cloudpickle


def _import_pydantic_or_raise() -> types.ModuleType:
    
    try:
        import pydantic

        _ = pydantic.Field
    except AttributeError:
        from pydantic import v1 as pydantic
    except ImportError as e:
        raise ImportError(
            "pydantic is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        ) from e
    return pydantic


def _import_opentelemetry_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import opentelemetry  

        return opentelemetry
    except ImportError:
        _LOGGER.warning(
            "opentelemetry-sdk is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        )
    return None


def _import_opentelemetry_sdk_trace_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import opentelemetry.sdk.trace  

        return opentelemetry.sdk.trace
    except ImportError:
        _LOGGER.warning(
            "opentelemetry-sdk is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        )
    return None


def _import_cloud_trace_v2_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import google.cloud.trace_v2

        return google.cloud.trace_v2
    except ImportError:
        _LOGGER.warning(
            "google-cloud-trace is not installed. Please call "
            "'pip install google-cloud-aiplatform[agent_engines]'."
        )
    return None


def _import_cloud_trace_exporter_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import opentelemetry.exporter.cloud_trace  

        return opentelemetry.exporter.cloud_trace
    except ImportError:
        _LOGGER.warning(
            "opentelemetry-exporter-gcp-trace is not installed. Please "
            "call 'pip install google-cloud-aiplatform[langchain]'."
        )
    return None


def _import_openinference_langchain_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import openinference.instrumentation.langchain  

        return openinference.instrumentation.langchain
    except ImportError:
        _LOGGER.warning(
            "openinference-instrumentation-langchain is not installed. Please "
            "call 'pip install google-cloud-aiplatform[langchain]'."
        )
    return None


def _import_openinference_autogen_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import openinference.instrumentation.autogen  

        return openinference.instrumentation.autogen
    except ImportError:
        _LOGGER.warning(
            "openinference-instrumentation-autogen is not installed. Please "
            "call 'pip install openinference-instrumentation-autogen'."
        )
    return None


def _import_openinference_llama_index_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import openinference.instrumentation.llama_index  

        return openinference.instrumentation.llama_index
    except ImportError:
        _LOGGER.warning(
            "openinference-instrumentation-llama_index is not installed. Please "
            "call 'pip install google-cloud-aiplatform[llama_index]'."
        )
    return None


def _import_autogen_tools_or_warn() -> Optional[types.ModuleType]:
    
    try:
        from autogen import tools

        return tools
    except ImportError:
        _LOGGER.warning(
            "autogen.tools is not installed. Please call: `pip install ag2[tools]`"
        )
    return None


def _import_nest_asyncio_or_warn() -> Optional[types.ModuleType]:
    
    try:
        import nest_asyncio

        return nest_asyncio
    except ImportError:
        _LOGGER.warning(
            "nest_asyncio is not installed. Please call: `pip install nest-asyncio`"
        )
    return None
