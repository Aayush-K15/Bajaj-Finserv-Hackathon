
















import base64
import dataclasses
import hashlib
import io
import json
import pathlib
import typing
from typing import Any, Dict, List, Literal, Optional, Union
import urllib

from google.cloud import storage

from google.cloud.aiplatform import initializer as aiplatform_initializer
from vertexai._model_garden import _model_garden_models
from vertexai._utils import warning_logs


try:
    from IPython import display as IPython_display
except ImportError:
    IPython_display = None

try:
    from PIL import Image as PIL_Image
except ImportError:
    PIL_Image = None


_SUPPORTED_UPSCALING_SIZES = [2048, 4096]


@dataclasses.dataclass
class MaskImageConfig:
    

    mask_mode: Literal[
        "MASK_MODE_DEFAULT",
        "MASK_MODE_USER_PROVIDED",
        "MASK_MODE_BACKGROUND",
        "MASK_MODE_FOREGROUND",
        "MASK_MODE_SEMANTIC",
    ]
    segmentation_classes: Optional[List[int]] = None
    dilation: Optional[float] = None


@dataclasses.dataclass
class ControlImageConfig:
    

    control_type: Literal[
        "CONTROL_TYPE_DEFAULT",
        "CONTROL_TYPE_SCRIBBLE",
        "CONTROL_TYPE_FACE_MESH",
        "CONTROL_TYPE_CANNY",
    ]
    enable_control_image_computation: Optional[bool] = False


@dataclasses.dataclass
class StyleImageConfig:
    

    style_description: str


@dataclasses.dataclass
class SubjectImageConfig:
    

    subject_description: str
    subject_type: Literal[
        "SUBJECT_TYPE_DEFAULT",
        "SUBJECT_TYPE_PERSON",
        "SUBJECT_TYPE_ANIMAL",
        "SUBJECT_TYPE_PRODUCT",
    ]


class Image:
    

    __module__ = "vertexai.vision_models"

    _loaded_bytes: Optional[bytes] = None
    _loaded_image: Optional["PIL_Image.Image"] = None
    _gcs_uri: Optional[str] = None

    def __init__(
        self,
        image_bytes: Optional[bytes] = None,
        gcs_uri: Optional[str] = None,
    ):
        
        warning_logs.show_deprecation_warning()
        if bool(image_bytes) == bool(gcs_uri):
            raise ValueError("Either image_bytes or gcs_uri must be provided.")

        self._image_bytes = image_bytes
        self._gcs_uri = gcs_uri

    @staticmethod
    def load_from_file(location: str) -> "Image":
        
        parsed_url = urllib.parse.urlparse(location)
        if (
            parsed_url.scheme == "https"
            and parsed_url.netloc == "storage.googleapis.com"
        ):
            parsed_url = parsed_url._replace(
                scheme="gs", netloc="", path=f"/{urllib.parse.unquote(parsed_url.path)}"
            )
            location = urllib.parse.urlunparse(parsed_url)

        if parsed_url.scheme == "gs":
            return Image(gcs_uri=location)

        
        image_bytes = pathlib.Path(location).read_bytes()
        image = Image(image_bytes=image_bytes)
        return image

    @property
    def _blob(self) -> storage.Blob:
        if self._gcs_uri is None:
            raise AttributeError("_blob is only supported when gcs_uri is set.")
        storage_client = storage.Client(
            credentials=aiplatform_initializer.global_config.credentials
        )
        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)
        
        blob.reload()
        return blob

    @property
    def _image_bytes(self) -> bytes:
        if self._loaded_bytes is None:
            self._loaded_bytes = self._blob.download_as_bytes()
        return self._loaded_bytes

    @_image_bytes.setter
    def _image_bytes(self, value: bytes):
        self._loaded_bytes = value

    @property
    def _pil_image(self) -> "PIL_Image.Image":
        if self._loaded_image is None:
            if not PIL_Image:
                raise RuntimeError(
                    "The PIL module is not available. Please install the Pillow package."
                )
            self._loaded_image = PIL_Image.open(io.BytesIO(self._image_bytes))
        return self._loaded_image

    @property
    def _size(self):
        return self._pil_image.size

    @property
    def _mime_type(self) -> str:
        
        if self._gcs_uri:
            return self._blob.content_type
        if PIL_Image:
            return PIL_Image.MIME.get(self._pil_image.format, "image/jpeg")
        
        return "image/jpeg"

    def show(self):
        
        if PIL_Image and IPython_display:
            IPython_display.display(self._pil_image)

    def save(self, location: str):
        
        pathlib.Path(location).write_bytes(self._image_bytes)

    def _as_base64_string(self) -> str:
        
        
        
        
        return base64.b64encode(self._image_bytes).decode("ascii")


class ReferenceImage:
    

    __module__ = "vertexai.vision_models"
    reference_image: Optional[Image] = None
    reference_id: int
    config: Optional[
        Union[MaskImageConfig, ControlImageConfig, StyleImageConfig, SubjectImageConfig]
    ] = None
    reference_type: Optional[str] = None

    def __init__(
        self,
        reference_id,
        image: Optional[Union[bytes, Image, str]] = None,
    ):
        
        if image is not None:
            if isinstance(image, Image):
                self.reference_image = image
            elif isinstance(image, bytes):
                self.reference_image = Image(image_bytes=image)
            elif isinstance(image, str):
                self.reference_image = Image(gcs_uri=image)
            else:
                raise ValueError("Image must be either Image object, bytes or gcs_uri.")
        self.reference_id = reference_id


class RawReferenceImage(ReferenceImage):
    

    reference_type = "REFERENCE_TYPE_RAW"


class MaskReferenceImage(ReferenceImage):
    

    mask_mode_enum_map = {
        "default": "MASK_MODE_DEFAULT",
        "user_provided": "MASK_MODE_USER_PROVIDED",
        "background": "MASK_MODE_BACKGROUND",
        "foreground": "MASK_MODE_FOREGROUND",
        "semantic": "MASK_MODE_SEMANTIC",
    }
    reference_type = "REFERENCE_TYPE_MASK"

    def __init__(
        self,
        reference_id,
        image: Optional[Union[bytes, Image, str]] = None,
        mask_mode: Optional[
            Literal["default", "user_provided", "background", "foreground", "semantic"]
        ] = None,
        dilation: Optional[float] = None,
        segmentation_classes: Optional[List[int]] = None,
    ):
        
        self.config = MaskImageConfig(
            mask_mode=self.mask_mode_enum_map[mask_mode]
            if mask_mode in self.mask_mode_enum_map
            else "MASK_MODE_DEFAULT",
            dilation=dilation,
            segmentation_classes=segmentation_classes,
        )
        super().__init__(reference_id, image)


class ControlReferenceImage(ReferenceImage):
    

    control_type_enum_map = {
        "default": "CONTROL_TYPE_DEFAULT",
        "scribble": "CONTROL_TYPE_SCRIBBLE",
        "face_mesh": "CONTROL_TYPE_FACE_MESH",
        "canny": "CONTROL_TYPE_CANNY",
    }
    reference_type = "REFERENCE_TYPE_CONTROL"

    def __init__(
        self,
        reference_id,
        image: Optional[Union[bytes, Image, str]] = None,
        control_type: Optional[
            Literal["default", "scribble", "face_mesh", "canny"]
        ] = None,
        enable_control_image_computation: Optional[bool] = False,
    ):
        
        super().__init__(reference_id, image)
        self.config = ControlImageConfig(
            control_type=self.control_type_enum_map[control_type]
            if control_type in self.control_type_enum_map
            else "CONTROL_TYPE_DEFAULT",
            enable_control_image_computation=enable_control_image_computation,
        )


class StyleReferenceImage(ReferenceImage):
    

    reference_type = "REFERENCE_TYPE_STYLE"

    def __init__(
        self,
        reference_id,
        image: Optional[Union[bytes, Image, str]] = None,
        style_description: Optional[str] = None,
    ):
        
        super().__init__(reference_id, image)
        self.config = StyleImageConfig(style_description=style_description)


class SubjectReferenceImage(ReferenceImage):
    

    subject_type_enum_map = {
        "default": "SUBJECT_TYPE_DEFAULT",
        "person": "SUBJECT_TYPE_PERSON",
        "animal": "SUBJECT_TYPE_ANIMAL",
        "product": "SUBJECT_TYPE_PRODUCT",
    }
    reference_type = "REFERENCE_TYPE_SUBJECT"

    def __init__(
        self,
        reference_id,
        image: Optional[Union[bytes, Image, str]] = None,
        subject_description: Optional[str] = None,
        subject_type: Optional[
            Literal["default", "person", "animal", "product"]
        ] = None,
    ):
        
        super().__init__(reference_id, image)
        self.config = SubjectImageConfig(
            subject_description=subject_description,
            subject_type=self.subject_type_enum_map[subject_type]
            if subject_type in self.subject_type_enum_map
            else "SUBJECT_TYPE_DEFAULT",
        )


class Video:
    

    __module__ = "vertexai.vision_models"

    _loaded_bytes: Optional[bytes] = None
    _gcs_uri: Optional[str] = None

    def __init__(
        self,
        video_bytes: Optional[bytes] = None,
        gcs_uri: Optional[str] = None,
    ):
        
        warning_logs.show_deprecation_warning()
        if bool(video_bytes) == bool(gcs_uri):
            raise ValueError("Either video_bytes or gcs_uri must be provided.")

        self._video_bytes = video_bytes
        self._gcs_uri = gcs_uri

    @staticmethod
    def load_from_file(location: str) -> "Video":
        
        parsed_url = urllib.parse.urlparse(location)
        if (
            parsed_url.scheme == "https"
            and parsed_url.netloc == "storage.googleapis.com"
        ):
            parsed_url = parsed_url._replace(
                scheme="gs", netloc="", path=f"/{urllib.parse.unquote(parsed_url.path)}"
            )
            location = urllib.parse.urlunparse(parsed_url)

        if parsed_url.scheme == "gs":
            return Video(gcs_uri=location)

        
        video_bytes = pathlib.Path(location).read_bytes()
        video = Video(video_bytes=video_bytes)
        return video

    @property
    def _blob(self) -> storage.Blob:
        if self._gcs_uri is None:
            raise AttributeError("_blob is only supported when gcs_uri is set.")
        storage_client = storage.Client(
            credentials=aiplatform_initializer.global_config.credentials
        )
        blob = storage.Blob.from_string(uri=self._gcs_uri, client=storage_client)
        
        blob.reload()
        return blob

    @property
    def _video_bytes(self) -> bytes:
        if self._loaded_bytes is None:
            self._loaded_bytes = self._blob.download_as_bytes()
        return self._loaded_bytes

    @_video_bytes.setter
    def _video_bytes(self, value: bytes):
        self._loaded_bytes = value

    @property
    def _mime_type(self) -> str:
        
        if self._gcs_uri:
            return self._blob.content_type
        
        return "video/mp4"

    def save(self, location: str):
        
        pathlib.Path(location).write_bytes(self._video_bytes)

    def _as_base64_string(self) -> str:
        
        
        
        
        return base64.b64encode(self._video_bytes).decode("ascii")


class VideoSegmentConfig:
    

    __module__ = "vertexai.vision_models"

    start_offset_sec: int
    end_offset_sec: int
    interval_sec: int

    def __init__(
        self,
        start_offset_sec: int = 0,
        end_offset_sec: int = 120,
        interval_sec: int = 16,
    ):
        
        warning_logs.show_deprecation_warning()
        self.start_offset_sec = start_offset_sec
        self.end_offset_sec = end_offset_sec
        self.interval_sec = interval_sec


class VideoEmbedding:
    

    __module__ = "vertexai.vision_models"

    start_offset_sec: int
    end_offset_sec: int
    embedding: List[float]

    def __init__(
        self, start_offset_sec: int, end_offset_sec: int, embedding: List[float]
    ):
        
        warning_logs.show_deprecation_warning()
        self.start_offset_sec = start_offset_sec
        self.end_offset_sec = end_offset_sec
        self.embedding = embedding


class ImageGenerationModel(
    _model_garden_models._ModelGardenModel  
):
    

    __module__ = "vertexai.preview.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/vision_generative_model_1.0.0.yaml"

    def _generate_images(
        self,
        prompt: str,
        *,
        negative_prompt: Optional[str] = None,
        number_of_images: int = 1,
        width: Optional[int] = None,
        height: Optional[int] = None,
        aspect_ratio: Optional[Literal["1:1", "9:16", "16:9", "4:3", "3:4"]] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        base_image: Optional["Image"] = None,
        mask: Optional["Image"] = None,
        reference_images: Optional[List["ReferenceImage"]] = None,
        edit_mode: Optional[
            Literal[
                "inpainting-insert",
                "inpainting-remove",
                "outpainting",
                "product-image",
                "background-swap",
                "default",
            ]
        ] = None,
        mask_mode: Optional[Literal["background", "foreground", "semantic"]] = None,
        segmentation_classes: Optional[List[str]] = None,
        mask_dilation: Optional[float] = None,
        product_position: Optional[Literal["fixed", "reposition"]] = None,
        output_mime_type: Optional[Literal["image/png", "image/jpeg"]] = None,
        compression_quality: Optional[float] = None,
        language: Optional[str] = None,
        output_gcs_uri: Optional[str] = None,
        add_watermark: Optional[bool] = None,
        safety_filter_level: Optional[
            Literal[
                "block_most",
                "block_some",
                "block_few",
                "block_fewest",
                "block_low_and_above",
                "block_medium_and_above",
                "block_only_high",
                "block_none",
            ]
        ] = None,
        person_generation: Optional[
            Literal["dont_allow", "allow_adult", "allow_all"]
        ] = None,
    ) -> "ImageGenerationResponse":
        
        
        instance = {"prompt": prompt}
        shared_generation_parameters = {
            "prompt": prompt,
            
            
            
            "number_of_images_in_batch": number_of_images,
        }

        if base_image:
            if base_image._gcs_uri:  
                instance["image"] = {
                    "gcsUri": base_image._gcs_uri  
                }
                shared_generation_parameters[
                    "base_image_uri"
                ] = base_image._gcs_uri  
            else:
                instance["image"] = {
                    "bytesBase64Encoded": base_image._as_base64_string()  
                }
                shared_generation_parameters["base_image_hash"] = hashlib.sha1(
                    base_image._image_bytes  
                ).hexdigest()

        if mask:
            if mask._gcs_uri:  
                instance["mask"] = {
                    "image": {
                        "gcsUri": mask._gcs_uri  
                    },
                }
                shared_generation_parameters[
                    "mask_uri"
                ] = mask._gcs_uri  
            else:
                instance["mask"] = {
                    "image": {
                        "bytesBase64Encoded": mask._as_base64_string()  
                    },
                }
                shared_generation_parameters["mask_hash"] = hashlib.sha1(
                    mask._image_bytes  
                ).hexdigest()

        if reference_images:
            instance["referenceImages"] = []
            for reference_image in reference_images:
                reference_image_instance = {}
                if not reference_image.reference_image:
                    if reference_image.reference_type != "REFERENCE_TYPE_MASK":
                        raise ValueError(
                            "Reference image must have an image or a gcs uri."
                        )
                else:
                    reference_image_instance["referenceImage"] = {}
                    if (
                        reference_image.reference_image._gcs_uri
                    ):  
                        reference_image_instance["referenceImage"] = {
                            "gcsUri": reference_image.reference_image._gcs_uri  
                        }
                        shared_generation_parameters[
                            f"reference_image_uri_{reference_image.reference_id}"
                        ] = (
                            reference_image.reference_image._gcs_uri
                        )  
                    elif reference_image.reference_image._image_bytes:
                        reference_image_instance["referenceImage"] = {
                            "bytesBase64Encoded": reference_image.reference_image._as_base64_string()  
                        }
                        shared_generation_parameters[
                            f"reference_image_hash_{reference_image.reference_id}"
                        ] = hashlib.sha1(
                            reference_image.reference_image._image_bytes  
                        ).hexdigest()

                reference_image_instance[
                    "referenceId"
                ] = reference_image.reference_id  
                reference_image_instance[
                    "referenceType"
                ] = reference_image.reference_type  
                shared_generation_parameters[
                    f"reference_type_{reference_image.reference_id}"
                ] = reference_image.reference_type  
                if isinstance(reference_image.config, MaskImageConfig):
                    reference_image_instance["maskImageConfig"] = {
                        "maskMode": reference_image.config.mask_mode,
                    }
                    if reference_image.config.dilation:
                        reference_image_instance["maskImageConfig"][
                            "dilation"
                        ] = reference_image.config.dilation
                    if reference_image.config.segmentation_classes:
                        reference_image_instance["maskImageConfig"][
                            "maskClasses"
                        ] = reference_image.config.segmentation_classes
                    shared_generation_parameters[
                        f"reference_image_mask_config_{reference_image.reference_id}"
                    ] = str(
                        reference_image.config
                    )  
                if isinstance(reference_image.config, ControlImageConfig):
                    reference_image_instance["controlImageConfig"] = {
                        "controlType": reference_image.config.control_type,
                        "enableControlImageComputation": (
                            reference_image.config.enable_control_image_computation
                        ),
                    }
                    shared_generation_parameters[
                        f"reference_image_control_config_{reference_image.reference_id}"
                    ] = str(
                        reference_image.config
                    )  
                if isinstance(reference_image.config, SubjectImageConfig):
                    reference_image_instance["subjectImageConfig"] = {
                        "subjectType": reference_image.config.subject_type,
                        "subjectDescription": reference_image.config.subject_description,
                    }
                    shared_generation_parameters[
                        f"reference_image_subject_config_{reference_image.reference_id}"
                    ] = str(
                        reference_image.config
                    )  
                if isinstance(reference_image.config, StyleImageConfig):
                    reference_image_instance["styleImageConfig"] = {
                        "styleDescription": reference_image.config.style_description,
                    }
                    shared_generation_parameters[
                        f"reference_image_style_config_{reference_image.reference_id}"
                    ] = str(
                        reference_image.config
                    )  
                instance["referenceImages"].append(reference_image_instance)

        edit_config = {}
        output_options = {}
        parameters = {}
        max_size = max(width or 0, height or 0) or None
        if aspect_ratio is not None:
            parameters["aspectRatio"] = aspect_ratio
        elif max_size:
            
            parameters["sampleImageSize"] = str(max_size)
            if height is not None and width is not None and height != width:
                parameters["aspectRatio"] = f"{width}:{height}"

        parameters["sampleCount"] = number_of_images
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
            shared_generation_parameters["negative_prompt"] = negative_prompt

        if seed is not None:
            
            parameters["seed"] = seed
            shared_generation_parameters["seed"] = seed

        if guidance_scale is not None:
            parameters["guidanceScale"] = guidance_scale
            shared_generation_parameters["guidance_scale"] = guidance_scale

        if language is not None:
            parameters["language"] = language
            shared_generation_parameters["language"] = language

        if output_gcs_uri is not None:
            parameters["storageUri"] = output_gcs_uri
            shared_generation_parameters["storage_uri"] = output_gcs_uri

        if edit_mode is not None:
            if reference_images is not None:
                edit_mode_to_enum_map = {
                    "inpainting-insert": "EDIT_MODE_INPAINT_INSERTION",
                    "inpainting-remove": "EDIT_MODE_INPAINT_REMOVAL",
                    "outpainting": "EDIT_MODE_OUTPAINT",
                    "background-swap": "EDIT_MODE_BGSWAP",
                }
                capability_mode = (
                    edit_mode_to_enum_map[edit_mode]
                    if edit_mode in edit_mode_to_enum_map
                    else "EDIT_MODE_DEFAULT"
                )
                parameters["editMode"] = capability_mode
                shared_generation_parameters["edit_mode"] = capability_mode
            else:
                edit_config["editMode"] = (
                    edit_mode if edit_mode != "background-swap" else "inpainting-insert"
                )
                shared_generation_parameters["edit_mode"] = edit_mode

        if mask is None and edit_mode is not None and edit_mode != "product-image":
            if mask_mode is not None:
                if "maskMode" not in edit_config:
                    edit_config["maskMode"] = {}
                edit_config["maskMode"]["maskType"] = mask_mode
                shared_generation_parameters["mask_mode"] = mask_mode

            if segmentation_classes is not None:
                if "maskMode" not in edit_config:
                    edit_config["maskMode"] = {}
                edit_config["maskMode"]["classes"] = segmentation_classes
                shared_generation_parameters["classes"] = segmentation_classes

        if mask_dilation is not None:
            edit_config["maskDilation"] = mask_dilation
            shared_generation_parameters["mask_dilation"] = mask_dilation

        if product_position is not None:
            edit_config["productPosition"] = product_position
            shared_generation_parameters["product_position"] = product_position

        if output_mime_type is not None:
            output_options["mimeType"] = output_mime_type
            shared_generation_parameters["mime_type"] = output_mime_type

        if compression_quality is not None:
            output_options["compressionQuality"] = compression_quality
            shared_generation_parameters["compression_quality"] = compression_quality

        if add_watermark is not None:
            parameters["addWatermark"] = add_watermark
            shared_generation_parameters["add_watermark"] = add_watermark

        if safety_filter_level is not None:
            parameters["safetySetting"] = safety_filter_level
            shared_generation_parameters["safety_filter_level"] = safety_filter_level

        if person_generation is not None:
            parameters["personGeneration"] = person_generation
            shared_generation_parameters["person_generation"] = person_generation

        if edit_config:
            parameters["editConfig"] = edit_config

        if output_options:
            parameters["outputOptions"] = output_options

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )

        generated_images: List["GeneratedImage"] = []
        for idx, prediction in enumerate(response.predictions):
            generation_parameters = dict(shared_generation_parameters)
            generation_parameters["index_of_image_in_batch"] = idx
            encoded_bytes = prediction.get("bytesBase64Encoded")
            generated_image = GeneratedImage(
                image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,
                generation_parameters=generation_parameters,
                gcs_uri=prediction.get("gcsUri"),
            )
            generated_images.append(generated_image)

        return ImageGenerationResponse(images=generated_images)

    def generate_images(
        self,
        prompt: str,
        *,
        negative_prompt: Optional[str] = None,
        number_of_images: int = 1,
        aspect_ratio: Optional[Literal["1:1", "9:16", "16:9", "4:3", "3:4"]] = None,
        guidance_scale: Optional[float] = None,
        language: Optional[str] = None,
        seed: Optional[int] = None,
        output_gcs_uri: Optional[str] = None,
        add_watermark: Optional[bool] = True,
        safety_filter_level: Optional[
            Literal["block_most", "block_some", "block_few", "block_fewest"]
        ] = None,
        person_generation: Optional[
            Literal["dont_allow", "allow_adult", "allow_all"]
        ] = None,
    ) -> "ImageGenerationResponse":
        
        return self._generate_images(
            prompt=prompt,
            negative_prompt=negative_prompt,
            number_of_images=number_of_images,
            aspect_ratio=aspect_ratio,
            guidance_scale=guidance_scale,
            language=language,
            seed=seed,
            output_gcs_uri=output_gcs_uri,
            add_watermark=add_watermark,
            safety_filter_level=safety_filter_level,
            person_generation=person_generation,
        )

    def edit_image(
        self,
        *,
        prompt: str,
        base_image: Optional["Image"] = None,
        mask: Optional["Image"] = None,
        reference_images: Optional[List["ReferenceImage"]] = None,
        negative_prompt: Optional[str] = None,
        number_of_images: int = 1,
        guidance_scale: Optional[float] = None,
        edit_mode: Optional[
            Literal[
                "inpainting-insert",
                "inpainting-remove",
                "outpainting",
                "product-image",
            ]
        ] = None,
        mask_mode: Optional[Literal["background", "foreground", "semantic"]] = None,
        segmentation_classes: Optional[List[str]] = None,
        mask_dilation: Optional[float] = None,
        product_position: Optional[Literal["fixed", "reposition"]] = None,
        output_mime_type: Optional[Literal["image/png", "image/jpeg"]] = None,
        compression_quality: Optional[float] = None,
        language: Optional[str] = None,
        seed: Optional[int] = None,
        output_gcs_uri: Optional[str] = None,
        safety_filter_level: Optional[
            Literal["block_most", "block_some", "block_few", "block_fewest"]
        ] = None,
        person_generation: Optional[
            Literal["dont_allow", "allow_adult", "allow_all"]
        ] = None,
    ) -> "ImageGenerationResponse":
        
        return self._generate_images(
            prompt=prompt,
            negative_prompt=negative_prompt,
            number_of_images=number_of_images,
            guidance_scale=guidance_scale,
            seed=seed,
            base_image=base_image,
            mask=mask,
            reference_images=reference_images,
            edit_mode=edit_mode,
            mask_mode=mask_mode,
            segmentation_classes=segmentation_classes,
            mask_dilation=mask_dilation,
            product_position=product_position,
            output_mime_type=output_mime_type,
            compression_quality=compression_quality,
            language=language,
            output_gcs_uri=output_gcs_uri,
            add_watermark=False,  
            safety_filter_level=safety_filter_level,
            person_generation=person_generation,
        )

    def upscale_image(
        self,
        image: Union["Image", "GeneratedImage"],
        new_size: Optional[int] = 2048,
        upscale_factor: Optional[Literal["x2", "x4"]] = None,
        output_mime_type: Optional[Literal["image/png", "image/jpeg"]] = "image/png",
        output_compression_quality: Optional[int] = None,
        output_gcs_uri: Optional[str] = None,
    ) -> "Image":
        
        target_image_size = new_size if new_size else None
        longest_dim = max(image._size[0], image._size[1])

        if not new_size and not upscale_factor:
            raise ValueError("Either new_size or upscale_factor must be provided.")

        if not upscale_factor:
            x2_factor = 2.0
            x4_factor = 4.0
            epsilon = 0.1
            is_upscaling_x2_request = abs(new_size / longest_dim - x2_factor) < epsilon
            is_upscaling_x4_request = abs(new_size / longest_dim - x4_factor) < epsilon

            if not is_upscaling_x2_request and not is_upscaling_x4_request:
                raise ValueError(
                    "Only x2 and x4 upscaling are currently supported. Requested"
                    f" upscaling factor: {new_size / longest_dim}"
                )
        else:
            if upscale_factor == "x2":
                target_image_size = longest_dim * 2
            else:
                target_image_size = longest_dim * 4
        if new_size not in _SUPPORTED_UPSCALING_SIZES:
            raise ValueError(
                "Only the folowing square upscaling sizes are currently supported:"
                f" {_SUPPORTED_UPSCALING_SIZES}."
            )

        instance = {"prompt": ""}

        if image._gcs_uri:  
            instance["image"] = {
                "gcsUri": image._gcs_uri  
            }
        else:
            instance["image"] = {
                "bytesBase64Encoded": image._as_base64_string()  
            }

        parameters = {
            "sampleCount": 1,
            "mode": "upscale",
        }

        if upscale_factor:
            parameters["upscaleConfig"] = {"upscaleFactor": upscale_factor}

        else:
            parameters["sampleImageSize"] = str(new_size)

        if output_gcs_uri is not None:
            parameters["storageUri"] = output_gcs_uri

        parameters["outputOptions"] = {"mimeType": output_mime_type}
        if output_mime_type == "image/jpeg" and output_compression_quality is not None:
            parameters["outputOptions"][
                "compressionQuality"
            ] = output_compression_quality

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )

        upscaled_image = response.predictions[0]

        if isinstance(image, GeneratedImage):
            generation_parameters = image.generation_parameters

        else:
            generation_parameters = {}

        generation_parameters["upscaled_image_size"] = target_image_size

        encoded_bytes = upscaled_image.get("bytesBase64Encoded")
        return GeneratedImage(
            image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,
            generation_parameters=generation_parameters,
            gcs_uri=upscaled_image.get("gcsUri"),
        )


@dataclasses.dataclass
class ImageGenerationResponse:
    

    __module__ = "vertexai.preview.vision_models"

    images: List["GeneratedImage"]

    def __iter__(self) -> typing.Iterator["GeneratedImage"]:
        
        yield from self.images

    def __getitem__(self, idx: int) -> "GeneratedImage":
        
        return self.images[idx]


_EXIF_USER_COMMENT_TAG_IDX = 0x9286
_IMAGE_GENERATION_PARAMETERS_EXIF_KEY = (
    "google.cloud.vertexai.image_generation.image_generation_parameters"
)


class GeneratedImage(Image):
    

    __module__ = "vertexai.preview.vision_models"

    def __init__(
        self,
        image_bytes: Optional[bytes],
        generation_parameters: Dict[str, Any],
        gcs_uri: Optional[str] = None,
    ):
        
        warning_logs.show_deprecation_warning()
        super().__init__(image_bytes=image_bytes, gcs_uri=gcs_uri)
        self._generation_parameters = generation_parameters

    @property
    def generation_parameters(self):
        
        return self._generation_parameters

    @staticmethod
    def load_from_file(location: str) -> "GeneratedImage":
        
        base_image = Image.load_from_file(location=location)
        exif = base_image._pil_image.getexif()  
        exif_comment_dict = json.loads(exif[_EXIF_USER_COMMENT_TAG_IDX])
        generation_parameters = exif_comment_dict[_IMAGE_GENERATION_PARAMETERS_EXIF_KEY]
        return GeneratedImage(
            image_bytes=base_image._image_bytes,  
            generation_parameters=generation_parameters,
            gcs_uri=base_image._gcs_uri,  
        )

    def save(self, location: str, include_generation_parameters: bool = True):
        
        if include_generation_parameters:
            if not self._generation_parameters:
                raise ValueError("Image does not have generation parameters.")
            if not PIL_Image:
                raise ValueError(
                    "The PIL module is required for saving generation parameters."
                )

            exif = self._pil_image.getexif()
            exif[_EXIF_USER_COMMENT_TAG_IDX] = json.dumps(
                {_IMAGE_GENERATION_PARAMETERS_EXIF_KEY: self._generation_parameters}
            )
            self._pil_image.save(location, exif=exif)
        else:
            super().save(location=location)


class ImageCaptioningModel(
    _model_garden_models._ModelGardenModel  
):
    

    __module__ = "vertexai.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml"

    def get_captions(
        self,
        image: Image,
        *,
        number_of_results: int = 1,
        language: str = "en",
        output_gcs_uri: Optional[str] = None,
    ) -> List[str]:
        
        instance = {}

        if image._gcs_uri:  
            instance["image"] = {
                "gcsUri": image._gcs_uri  
            }
        else:
            instance["image"] = {
                "bytesBase64Encoded": image._as_base64_string()  
            }
        parameters = {
            "sampleCount": number_of_results,
            "language": language,
        }
        if output_gcs_uri is not None:
            parameters["storageUri"] = output_gcs_uri

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )
        return response.predictions


class ImageQnAModel(
    _model_garden_models._ModelGardenModel  
):
    

    __module__ = "vertexai.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml"

    def ask_question(
        self,
        image: Image,
        question: str,
        *,
        number_of_results: int = 1,
    ) -> List[str]:
        
        instance = {"prompt": question}

        if image._gcs_uri:  
            instance["image"] = {
                "gcsUri": image._gcs_uri  
            }
        else:
            instance["image"] = {
                "bytesBase64Encoded": image._as_base64_string()  
            }
        parameters = {
            "sampleCount": number_of_results,
        }

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )
        return response.predictions


class MultiModalEmbeddingModel(_model_garden_models._ModelGardenModel):
    

    __module__ = "vertexai.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/vision_embedding_model_1.0.0.yaml"

    def get_embeddings(
        self,
        image: Optional[Image] = None,
        video: Optional[Video] = None,
        contextual_text: Optional[str] = None,
        dimension: Optional[int] = None,
        video_segment_config: Optional[VideoSegmentConfig] = None,
    ) -> "MultiModalEmbeddingResponse":
        

        if not image and not video and not contextual_text:
            raise ValueError(
                "One of `image`, `video`, or `contextual_text` is required."
            )

        instance = {}

        if image:
            if image._gcs_uri:  
                instance["image"] = {
                    "gcsUri": image._gcs_uri  
                }
            else:
                instance["image"] = {
                    "bytesBase64Encoded": image._as_base64_string()  
                }

        if video:
            if video._gcs_uri:  
                instance["video"] = {
                    "gcsUri": video._gcs_uri  
                }
            else:
                instance["video"] = {
                    "bytesBase64Encoded": video._as_base64_string()  
                }  

            if video_segment_config:
                instance["video"]["videoSegmentConfig"] = {
                    "startOffsetSec": video_segment_config.start_offset_sec,
                    "endOffsetSec": video_segment_config.end_offset_sec,
                    "intervalSec": video_segment_config.interval_sec,
                }

        if contextual_text:
            instance["text"] = contextual_text

        parameters = {}
        if dimension:
            parameters["dimension"] = dimension

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )
        image_embedding = response.predictions[0].get("imageEmbedding")
        video_embeddings = []
        for video_embedding in response.predictions[0].get("videoEmbeddings", []):
            video_embeddings.append(
                VideoEmbedding(
                    embedding=video_embedding["embedding"],
                    start_offset_sec=video_embedding["startOffsetSec"],
                    end_offset_sec=video_embedding["endOffsetSec"],
                )
            )
        text_embedding = (
            response.predictions[0].get("textEmbedding")
            if "textEmbedding" in response.predictions[0]
            else None
        )
        return MultiModalEmbeddingResponse(
            image_embedding=image_embedding,
            video_embeddings=video_embeddings,
            _prediction_response=response,
            text_embedding=text_embedding,
        )


@dataclasses.dataclass
class MultiModalEmbeddingResponse:
    

    __module__ = "vertexai.vision_models"

    _prediction_response: Any
    image_embedding: Optional[List[float]] = None
    video_embeddings: Optional[List[VideoEmbedding]] = None
    text_embedding: Optional[List[float]] = None


class ImageTextModel(ImageCaptioningModel, ImageQnAModel):
    

    __module__ = "vertexai.vision_models"

    
    

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/vision_reasoning_model_1.0.0.yaml"


@dataclasses.dataclass
class WatermarkVerificationResponse:

    __module__ = "vertexai.preview.vision_models"

    _prediction_response: Any
    watermark_verification_result: Optional[str] = None


class WatermarkVerificationModel(_model_garden_models._ModelGardenModel):
    

    __module__ = "vertexai.preview.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/watermark_verification_model_1.0.0.yaml"

    def verify_image(self, image: Image) -> WatermarkVerificationResponse:
        
        if not image:
            raise ValueError("Image is required.")

        instance = {}

        if image._gcs_uri:
            instance["image"] = {"gcsUri": image._gcs_uri}
        else:
            instance["image"] = {"bytesBase64Encoded": image._as_base64_string()}

        parameters = {}
        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )

        verification_likelihood = response.predictions[0].get("decision")
        return WatermarkVerificationResponse(
            _prediction_response=response,
            watermark_verification_result=verification_likelihood,
        )


class Scribble:
    

    __module__ = "vertexai.preview.vision_models"

    _image_: Optional[Image] = None

    def __init__(
        self,
        image_bytes: Optional[bytes],
        gcs_uri: Optional[str] = None,
    ):
        
        if bool(image_bytes) == bool(gcs_uri):
            raise ValueError("Either image_bytes or gcs_uri must be provided.")

        self._image_ = Image(image_bytes, gcs_uri)

    @property
    def image(self) -> Optional[Image]:
        
        return self._image_


@dataclasses.dataclass
class EntityLabel:
    

    __module__ = "vertexai.preview.vision_models"

    label: Optional[str] = None
    score: Optional[float] = None


class GeneratedMask(Image):
    

    __module__ = "vertexai.preview.vision_models"

    __labels__: Optional[List[EntityLabel]] = None

    def __init__(
        self,
        image_bytes: Optional[bytes],
        gcs_uri: Optional[str] = None,
        labels: Optional[List[EntityLabel]] = None,
    ):
        

        super().__init__(
            image_bytes=image_bytes,
            gcs_uri=gcs_uri,
        )
        self.__labels__ = labels

    @property
    def labels(self) -> Optional[List[EntityLabel]]:
        
        return self.__labels__


@dataclasses.dataclass
class ImageSegmentationResponse:
    

    __module__ = "vertexai.preview.vision_models"

    _prediction_response: Any
    masks: List[GeneratedMask]

    def __iter__(self) -> typing.Iterator[GeneratedMask]:
        
        yield from self.masks

    def __getitem__(self, idx: int) -> GeneratedMask:
        
        return self.masks[idx]


class ImageSegmentationModel(_model_garden_models._ModelGardenModel):
    

    __module__ = "vertexai.preview.vision_models"

    _INSTANCE_SCHEMA_URI = "gs://google-cloud-aiplatform/schema/predict/instance/image_segmentation_model_1.0.0.yaml"

    def segment_image(
        self,
        base_image: Image,
        prompt: Optional[str] = None,
        scribble: Optional[Scribble] = None,
        mode: Literal[
            "foreground", "background", "semantic", "prompt", "interactive"
        ] = "foreground",
        max_predictions: Optional[int] = None,
        confidence_threshold: Optional[float] = 0.1,
        mask_dilation: Optional[float] = None,
        binary_color_threshold: Optional[float] = None,
    ) -> ImageSegmentationResponse:
        
        if not base_image:
            raise ValueError("Base image is required.")
        instance = {}

        if base_image._gcs_uri:
            instance["image"] = {"gcsUri": base_image._gcs_uri}
        else:
            instance["image"] = {"bytesBase64Encoded": base_image._as_base64_string()}

        if prompt:
            instance["prompt"] = prompt

        parameters = {}
        if scribble and scribble.image:
            scribble_image = scribble.image
            if scribble_image._gcs_uri:
                instance["scribble"] = {"image": {"gcsUri": scribble_image._gcs_uri}}
            else:
                instance["scribble"] = {
                    "image": {"bytesBase64Encoded": scribble_image._as_base64_string()}
                }
        parameters["mode"] = mode
        if max_predictions:
            parameters["maxPredictions"] = max_predictions
        if confidence_threshold:
            parameters["confidenceThreshold"] = confidence_threshold
        if mask_dilation:
            parameters["maskDilation"] = mask_dilation
        if binary_color_threshold:
            parameters["binaryColorThreshold"] = binary_color_threshold

        response = self._endpoint.predict(
            instances=[instance],
            parameters=parameters,
        )

        masks: List[GeneratedMask] = []
        for prediction in response.predictions:
            encoded_bytes = prediction.get("bytesBase64Encoded")
            labels = []
            if "labels" in prediction:
                for label in prediction["labels"]:
                    labels.append(
                        EntityLabel(
                            label=label.get("label"),
                            score=label.get("score"),
                        )
                    )
            generated_image = GeneratedMask(
                image_bytes=base64.b64decode(encoded_bytes) if encoded_bytes else None,
                gcs_uri=prediction.get("gcsUri"),
                labels=labels,
            )
            masks.append(generated_image)

        return ImageSegmentationResponse(
            _prediction_response=response,
            masks=masks,
        )
