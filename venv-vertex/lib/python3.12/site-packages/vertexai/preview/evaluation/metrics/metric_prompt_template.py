

















from typing import Dict, List, Optional

from google.cloud.aiplatform import base
from vertexai.preview.evaluation import (
    prompt_template,
)


_LOGGER = base.Logger(__name__)
_NEWLINE = "\n"


def serialize_dict_in_order(elements: Optional[Dict[str, str]]):
    
    if elements is None:
        return ""
    return _NEWLINE.join(f"{key}: {value}" for key, value in sorted(elements.items()))


class _MetricPromptTemplate(prompt_template.PromptTemplate):
    

    def __init__(
        self,
        *,
        criteria: Dict[str, str],
        rating_rubric: Dict[str, str],
        input_variables: List[str],
        instruction: Optional[str] = None,
        evaluation_steps: Optional[Dict[str, str]] = None,
        metric_definition: Optional[str] = None,
        few_shot_examples: Optional[List[str]] = None,
    ):
        
        self._input_variables = input_variables

        self._instruction = instruction
        self._metric_definition = metric_definition
        self._criteria = criteria
        self._rating_rubric = rating_rubric
        self._evaluation_steps = evaluation_steps
        self._few_shot_examples = few_shot_examples

        self.template = self.__str__()

    @property
    def prompt_data(self) -> str:
        return self.template


class PointwiseMetricPromptTemplate(_MetricPromptTemplate):
    

    def __init__(
        self,
        *,
        criteria: Dict[str, str],
        rating_rubric: Dict[str, str],
        input_variables: Optional[List[str]] = None,
        instruction: Optional[str] = None,
        metric_definition: Optional[str] = None,
        evaluation_steps: Optional[Dict[str, str]] = None,
        few_shot_examples: Optional[List[str]] = None,
    ):
        
        if not input_variables:
            input_variables = []
            _LOGGER.info(
                "The `input_variables` parameter is empty. Only the `response`"
                " column is used for computing this model-based metric."
            )
        input_variables = list(set(input_variables + ["response"]))

        instruction = instruction or self.get_default_pointwise_instruction()

        evaluation_steps = (
            evaluation_steps or self.get_default_pointwise_evaluation_steps()
        )

        super().__init__(
            input_variables=input_variables,
            criteria=criteria,
            rating_rubric=rating_rubric,
            instruction=instruction,
            metric_definition=metric_definition,
            evaluation_steps=evaluation_steps,
            few_shot_examples=few_shot_examples,
        )

    def get_default_pointwise_instruction(self) -> str:
        

        return (
            "You are an expert evaluator. Your task is to evaluate the quality of"
            " the responses generated by AI models. We will provide you with the"
            " user prompt and an AI-generated responses.\nYou should first read"
            " the user input carefully for analyzing the task, and then evaluate"
            " the quality of the responses based on the Criteria provided in the"
            " Evaluation section below.\nYou will assign the response a rating"
            " following the Rating Rubric and Evaluation Steps. Give step by step"
            " explanations for your rating, and only choose ratings from the Rating"
            " Rubric."
        )

    def get_default_pointwise_evaluation_steps(self) -> Dict[str, str]:
        
        return {
            "Step 1": (
                "Assess the response in aspects of all criteria provided. Provide"
                " assessment according to each criterion."
            ),
            "Step 2": (
                "Score based on the rating rubric. Give a brief rationale to"
                " explain your evaluation considering each individual criterion."
            ),
        }

    def __str__(self):
        
        metric_prompt_template_str = [
            "
            f"{self._instruction}",
            _NEWLINE,
            "
        ]
        if self._metric_definition:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{self._metric_definition}\n",
                ]
            )
        metric_prompt_template_str.extend(
            [
                "
                f"{serialize_dict_in_order(self._criteria)}\n",
                "
                f"{serialize_dict_in_order(self._rating_rubric)}\n",
            ]
        )
        if self._evaluation_steps:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{serialize_dict_in_order(self._evaluation_steps)}\n",
                ]
            )
        if self._few_shot_examples:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{_NEWLINE.join(self._few_shot_examples)}\n",
                ]
            )
        metric_prompt_template_str.extend(
            ["\n
        )
        for input_variable in self._input_variables:
            if input_variable == "response":
                continue
            metric_prompt_template_str.extend(
                [
                    f"
                    f"{{{input_variable}}}\n",
                ]
            )
        metric_prompt_template_str.extend(
            [
                _NEWLINE,
                "\n
                "{response}",
            ]
        )
        return _NEWLINE.join(metric_prompt_template_str)

    def __repr__(self):
        return (
            f"PointwiseMetricPromptTemplate(prompt_data={self.prompt_data},"
            f" variables={self.variables})"
        )


class PairwiseMetricPromptTemplate(_MetricPromptTemplate):
    

    def __init__(
        self,
        *,
        criteria: Dict[str, str],
        rating_rubric: Dict[str, str],
        input_variables: Optional[List[str]] = None,
        instruction: Optional[str] = None,
        metric_definition: Optional[str] = None,
        evaluation_steps: Optional[Dict[str, str]] = None,
        few_shot_examples: Optional[List[str]] = None,
    ):
        
        if not input_variables:
            input_variables = []
            _LOGGER.info(
                "The `input_variables` parameter is empty. Only the `response`"
                " column and `baseline_model_response` columns are used for"
                " computing this model-based metric."
            )
        input_variables = list(
            set(input_variables + ["response", "baseline_model_response"])
        )

        instruction = instruction or self.get_default_pairwise_instruction()

        evaluation_steps = (
            evaluation_steps or self.get_default_pairwise_evaluation_steps()
        )

        super().__init__(
            input_variables=input_variables,
            criteria=criteria,
            rating_rubric=rating_rubric,
            instruction=instruction,
            metric_definition=metric_definition,
            evaluation_steps=evaluation_steps,
            few_shot_examples=few_shot_examples,
        )

    def get_default_pairwise_instruction(self) -> str:
        

        return (
            "You are an expert evaluator. Your task is to evaluate the quality of"
            " the responses generated by two AI models. We will provide you with"
            " the user input and a pair of AI-generated responses (Response A and"
            " Response B).\nYou should first read the user input carefully for"
            " analyzing the task, and then evaluate the quality of the responses"
            " based on based on the Criteria provided in the Evaluation section"
            " below.\nYou will first judge responses individually, following the"
            " Rating Rubric and Evaluation Steps. Then you will give step by step"
            " explanations for your judgement, compare results to declare the"
            " winner based on the Rating Rubric and Evaluation Steps."
        )

    def get_default_pairwise_evaluation_steps(self) -> Dict[str, str]:
        
        return {
            "Step 1": "Analyze Response A based on all the Criteria.",
            "Step 2": "Analyze Response B based on all the Criteria.",
            "Step 3": (
                "Compare the overall performance of Response A and Response B based"
                " on your analyses and assessment."
            ),
            "Step 4": (
                'Output your preference of "A", "SAME" or "B" to the'
                " pairwise_choice field according to the Rating Rubrics."
            ),
            "Step 5": "Output your assessment reasoning in the explanation field",
        }

    def __str__(self):
        
        metric_prompt_template_str = [
            "
            f"{self._instruction}",
            _NEWLINE,
            "
        ]
        if self._metric_definition:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{self._metric_definition}\n",
                ]
            )
        metric_prompt_template_str.extend(
            [
                "
                f"{serialize_dict_in_order(self._criteria)}\n",
                "
                f"{serialize_dict_in_order(self._rating_rubric)}\n",
            ]
        )
        if self._evaluation_steps:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{serialize_dict_in_order(self._evaluation_steps)}\n",
                ]
            )
        if self._few_shot_examples:
            metric_prompt_template_str.extend(
                [
                    "
                    f"{_NEWLINE.join(self._few_shot_examples)}\n",
                ]
            )
        metric_prompt_template_str.extend(
            ["\n
        )
        for input_variable in self._input_variables:
            if input_variable in ["response", "baseline_model_response"]:
                continue
            metric_prompt_template_str.extend(
                [
                    f"
                    f"{{{input_variable}}}\n",
                ]
            )
        metric_prompt_template_str.extend(
            [
                "\n
                "
                "{baseline_model_response}\n",
                "
                "{response}",
            ]
        )
        return _NEWLINE.join(metric_prompt_template_str)

    def __repr__(self):
        return (
            f"PairwiseMetricPromptTemplate(prompt_data={self.prompt_data},"
            f" variables={self.variables})"
        )
