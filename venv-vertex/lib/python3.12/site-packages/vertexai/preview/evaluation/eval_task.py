

















import logging
from typing import Any, Callable, Dict, List, Literal, Optional, TYPE_CHECKING, Union
import uuid
import warnings

from google.api_core import exceptions
import vertexai
from google.cloud.aiplatform import base
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.metadata import metadata
from vertexai import generative_models
from vertexai.preview import reasoning_engines
from vertexai.preview.evaluation import _base as eval_base
from vertexai.preview.evaluation import _evaluation
from vertexai.preview.evaluation import constants
from vertexai.preview.evaluation import utils as eval_utils
from vertexai.preview.evaluation.metrics import (
    _base as metrics_base,
)
from vertexai.preview.evaluation.metrics import pairwise_metric
from vertexai.preview.evaluation.metrics import pointwise_metric
from vertexai.preview.evaluation.metrics import (
    rubric_based_metric,
)
import numpy as np


if TYPE_CHECKING:
    import pandas as pd
    from google.colab import sheets



try:
    from IPython import display as IPython_display
except ImportError:
    IPython_display = None

_LOGGER = base.Logger(__name__)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

AutoraterConfig = eval_base.AutoraterConfig
EvalResult = eval_base.EvalResult
GenerativeModel = generative_models.GenerativeModel

_RunnableType = Union[reasoning_engines.Queryable, Callable[[str], Dict[str, str]]]
_ModelType = Union[generative_models.GenerativeModel, Callable[[str], str]]


class EvalTask:
    

    def __init__(
        self,
        *,
        dataset: Union["pd.DataFrame", str, Dict[str, Any], "sheets.InteractiveSheet"],
        metrics: List[
            Union[
                Literal[
                    "exact_match",
                    "bleu",
                    "rouge_1",
                    "rouge_2",
                    "rouge_l",
                    "rouge_l_sum",
                    "tool_call_valid",
                    "tool_name_match",
                    "tool_parameter_key_match",
                    "tool_parameter_kv_match",
                    "trajectory_exact_match",
                    "trajectory_in_order_match",
                    "trajectory_any_order_match",
                    "trajectory_precision",
                    "trajectory_recall",
                    "rubric_based_instruction_following",
                ],
                metrics_base.CustomMetric,
                metrics_base._AutomaticMetric,
                pointwise_metric.PointwiseMetric,
                pairwise_metric.PairwiseMetric,
                rubric_based_metric.RubricBasedMetric,
            ]
        ],
        experiment: Optional[str] = None,
        metric_column_mapping: Optional[Dict[str, str]] = None,
        output_uri_prefix: Optional[str] = "",
        autorater_config: Optional[AutoraterConfig] = None,
    ):
        
        self._dataset = eval_utils.load_dataset(dataset)
        self._metrics = metrics
        self._experiment = experiment
        self._metric_column_mapping = eval_utils.initialize_metric_column_mapping(
            metric_column_mapping, self._dataset
        )
        self.output_uri_prefix = output_uri_prefix
        self._autorater_config = autorater_config

    @property
    def dataset(self) -> "pd.DataFrame":
        
        return self._dataset

    @property
    def metrics(self) -> List[Union[str, metrics_base.CustomMetric]]:
        
        return self._metrics

    @property
    def autorater_config(self) -> Optional[AutoraterConfig]:
        
        return self._autorater_config

    @property
    def experiment(self) -> Optional[str]:
        
        return self._experiment

    def _evaluate_with_experiment(
        self,
        model: Optional[_ModelType] = None,
        runnable: Optional[_RunnableType] = None,
        prompt_template: Optional[str] = None,
        experiment_run_name: Optional[str] = None,
        evaluation_service_qps: Optional[float] = None,
        retry_timeout: float = 120.0,
        output_file_name: Optional[str] = None,
    ) -> EvalResult:
        
        self._validate_experiment_run()
        with vertexai.preview.start_run(experiment_run_name):
            self._log_eval_experiment_param(
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                output_file_name=output_file_name,
            )
            eval_result = _evaluation.evaluate(
                dataset=self._dataset,
                metrics=self._metrics,
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                metric_column_mapping=self._metric_column_mapping,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                autorater_config=self._autorater_config,
            )

            eval_result.summary_metrics = {
                k: ("NaN" if isinstance(v, float) and np.isnan(v) else v)
                for k, v in eval_result.summary_metrics.items()
            }
            eval_result.metadata = {
                "experiment": self._experiment,
                "experiment_run": experiment_run_name,
            }
            try:
                vertexai.preview.log_metrics(eval_result.summary_metrics)
            except (TypeError, exceptions.InvalidArgument) as e:
                _LOGGER.warning(f"Experiment metrics logging failed: {str(e)}")
        return eval_result

    def evaluate(
        self,
        *,
        model: Optional[_ModelType] = None,
        runnable: Optional[_RunnableType] = None,
        prompt_template: Optional[str] = None,
        experiment_run_name: Optional[str] = None,
        response_column_name: Optional[str] = None,
        baseline_model_response_column_name: Optional[str] = None,
        evaluation_service_qps: Optional[float] = None,
        retry_timeout: float = 120.0,
        output_file_name: Optional[str] = "",
    ) -> EvalResult:
        
        global_experiment_name = (
            metadata._experiment_tracker.experiment_name
        )  
        if experiment_run_name and not self._experiment and not global_experiment_name:
            raise ValueError(
                "Experiment is not set. Please initialize EvalTask with an"
                " experiment, or initialize a global experiment with "
                "`vertexai.init(experiment='experiment_name')`for logging this"
                " evaluation run."
            )

        self._verify_and_set_response_column_name(
            response_column_name=response_column_name,
            metric_column_mapping_key=constants.Dataset.MODEL_RESPONSE_COLUMN,
        )
        self._verify_and_set_response_column_name(
            response_column_name=baseline_model_response_column_name,
            metric_column_mapping_key=constants.Dataset.BASELINE_MODEL_RESPONSE_COLUMN,
        )
        if self.output_uri_prefix and not output_file_name:
            output_file_name = f"eval_results_{utils.timestamped_unique_name()}.csv"
        experiment_run_name = experiment_run_name or f"{uuid.uuid4()}"
        if self._experiment and global_experiment_name:
            metadata._experiment_tracker.set_experiment(  
                experiment=self._experiment, backing_tensorboard=False
            )
            eval_result = self._evaluate_with_experiment(
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
            metadata._experiment_tracker.set_experiment(  
                experiment=global_experiment_name, backing_tensorboard=False
            )
        elif self._experiment and not global_experiment_name:
            metadata._experiment_tracker.set_experiment(  
                experiment=self._experiment, backing_tensorboard=False
            )
            eval_result = self._evaluate_with_experiment(
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
            metadata._experiment_tracker.reset()  
        elif not self._experiment and global_experiment_name:
            eval_result = self._evaluate_with_experiment(
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
        else:
            eval_result = _evaluation.evaluate(
                dataset=self._dataset,
                metrics=self._metrics,
                model=model,
                runnable=runnable,
                prompt_template=prompt_template,
                metric_column_mapping=self._metric_column_mapping,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                autorater_config=self._autorater_config,
            )
        eval_utils.upload_evaluation_results(
            eval_result, self.output_uri_prefix, output_file_name
        )
        return eval_result

    def _validate_experiment_run(self) -> None:
        
        if (
            metadata._experiment_tracker.experiment_run
        ):  
            raise ValueError(
                "Experiment run already exists. Please specify the name of the"
                " experiment run to assign current session within this evaluation."
            )

    def _log_eval_experiment_param(
        self,
        model: _ModelType = None,
        runnable: _RunnableType = None,
        prompt_template: Optional[str] = None,
        output_file_name: Optional[str] = None,
    ) -> None:
        
        eval_metadata = {}
        if prompt_template is not None:
            eval_metadata.update({"prompt_template": prompt_template})

        if model:
            if isinstance(model, GenerativeModel):
                eval_metadata.update(
                    {
                        "model_name": model._model_name,  
                    }
                )

                if (
                    model._generation_config
                    and isinstance(  
                        model._generation_config,
                        dict,  
                    )
                ):
                    eval_metadata.update(
                        **model._generation_config
                    )  

                if model._safety_settings and isinstance(
                    model._safety_settings, dict
                ):  
                    safety_settings = (
                        model._safety_settings
                    )  
                    safety_settings_as_str = {
                        category.name: threshold.name
                        for category, threshold in safety_settings.items()
                    }
                    eval_metadata.update(safety_settings_as_str)

        if runnable:
            if isinstance(runnable, reasoning_engines.LangchainAgent):
                eval_metadata.update(
                    {
                        "model_name": runnable._model_name,
                        "tools": runnable._tools,
                    }  
                )

        if self.output_uri_prefix and output_file_name:
            eval_metadata.update(
                {"output_file": self.output_uri_prefix + "/" + output_file_name}
            )

        if eval_metadata:
            _LOGGER.info(
                f"Logging Eval experiment evaluation metadata: {eval_metadata}"
            )
            try:
                vertexai.preview.log_params(eval_metadata)
            except (ValueError, TypeError) as e:
                _LOGGER.warning(
                    f"Experiment evaluation metadata logging failed: {str(e)}"
                )

    def _verify_and_set_response_column_name(
        self, response_column_name: str, metric_column_mapping_key: str
    ) -> None:
        
        if response_column_name:
            if response_column_name in self._dataset.columns:
                self._metric_column_mapping[
                    metric_column_mapping_key
                ] = response_column_name
            else:
                raise ValueError(
                    f"(Baseline) Model response column {response_column_name} is not"
                    " found in the dataset."
                )

    def display_runs(self):
        
        if not self._experiment:
            raise ValueError("Experiment is not set.")
        elif IPython_display:
            IPython_display.display(
                vertexai.preview.get_experiment_df(self._experiment)
            )
