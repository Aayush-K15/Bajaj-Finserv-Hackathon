














from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Mapping,
    Optional,
    Sequence,
    Union,
)

if TYPE_CHECKING:
    try:
        from autogen import agentchat

        ConversableAgent = agentchat.ConversableAgent
        ChatResult = agentchat.ChatResult
    except ImportError:
        ConversableAgent = Any

    try:
        from opentelemetry.sdk import trace

        TracerProvider = trace.TracerProvider
        SpanProcessor = trace.SpanProcessor
        SynchronousMultiSpanProcessor = trace.SynchronousMultiSpanProcessor
    except ImportError:
        TracerProvider = Any
        SpanProcessor = Any
        SynchronousMultiSpanProcessor = Any


def _prepare_runnable_kwargs(
    runnable_kwargs: Mapping[str, Any],
    system_instruction: str,
    runnable_name: str,
    llm_config: Mapping[str, Any],
) -> Mapping[str, Any]:
    
    if runnable_kwargs is None:
        runnable_kwargs = {}

    if (
        "human_input_mode" in runnable_kwargs
        and runnable_kwargs["human_input_mode"] != "NEVER"
    ):
        from google.cloud.aiplatform import base

        _LOGGER = base.Logger(__name__)
        _LOGGER.warning(
            f"human_input_mode={runnable_kwargs['human_input_mode']}"
            "is not supported. Will be enforced to 'NEVER'."
        )
    runnable_kwargs["human_input_mode"] = "NEVER"

    if "system_message" not in runnable_kwargs and system_instruction:
        runnable_kwargs["system_message"] = system_instruction

    if "name" not in runnable_kwargs:
        runnable_kwargs["name"] = runnable_name

    if "llm_config" not in runnable_kwargs:
        runnable_kwargs["llm_config"] = llm_config

    return runnable_kwargs


def _default_runnable_builder(
    **runnable_kwargs: Any,
) -> "ConversableAgent":
    from autogen import agentchat

    return agentchat.ConversableAgent(**runnable_kwargs)


def _validate_callable_parameters_are_annotated(callable: Callable):
    
    import inspect

    parameters = dict(inspect.signature(callable).parameters)
    for name, parameter in parameters.items():
        if parameter.annotation == inspect.Parameter.empty:
            raise TypeError(
                f"Callable={callable.__name__} has untyped input_arg={name}. "
                f"Please specify a type when defining it, e.g. `{name}: str`."
            )


def _validate_tools(tools: Sequence[Callable[..., Any]]):
    
    for tool in tools:
        if isinstance(tool, Callable):
            _validate_callable_parameters_are_annotated(tool)


def _override_active_span_processor(
    tracer_provider: "TracerProvider",
    active_span_processor: "SynchronousMultiSpanProcessor",
):
    
    if tracer_provider._active_span_processor:
        tracer_provider._active_span_processor.shutdown()
    tracer_provider._active_span_processor = active_span_processor


class AG2Agent:
    

    agent_framework = "ag2"

    def __init__(
        self,
        model: str,
        runnable_name: str,
        *,
        api_type: Optional[str] = None,
        llm_config: Optional[Mapping[str, Any]] = None,
        system_instruction: Optional[str] = None,
        runnable_kwargs: Optional[Mapping[str, Any]] = None,
        runnable_builder: Optional[Callable[..., "ConversableAgent"]] = None,
        tools: Optional[Sequence[Callable[..., Any]]] = None,
        enable_tracing: bool = False,
    ):
        
        from google.cloud.aiplatform import initializer

        
        self._project = initializer.global_config.project
        self._location = initializer.global_config.location
        self._model_name = model or "gemini-1.0-pro-001"
        self._api_type = api_type or "google"
        self._llm_config = llm_config or {
            "config_list": [
                {
                    "project_id": self._project,
                    "location": self._location,
                    "model": self._model_name,
                    "api_type": self._api_type,
                }
            ]
        }
        self._system_instruction = system_instruction
        self._runnable_name = runnable_name
        self._runnable_kwargs = _prepare_runnable_kwargs(
            runnable_kwargs=runnable_kwargs,
            llm_config=self._llm_config,
            system_instruction=self._system_instruction,
            runnable_name=self._runnable_name,
        )

        self._tools = []
        if tools:
            
            
            _validate_tools(tools)
            self._tools = tools
        self._ag2_tool_objects = []
        self._runnable = None
        self._runnable_builder = runnable_builder

        self._instrumentor = None
        self._enable_tracing = enable_tracing

    def set_up(self):
        
        if self._enable_tracing:
            from vertexai.reasoning_engines import _utils

            cloud_trace_exporter = _utils._import_cloud_trace_exporter_or_warn()
            cloud_trace_v2 = _utils._import_cloud_trace_v2_or_warn()
            openinference_autogen = _utils._import_openinference_autogen_or_warn()
            opentelemetry = _utils._import_opentelemetry_or_warn()
            opentelemetry_sdk_trace = _utils._import_opentelemetry_sdk_trace_or_warn()
            if all(
                (
                    cloud_trace_exporter,
                    cloud_trace_v2,
                    openinference_autogen,
                    opentelemetry,
                    opentelemetry_sdk_trace,
                )
            ):
                import google.auth

                credentials, _ = google.auth.default()
                span_exporter = cloud_trace_exporter.CloudTraceSpanExporter(
                    project_id=self._project,
                    client=cloud_trace_v2.TraceServiceClient(
                        credentials=credentials.with_quota_project(self._project),
                    ),
                )
                span_processor: SpanProcessor = (
                    opentelemetry_sdk_trace.export.SimpleSpanProcessor(
                        span_exporter=span_exporter,
                    )
                )
                tracer_provider: TracerProvider = (
                    opentelemetry.trace.get_tracer_provider()
                )
                
                
                
                
                
                
                
                if not tracer_provider:
                    from google.cloud.aiplatform import base

                    _LOGGER = base.Logger(__name__)
                    _LOGGER.warning(
                        "No tracer provider. By default, "
                        "we should get one of the following providers: "
                        "OTEL_PYTHON_TRACER_PROVIDER, _TRACER_PROVIDER, "
                        "or _PROXY_TRACER_PROVIDER."
                    )
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                
                
                if _utils.is_noop_or_proxy_tracer_provider(tracer_provider):
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                _override_active_span_processor(
                    tracer_provider,
                    opentelemetry_sdk_trace.SynchronousMultiSpanProcessor(),
                )
                tracer_provider.add_span_processor(span_processor)
                
                
                
                
                
                
                
                self._instrumentor = openinference_autogen.AutogenInstrumentor()
                self._instrumentor.uninstrument()
                self._instrumentor.instrument()
            else:
                from google.cloud.aiplatform import base

                _LOGGER = base.Logger(__name__)
                _LOGGER.warning(
                    "enable_tracing=True but proceeding with tracing disabled "
                    "because not all packages for tracing have been installed"
                )

        
        if self._tools and not self._ag2_tool_objects:
            from vertexai.reasoning_engines import _utils

            autogen_tools = _utils._import_autogen_tools_or_warn()
            if autogen_tools:
                for tool in self._tools:
                    self._ag2_tool_objects.append(autogen_tools.Tool(func_or_tool=tool))

        
        runnable_builder = self._runnable_builder or _default_runnable_builder
        self._runnable = runnable_builder(
            **self._runnable_kwargs,
        )

    def clone(self) -> "AG2Agent":
        
        import copy

        return AG2Agent(
            model=self._model_name,
            api_type=self._api_type,
            llm_config=copy.deepcopy(self._llm_config),
            system_instruction=self._system_instruction,
            runnable_name=self._runnable_name,
            tools=copy.deepcopy(self._tools),
            runnable_kwargs=copy.deepcopy(self._runnable_kwargs),
            runnable_builder=self._runnable_builder,
            enable_tracing=self._enable_tracing,
        )

    def query(
        self,
        *,
        input: Union[str, Mapping[str, Any]],
        max_turns: Optional[int] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        
        if isinstance(input, str):
            input = {"content": input}

        if max_turns and isinstance(max_turns, float):
            
            max_turns = round(max_turns)

        if "user_input" in kwargs:
            from google.cloud.aiplatform import base

            _LOGGER = base.Logger(__name__)
            _LOGGER.warning(
                "The `user_input` parameter should not be passed through"
                "kwargs. The `user_input` defaults to `False`."
            )
            kwargs.pop("user_input")

        if not self._runnable:
            self.set_up()

        from vertexai.reasoning_engines import _utils

        
        
        
        
        return _utils.dataclass_to_dict(
            self._runnable.run(
                input,
                user_input=False,
                tools=self._ag2_tool_objects,
                max_turns=max_turns,
                **kwargs,
            )
        )
