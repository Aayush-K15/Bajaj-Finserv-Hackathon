














from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Iterable,
    Mapping,
    Optional,
    Sequence,
    Union,
)

if TYPE_CHECKING:
    try:
        from langchain_core import runnables
        from langchain_core import tools as lc_tools
        from langchain_core.language_models import base as lc_language_models

        BaseTool = lc_tools.BaseTool
        BaseLanguageModel = lc_language_models.BaseLanguageModel
        RunnableConfig = runnables.RunnableConfig
        RunnableSerializable = runnables.RunnableSerializable
    except ImportError:
        BaseTool = Any
        BaseLanguageModel = Any
        RunnableConfig = Any
        RunnableSerializable = Any

    try:
        from langchain_google_vertexai.functions_utils import _ToolsType

        _ToolLike = _ToolsType
    except ImportError:
        _ToolLike = Any

    try:
        from opentelemetry.sdk import trace

        TracerProvider = trace.TracerProvider
        SpanProcessor = trace.SpanProcessor
        SynchronousMultiSpanProcessor = trace.SynchronousMultiSpanProcessor
    except ImportError:
        TracerProvider = Any
        SpanProcessor = Any
        SynchronousMultiSpanProcessor = Any

    try:
        from langgraph_checkpoint.checkpoint import base

        BaseCheckpointSaver = base.BaseCheckpointSaver
    except ImportError:
        try:
            from langgraph.checkpoint import base

            BaseCheckpointSaver = base.BaseCheckpointSaver
        except ImportError:
            BaseCheckpointSaver = Any


def _default_model_builder(
    model_name: str,
    *,
    project: str,
    location: str,
    model_kwargs: Optional[Mapping[str, Any]] = None,
) -> "BaseLanguageModel":
    
    import vertexai
    from google.cloud.aiplatform import initializer
    from langchain_google_vertexai import ChatVertexAI

    model_kwargs = model_kwargs or {}
    current_project = initializer.global_config.project
    current_location = initializer.global_config.location
    vertexai.init(project=project, location=location)
    model = ChatVertexAI(model_name=model_name, **model_kwargs)
    vertexai.init(project=current_project, location=current_location)
    return model


def _default_runnable_builder(
    model: "BaseLanguageModel",
    *,
    tools: Optional[Sequence["_ToolLike"]] = None,
    checkpointer: Optional[Any] = None,
    model_tool_kwargs: Optional[Mapping[str, Any]] = None,
    runnable_kwargs: Optional[Mapping[str, Any]] = None,
) -> "RunnableSerializable":
    
    from langgraph import prebuilt as langgraph_prebuilt

    model_tool_kwargs = model_tool_kwargs or {}
    runnable_kwargs = runnable_kwargs or {}
    if tools:
        model = model.bind_tools(tools=tools, **model_tool_kwargs)
    else:
        tools = []
    if checkpointer:
        if "checkpointer" in runnable_kwargs:
            from google.cloud.aiplatform import base

            base.Logger(__name__).warning(
                "checkpointer is being specified in both checkpointer_builder "
                "and runnable_kwargs. Please specify it in only one of them. "
                "Overriding the checkpointer in runnable_kwargs."
            )
        runnable_kwargs["checkpointer"] = checkpointer
    return langgraph_prebuilt.create_react_agent(
        model,
        tools=tools,
        **runnable_kwargs,
    )


def _validate_callable_parameters_are_annotated(callable: Callable):
    
    import inspect

    parameters = dict(inspect.signature(callable).parameters)
    for name, parameter in parameters.items():
        if parameter.annotation == inspect.Parameter.empty:
            raise TypeError(
                f"Callable={callable.__name__} has untyped input_arg={name}. "
                f"Please specify a type when defining it, e.g. `{name}: str`."
            )


def _validate_tools(tools: Sequence["_ToolLike"]):
    
    for tool in tools:
        if isinstance(tool, Callable):
            _validate_callable_parameters_are_annotated(tool)


def _override_active_span_processor(
    tracer_provider: "TracerProvider",
    active_span_processor: "SynchronousMultiSpanProcessor",
):
    
    if tracer_provider._active_span_processor:
        tracer_provider._active_span_processor.shutdown()
    tracer_provider._active_span_processor = active_span_processor


class LanggraphAgent:
    

    agent_framework = "langgraph"

    def __init__(
        self,
        model: str,
        *,
        tools: Optional[Sequence["_ToolLike"]] = None,
        model_kwargs: Optional[Mapping[str, Any]] = None,
        model_tool_kwargs: Optional[Mapping[str, Any]] = None,
        model_builder: Optional[Callable[..., "BaseLanguageModel"]] = None,
        runnable_kwargs: Optional[Mapping[str, Any]] = None,
        runnable_builder: Optional[Callable[..., "RunnableSerializable"]] = None,
        checkpointer_kwargs: Optional[Mapping[str, Any]] = None,
        checkpointer_builder: Optional[Callable[..., "BaseCheckpointSaver"]] = None,
        enable_tracing: bool = False,
    ):
        
        from google.cloud.aiplatform import initializer

        self._project = initializer.global_config.project
        self._location = initializer.global_config.location
        self._tools = []
        if tools:
            
            
            _validate_tools(tools)
            self._tools = tools
        self._model_name = model
        self._model_kwargs = model_kwargs
        self._model_tool_kwargs = model_tool_kwargs
        self._runnable_kwargs = runnable_kwargs
        self._checkpointer_kwargs = checkpointer_kwargs
        self._model = None
        self._model_builder = model_builder
        self._runnable = None
        self._runnable_builder = runnable_builder
        self._checkpointer_builder = checkpointer_builder
        self._instrumentor = None
        self._enable_tracing = enable_tracing

    def set_up(self):
        
        if self._enable_tracing:
            from vertexai.reasoning_engines import _utils

            cloud_trace_exporter = _utils._import_cloud_trace_exporter_or_warn()
            cloud_trace_v2 = _utils._import_cloud_trace_v2_or_warn()
            openinference_langchain = _utils._import_openinference_langchain_or_warn()
            opentelemetry = _utils._import_opentelemetry_or_warn()
            opentelemetry_sdk_trace = _utils._import_opentelemetry_sdk_trace_or_warn()
            if all(
                (
                    cloud_trace_exporter,
                    cloud_trace_v2,
                    openinference_langchain,
                    opentelemetry,
                    opentelemetry_sdk_trace,
                )
            ):
                import google.auth

                credentials, _ = google.auth.default()
                span_exporter = cloud_trace_exporter.CloudTraceSpanExporter(
                    project_id=self._project,
                    client=cloud_trace_v2.TraceServiceClient(
                        credentials=credentials.with_quota_project(self._project),
                    ),
                )
                span_processor: SpanProcessor = (
                    opentelemetry_sdk_trace.export.SimpleSpanProcessor(
                        span_exporter=span_exporter,
                    )
                )
                tracer_provider: TracerProvider = (
                    opentelemetry.trace.get_tracer_provider()
                )
                
                
                
                
                
                
                
                if not tracer_provider:
                    from google.cloud.aiplatform import base

                    base.Logger(__name__).warning(
                        "No tracer provider. By default, "
                        "we should get one of the following providers: "
                        "OTEL_PYTHON_TRACER_PROVIDER, _TRACER_PROVIDER, "
                        "or _PROXY_TRACER_PROVIDER."
                    )
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                
                
                if _utils.is_noop_or_proxy_tracer_provider(tracer_provider):
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                _override_active_span_processor(
                    tracer_provider,
                    opentelemetry_sdk_trace.SynchronousMultiSpanProcessor(),
                )
                tracer_provider.add_span_processor(span_processor)
                
                
                
                
                
                
                
                self._instrumentor = openinference_langchain.LangChainInstrumentor()
                if self._instrumentor.is_instrumented_by_opentelemetry:
                    self._instrumentor.uninstrument()
                self._instrumentor.instrument()
            else:
                from google.cloud.aiplatform import base

                _LOGGER = base.Logger(__name__)
                _LOGGER.warning(
                    "enable_tracing=True but proceeding with tracing disabled "
                    "because not all packages for tracing have been installed"
                )
        model_builder = self._model_builder or _default_model_builder
        self._model = model_builder(
            model_name=self._model_name,
            model_kwargs=self._model_kwargs,
            project=self._project,
            location=self._location,
        )
        self._checkpointer = None
        if self._checkpointer_builder:
            checkpointer_kwargs = self._checkpointer_kwargs or {}
            self._checkpointer = self._checkpointer_builder(
                **checkpointer_kwargs,
            )
        runnable_builder = self._runnable_builder or _default_runnable_builder
        self._runnable = runnable_builder(
            model=self._model,
            tools=self._tools,
            checkpointer=self._checkpointer,
            model_tool_kwargs=self._model_tool_kwargs,
            runnable_kwargs=self._runnable_kwargs,
        )

    def clone(self) -> "LanggraphAgent":
        
        import copy

        return LanggraphAgent(
            model=self._model_name,
            tools=copy.deepcopy(self._tools),
            model_kwargs=copy.deepcopy(self._model_kwargs),
            model_tool_kwargs=copy.deepcopy(self._model_tool_kwargs),
            runnable_kwargs=copy.deepcopy(self._runnable_kwargs),
            checkpointer_kwargs=copy.deepcopy(self._checkpointer_kwargs),
            model_builder=self._model_builder,
            runnable_builder=self._runnable_builder,
            checkpointer_builder=self._checkpointer_builder,
            enable_tracing=self._enable_tracing,
        )

    def query(
        self,
        *,
        input: Union[str, Mapping[str, Any]],
        config: Optional["RunnableConfig"] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        
        from langchain.load import dump as langchain_load_dump

        if isinstance(input, str):
            input = {"input": input}
        if not self._runnable:
            self.set_up()
        return langchain_load_dump.dumpd(
            self._runnable.invoke(input=input, config=config, **kwargs)
        )

    def stream_query(
        self,
        *,
        input: Union[str, Mapping[str, Any]],
        config: Optional["RunnableConfig"] = None,
        **kwargs,
    ) -> Iterable[Any]:
        
        from langchain.load import dump as langchain_load_dump

        if isinstance(input, str):
            input = {"input": input}
        if not self._runnable:
            self.set_up()
        for chunk in self._runnable.stream(input=input, config=config, **kwargs):
            yield langchain_load_dump.dumpd(chunk)

    def get_state_history(
        self,
        config: Optional["RunnableConfig"] = None,
        **kwargs: Any,
    ) -> Iterable[Any]:
        
        if not self._runnable:
            self.set_up()
        for state_snapshot in self._runnable.get_state_history(config=config, **kwargs):
            yield state_snapshot._asdict()

    def get_state(
        self,
        config: Optional["RunnableConfig"] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        
        if not self._runnable:
            self.set_up()
        return self._runnable.get_state(config=config, **kwargs)._asdict()

    def update_state(
        self,
        config: Optional["RunnableConfig"] = None,
        **kwargs: Any,
    ) -> Dict[str, Any]:
        
        if not self._runnable:
            self.set_up()
        return self._runnable.update_state(config=config, **kwargs)

    def register_operations(self) -> Mapping[str, Sequence[str]]:
        
        return {
            "": ["query", "get_state", "update_state"],
            "stream": ["stream_query", "get_state_history"],
        }
