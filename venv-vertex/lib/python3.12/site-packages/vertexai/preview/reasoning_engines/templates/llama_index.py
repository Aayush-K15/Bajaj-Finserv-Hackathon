














from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Dict,
    Mapping,
    Optional,
    Sequence,
    Union,
)

if TYPE_CHECKING:
    try:
        from llama_index.core.base.query_pipeline import query
        from llama_index.core.llms import function_calling
        from llama_index.core import query_pipeline

        FunctionCallingLLM = function_calling.FunctionCallingLLM
        QueryComponent = query.QUERY_COMPONENT_TYPE
        QueryPipeline = query_pipeline.QueryPipeline
    except ImportError:
        FunctionCallingLLM = Any
        QueryComponent = Any
        QueryPipeline = Any

    try:
        from opentelemetry.sdk import trace

        TracerProvider = trace.TracerProvider
        SpanProcessor = trace.SpanProcessor
        SynchronousMultiSpanProcessor = trace.SynchronousMultiSpanProcessor
    except ImportError:
        TracerProvider = Any
        SpanProcessor = Any
        SynchronousMultiSpanProcessor = Any


def _default_model_builder(
    model_name: str,
    *,
    project: str,
    location: str,
    model_kwargs: Optional[Mapping[str, Any]] = None,
) -> "FunctionCallingLLM":
    
    import vertexai
    from google.cloud.aiplatform import initializer
    from llama_index.llms import google_genai

    model_kwargs = model_kwargs or {}
    model = google_genai.GoogleGenAI(
        model=model_name,
        vertexai_config={"project": project, "location": location},
        **model_kwargs,
    )
    current_project = initializer.global_config.project
    current_location = initializer.global_config.location
    vertexai.init(project=current_project, location=current_location)
    return model


def _default_runnable_builder(
    model: "FunctionCallingLLM",
    *,
    system_instruction: Optional[str] = None,
    prompt: Optional["QueryComponent"] = None,
    retriever: Optional["QueryComponent"] = None,
    response_synthesizer: Optional["QueryComponent"] = None,
    runnable_kwargs: Optional[Mapping[str, Any]] = None,
) -> "QueryPipeline":
    
    try:
        from llama_index.core.query_pipeline import QueryPipeline
    except ImportError:
        raise ImportError(
            "Please call 'pip install google-cloud-aiplatform[llama_index]'."
        )

    prompt = prompt or _default_prompt(
        system_instruction=system_instruction,
    )
    pipeline = QueryPipeline(**runnable_kwargs)
    pipeline_modules = {
        "prompt": prompt,
        "model": model,
    }
    if retriever:
        pipeline_modules["retriever"] = retriever
    if response_synthesizer:
        pipeline_modules["response_synthesizer"] = response_synthesizer

    pipeline.add_modules(pipeline_modules)
    pipeline.add_link("prompt", "model")
    if "retriever" in pipeline_modules:
        pipeline.add_link("model", "retriever")
    if "response_synthesizer" in pipeline_modules:
        pipeline.add_link("model", "response_synthesizer", dest_key="query_str")
        if "retriever" in pipeline_modules:
            pipeline.add_link("retriever", "response_synthesizer", dest_key="nodes")

    return pipeline


def _default_prompt(
    system_instruction: Optional[str] = None,
) -> "QueryComponent":
    
    try:
        from llama_index.core import prompts
        from llama_index.core.base.llms import types
    except ImportError:
        raise ImportError(
            "Please call 'pip install google-cloud-aiplatform[llama_index]'."
        )

    
    message_templates = []
    if system_instruction:
        message_templates.append(
            types.ChatMessage(role=types.MessageRole.SYSTEM, content=system_instruction)
        )
    
    message_templates.append(
        types.ChatMessage(role=types.MessageRole.USER, content="{input}")
    )

    
    return prompts.ChatPromptTemplate(message_templates=message_templates)


def _override_active_span_processor(
    tracer_provider: "TracerProvider",
    active_span_processor: "SynchronousMultiSpanProcessor",
):
    
    if tracer_provider._active_span_processor:
        tracer_provider._active_span_processor.shutdown()
    tracer_provider._active_span_processor = active_span_processor


class LlamaIndexQueryPipelineAgent:
    

    agent_framework = "llama-index"

    def __init__(
        self,
        model: str,
        *,
        system_instruction: Optional[str] = None,
        prompt: Optional["QueryComponent"] = None,
        model_kwargs: Optional[Mapping[str, Any]] = None,
        model_builder: Optional[Callable[..., "FunctionCallingLLM"]] = None,
        retriever_kwargs: Optional[Mapping[str, Any]] = None,
        retriever_builder: Optional[Callable[..., "QueryComponent"]] = None,
        response_synthesizer_kwargs: Optional[Mapping[str, Any]] = None,
        response_synthesizer_builder: Optional[Callable[..., "QueryComponent"]] = None,
        runnable_kwargs: Optional[Mapping[str, Any]] = None,
        runnable_builder: Optional[Callable[..., "QueryPipeline"]] = None,
        enable_tracing: bool = False,
    ):
        
        from google.cloud.aiplatform import initializer

        self._project = initializer.global_config.project
        self._location = initializer.global_config.location
        self._model_name = model
        self._system_instruction = system_instruction
        self._prompt = prompt

        self._model = None
        self._model_kwargs = model_kwargs or {}
        self._model_builder = model_builder

        self._retriever = None
        self._retriever_kwargs = retriever_kwargs or {}
        self._retriever_builder = retriever_builder

        self._response_synthesizer = None
        self._response_synthesizer_kwargs = response_synthesizer_kwargs or {}
        self._response_synthesizer_builder = response_synthesizer_builder

        self._runnable = None
        self._runnable_kwargs = runnable_kwargs or {}
        self._runnable_builder = runnable_builder

        self._instrumentor = None
        self._enable_tracing = enable_tracing

    def set_up(self):
        
        if self._enable_tracing:
            from vertexai.reasoning_engines import _utils

            cloud_trace_exporter = _utils._import_cloud_trace_exporter_or_warn()
            cloud_trace_v2 = _utils._import_cloud_trace_v2_or_warn()
            openinference_llama_index = (
                _utils._import_openinference_llama_index_or_warn()
            )
            opentelemetry = _utils._import_opentelemetry_or_warn()
            opentelemetry_sdk_trace = _utils._import_opentelemetry_sdk_trace_or_warn()
            if all(
                (
                    cloud_trace_exporter,
                    cloud_trace_v2,
                    openinference_llama_index,
                    opentelemetry,
                    opentelemetry_sdk_trace,
                )
            ):
                import google.auth

                credentials, _ = google.auth.default()
                span_exporter = cloud_trace_exporter.CloudTraceSpanExporter(
                    project_id=self._project,
                    client=cloud_trace_v2.TraceServiceClient(
                        credentials=credentials.with_quota_project(self._project),
                    ),
                )
                span_processor: SpanProcessor = (
                    opentelemetry_sdk_trace.export.SimpleSpanProcessor(
                        span_exporter=span_exporter,
                    )
                )
                tracer_provider: TracerProvider = (
                    opentelemetry.trace.get_tracer_provider()
                )
                
                
                
                
                
                
                
                if not tracer_provider:
                    from google.cloud.aiplatform import base

                    _LOGGER = base.Logger(__name__)
                    _LOGGER.warning(
                        "No tracer provider. By default, "
                        "we should get one of the following providers: "
                        "OTEL_PYTHON_TRACER_PROVIDER, _TRACER_PROVIDER, "
                        "or _PROXY_TRACER_PROVIDER."
                    )
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                
                
                if _utils.is_noop_or_proxy_tracer_provider(tracer_provider):
                    tracer_provider = opentelemetry_sdk_trace.TracerProvider()
                    opentelemetry.trace.set_tracer_provider(tracer_provider)
                
                _override_active_span_processor(
                    tracer_provider,
                    opentelemetry_sdk_trace.SynchronousMultiSpanProcessor(),
                )
                tracer_provider.add_span_processor(span_processor)
                
                
                
                
                
                
                
                self._instrumentor = openinference_llama_index.LlamaIndexInstrumentor()
                if self._instrumentor.is_instrumented_by_opentelemetry:
                    self._instrumentor.uninstrument()
                self._instrumentor.instrument()
            else:
                from google.cloud.aiplatform import base

                _LOGGER = base.Logger(__name__)
                _LOGGER.warning(
                    "enable_tracing=True but proceeding with tracing disabled "
                    "because not all packages for tracing have been installed"
                )

        model_builder = self._model_builder or _default_model_builder
        self._model = model_builder(
            model_name=self._model_name,
            model_kwargs=self._model_kwargs,
            project=self._project,
            location=self._location,
        )

        if self._retriever_builder:
            self._retriever = self._retriever_builder(
                model=self._model,
                retriever_kwargs=self._retriever_kwargs,
            )

        if self._response_synthesizer_builder:
            self._response_synthesizer = self._response_synthesizer_builder(
                model=self._model,
                response_synthesizer_kwargs=self._response_synthesizer_kwargs,
            )

        runnable_builder = self._runnable_builder or _default_runnable_builder
        self._runnable = runnable_builder(
            prompt=self._prompt,
            model=self._model,
            system_instruction=self._system_instruction,
            retriever=self._retriever,
            response_synthesizer=self._response_synthesizer,
            runnable_kwargs=self._runnable_kwargs,
        )

    def clone(self) -> "LlamaIndexQueryPipelineAgent":
        
        import copy

        return LlamaIndexQueryPipelineAgent(
            model=self._model_name,
            system_instruction=self._system_instruction,
            prompt=copy.deepcopy(self._prompt),
            model_kwargs=copy.deepcopy(self._model_kwargs),
            model_builder=self._model_builder,
            retriever_kwargs=copy.deepcopy(self._retriever_kwargs),
            retriever_builder=self._retriever_builder,
            response_synthesizer_kwargs=copy.deepcopy(
                self._response_synthesizer_kwargs
            ),
            response_synthesizer_builder=self._response_synthesizer_builder,
            runnable_kwargs=copy.deepcopy(self._runnable_kwargs),
            runnable_builder=self._runnable_builder,
            enable_tracing=self._enable_tracing,
        )

    def query(
        self,
        input: Union[str, Mapping[str, Any]],
        **kwargs: Any,
    ) -> Union[str, Dict[str, Any], Sequence[Union[str, Dict[str, Any]]]]:
        
        from vertexai.reasoning_engines import _utils

        if isinstance(input, str):
            input = {"input": input}

        if not self._runnable:
            self.set_up()

        if kwargs.get("batch"):
            nest_asyncio = _utils._import_nest_asyncio_or_warn()
            nest_asyncio.apply()

        return _utils.to_json_serializable_llama_index_object(
            self._runnable.run(**input, **kwargs)
        )
