















import logging
from typing import Any, Callable, Dict, List, Literal, Optional, TYPE_CHECKING, Union
import uuid
import warnings

from google.api_core import exceptions
import vertexai
from google.cloud.aiplatform import base
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.metadata import metadata
from vertexai import generative_models
from vertexai.evaluation import _base as eval_base
from vertexai.evaluation import _evaluation
from vertexai.evaluation import constants
from vertexai.evaluation import utils as eval_utils
from vertexai.evaluation.metrics import (
    _base as metrics_base,
)
from vertexai.evaluation.metrics import (
    pairwise_metric,
)
from vertexai.evaluation.metrics import (
    pointwise_metric,
)
import numpy as np

if TYPE_CHECKING:
    import pandas as pd


try:
    from IPython import display as IPython_display
except ImportError:
    IPython_display = None

_LOGGER = base.Logger(__name__)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
warnings.filterwarnings("ignore")

EvalResult = eval_base.EvalResult
GenerativeModel = generative_models.GenerativeModel


class EvalTask:
    

    _resource_noun = "evaluationTasks"

    def __init__(
        self,
        *,
        dataset: Union["pd.DataFrame", str, Dict[str, Any]],
        metrics: List[
            Union[
                Literal[
                    "exact_match",
                    "bleu",
                    "rouge_1",
                    "rouge_2",
                    "rouge_l",
                    "rouge_l_sum",
                    "tool_call_valid",
                    "tool_name_match",
                    "tool_parameter_key_match",
                    "tool_parameter_kv_match",
                ],
                metrics_base.CustomMetric,
                metrics_base._AutomaticMetric,
                metrics_base._TranslationMetric,
                pointwise_metric.PointwiseMetric,
                pairwise_metric.PairwiseMetric,
            ]
        ],
        experiment: Optional[str] = None,
        metric_column_mapping: Optional[Dict[str, str]] = None,
        output_uri_prefix: Optional[str] = "",
    ):
        
        self._raw_dataset = dataset
        self._dataset = eval_utils.load_dataset(dataset)
        self._metrics = metrics
        self._experiment = experiment
        self._metric_column_mapping = eval_utils.initialize_metric_column_mapping(
            metric_column_mapping, self._dataset
        )
        self.output_uri_prefix = output_uri_prefix

    @property
    def dataset(self) -> "pd.DataFrame":
        
        return self._dataset

    @property
    def metrics(self) -> List[Union[str, metrics_base.CustomMetric]]:
        
        return self._metrics

    @property
    def experiment(self) -> Optional[str]:
        
        return self._experiment

    def _evaluate_with_experiment(
        self,
        *,
        model: Optional[Union[GenerativeModel, Callable[[str], str]]] = None,
        prompt_template: Optional[str] = None,
        experiment_run_name: Optional[str] = None,
        evaluation_service_qps: Optional[float] = None,
        retry_timeout: float = 120.0,
        output_file_name: Optional[str] = None,
    ) -> EvalResult:
        
        self._validate_experiment_run()
        with vertexai.preview.start_run(experiment_run_name):
            self._log_eval_experiment_param(
                model=model,
                prompt_template=prompt_template,
                output_file_name=output_file_name,
            )
            eval_result = _evaluation.evaluate(
                dataset=self._dataset,
                metrics=self._metrics,
                model=model,
                prompt_template=prompt_template,
                metric_column_mapping=self._metric_column_mapping,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
            )

            eval_result.summary_metrics = {
                k: ("NaN" if isinstance(v, float) and np.isnan(v) else v)
                for k, v in eval_result.summary_metrics.items()
            }
            eval_result.metadata = {
                "experiment": self._experiment,
                "experiment_run": experiment_run_name,
            }
            try:
                vertexai.preview.log_metrics(eval_result.summary_metrics)
            except (TypeError, exceptions.InvalidArgument) as e:
                _LOGGER.warning(f"Experiment metrics logging failed: {str(e)}")
        return eval_result

    def evaluate(
        self,
        *,
        model: Optional[Union[GenerativeModel, Callable[[str], str]]] = None,
        prompt_template: Optional[str] = None,
        experiment_run_name: Optional[str] = None,
        response_column_name: Optional[str] = None,
        baseline_model_response_column_name: Optional[str] = None,
        evaluation_service_qps: Optional[float] = None,
        retry_timeout: float = 120.0,
        output_file_name: Optional[str] = None,
    ) -> EvalResult:
        
        global_experiment_name = metadata._experiment_tracker.experiment_name
        if experiment_run_name and not self._experiment and not global_experiment_name:
            raise ValueError(
                "Experiment is not set. Please initialize `EvalTask` with an"
                " experiment, or initialize a global experiment with "
                "`vertexai.init(experiment='experiment_name')`for logging this"
                " evaluation run."
            )
        if self.output_uri_prefix and not output_file_name:
            output_file_name = f"eval_results_{utils.timestamped_unique_name()}.csv"
        self._verify_and_set_response_column_name(
            response_column_name=response_column_name,
            metric_column_mapping_key=constants.Dataset.MODEL_RESPONSE_COLUMN,
        )
        self._verify_and_set_response_column_name(
            response_column_name=baseline_model_response_column_name,
            metric_column_mapping_key=constants.Dataset.BASELINE_MODEL_RESPONSE_COLUMN,
        )

        experiment_run_name = experiment_run_name or f"{uuid.uuid4()}"
        if self._experiment and global_experiment_name:
            metadata._experiment_tracker.set_experiment(
                experiment=self._experiment, backing_tensorboard=False
            )
            eval_result = self._evaluate_with_experiment(
                model=model,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
            metadata._experiment_tracker.set_experiment(
                experiment=global_experiment_name,
                backing_tensorboard=False,
                display_button=False,
            )
        elif self._experiment and not global_experiment_name:
            metadata._experiment_tracker.set_experiment(
                experiment=self._experiment, backing_tensorboard=False
            )
            eval_result = self._evaluate_with_experiment(
                model=model,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
            metadata._experiment_tracker.reset()
        elif not self._experiment and global_experiment_name:
            eval_result = self._evaluate_with_experiment(
                model=model,
                prompt_template=prompt_template,
                experiment_run_name=experiment_run_name,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
                output_file_name=output_file_name,
            )
        else:
            eval_result = _evaluation.evaluate(
                dataset=self.dataset,
                metrics=self.metrics,
                model=model,
                prompt_template=prompt_template,
                metric_column_mapping=self._metric_column_mapping,
                evaluation_service_qps=evaluation_service_qps,
                retry_timeout=retry_timeout,
            )

        candidate_model_name = None
        if isinstance(model, generative_models.GenerativeModel):
            candidate_model_name = model._model_name

        baseline_model_name = None
        pairwise_metrics = [
            metric
            for metric in self.metrics
            if isinstance(metric, pairwise_metric.PairwiseMetric)
        ]
        if pairwise_metrics:
            
            baseline_model = pairwise_metrics[0].baseline_model
            if isinstance(baseline_model, generative_models.GenerativeModel):
                baseline_model_name = baseline_model._model_name

        dataset_uri = None
        if isinstance(self._raw_dataset, str):
            dataset_uri = self._raw_dataset

        eval_utils.upload_evaluation_results(
            eval_result,
            self.output_uri_prefix,
            output_file_name,
            candidate_model_name,
            baseline_model_name,
            dataset_uri,
            self.metrics,
        )
        return eval_result

    def _validate_experiment_run(self) -> None:
        
        if metadata._experiment_tracker.experiment_run:
            raise ValueError(
                "Experiment run already exists. Please specify the name of the"
                " experiment run to assign current session within this evaluation."
            )

    def _log_eval_experiment_param(
        self,
        model: Optional[Union[GenerativeModel, Callable[[str], str]]] = None,
        prompt_template: Optional[str] = None,
        output_file_name: Optional[str] = None,
    ) -> None:
        
        eval_metadata = {}

        if prompt_template is not None:
            eval_metadata.update({"prompt_template": prompt_template})

        if isinstance(model, GenerativeModel):
            eval_metadata.update(
                {
                    "model_name": model._model_name,
                }
            )

            if model._generation_config and isinstance(model._generation_config, dict):
                eval_metadata.update(**model._generation_config)

            if model._safety_settings and isinstance(model._safety_settings, dict):
                safety_settings = model._safety_settings
                safety_settings_as_str = {
                    category.name: threshold.name
                    for category, threshold in safety_settings.items()
                }
                eval_metadata.update(safety_settings_as_str)

        if self.output_uri_prefix and output_file_name:
            eval_metadata.update(
                {"output_file": self.output_uri_prefix + "/" + output_file_name}
            )

        if eval_metadata:
            _LOGGER.info(f"Logging Eval Experiment metadata: {eval_metadata}")
            try:
                vertexai.preview.log_params(eval_metadata)
            except (ValueError, TypeError) as e:
                _LOGGER.warning(f"Experiment metadata logging failed: {str(e)}")

    def _verify_and_set_response_column_name(
        self, response_column_name: str, metric_column_mapping_key: str
    ) -> None:
        
        if response_column_name:
            if response_column_name in self._dataset.columns:
                self._metric_column_mapping[
                    metric_column_mapping_key
                ] = response_column_name
            else:
                raise ValueError(
                    f"(Baseline) Model response column {response_column_name} is not"
                    " found in the dataset."
                )

    def display_runs(self):
        
        if not self._experiment:
            raise ValueError("Experiment is not set.")
        elif IPython_display:
            IPython_display.display(
                vertexai.preview.get_experiment_df(self._experiment)
            )
