

from __future__ import annotations

from typing import Any, Callable, Optional, Protocol

from langchain_core._api import deprecated
from langchain_core.callbacks import Callbacks
from langchain_core.documents import Document
from pydantic import ConfigDict

from langchain.chains.combine_documents.base import BaseCombineDocumentsChain


class CombineDocsProtocol(Protocol):
    

    def __call__(self, docs: list[Document], **kwargs: Any) -> str:
        


class AsyncCombineDocsProtocol(Protocol):
    

    async def __call__(self, docs: list[Document], **kwargs: Any) -> str:
        


def split_list_of_docs(
    docs: list[Document],
    length_func: Callable,
    token_max: int,
    **kwargs: Any,
) -> list[list[Document]]:
    
    new_result_doc_list = []
    _sub_result_docs = []
    for doc in docs:
        _sub_result_docs.append(doc)
        _num_tokens = length_func(_sub_result_docs, **kwargs)
        if _num_tokens > token_max:
            if len(_sub_result_docs) == 1:
                msg = (
                    "A single document was longer than the context length,"
                    " we cannot handle this."
                )
                raise ValueError(msg)
            new_result_doc_list.append(_sub_result_docs[:-1])
            _sub_result_docs = _sub_result_docs[-1:]
    new_result_doc_list.append(_sub_result_docs)
    return new_result_doc_list


def collapse_docs(
    docs: list[Document],
    combine_document_func: CombineDocsProtocol,
    **kwargs: Any,
) -> Document:
    
    result = combine_document_func(docs, **kwargs)
    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}
    for doc in docs[1:]:
        for k, v in doc.metadata.items():
            if k in combined_metadata:
                combined_metadata[k] += f", {v}"
            else:
                combined_metadata[k] = str(v)
    return Document(page_content=result, metadata=combined_metadata)


async def acollapse_docs(
    docs: list[Document],
    combine_document_func: AsyncCombineDocsProtocol,
    **kwargs: Any,
) -> Document:
    
    result = await combine_document_func(docs, **kwargs)
    combined_metadata = {k: str(v) for k, v in docs[0].metadata.items()}
    for doc in docs[1:]:
        for k, v in doc.metadata.items():
            if k in combined_metadata:
                combined_metadata[k] += f", {v}"
            else:
                combined_metadata[k] = str(v)
    return Document(page_content=result, metadata=combined_metadata)


@deprecated(
    since="0.3.1",
    removal="1.0",
    message=(
        "This class is deprecated. Please see the migration guide here for "
        "a recommended replacement: "
        "https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/"
    ),
)
class ReduceDocumentsChain(BaseCombineDocumentsChain):
    

    combine_documents_chain: BaseCombineDocumentsChain
    
    collapse_documents_chain: Optional[BaseCombineDocumentsChain] = None
    
    token_max: int = 3000
    
    collapse_max_retries: Optional[int] = None
    

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="forbid",
    )

    @property
    def _collapse_chain(self) -> BaseCombineDocumentsChain:
        if self.collapse_documents_chain is not None:
            return self.collapse_documents_chain
        return self.combine_documents_chain

    def combine_docs(
        self,
        docs: list[Document],
        token_max: Optional[int] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> tuple[str, dict]:
        
        result_docs, extra_return_dict = self._collapse(
            docs,
            token_max=token_max,
            callbacks=callbacks,
            **kwargs,
        )
        return self.combine_documents_chain.combine_docs(
            docs=result_docs,
            callbacks=callbacks,
            **kwargs,
        )

    async def acombine_docs(
        self,
        docs: list[Document],
        token_max: Optional[int] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> tuple[str, dict]:
        
        result_docs, extra_return_dict = await self._acollapse(
            docs,
            token_max=token_max,
            callbacks=callbacks,
            **kwargs,
        )
        return await self.combine_documents_chain.acombine_docs(
            docs=result_docs,
            callbacks=callbacks,
            **kwargs,
        )

    def _collapse(
        self,
        docs: list[Document],
        token_max: Optional[int] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> tuple[list[Document], dict]:
        result_docs = docs
        length_func = self.combine_documents_chain.prompt_length
        num_tokens = length_func(result_docs, **kwargs)

        def _collapse_docs_func(docs: list[Document], **kwargs: Any) -> str:
            return self._collapse_chain.run(
                input_documents=docs,
                callbacks=callbacks,
                **kwargs,
            )

        _token_max = token_max or self.token_max
        retries: int = 0
        while num_tokens is not None and num_tokens > _token_max:
            new_result_doc_list = split_list_of_docs(
                result_docs,
                length_func,
                _token_max,
                **kwargs,
            )
            result_docs = [
                collapse_docs(docs_, _collapse_docs_func, **kwargs)
                for docs_ in new_result_doc_list
            ]
            num_tokens = length_func(result_docs, **kwargs)
            retries += 1
            if self.collapse_max_retries and retries == self.collapse_max_retries:
                msg = f"Exceed {self.collapse_max_retries} tries to \
                        collapse document to {_token_max} tokens."
                raise ValueError(msg)
        return result_docs, {}

    async def _acollapse(
        self,
        docs: list[Document],
        token_max: Optional[int] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> tuple[list[Document], dict]:
        result_docs = docs
        length_func = self.combine_documents_chain.prompt_length
        num_tokens = length_func(result_docs, **kwargs)

        async def _collapse_docs_func(docs: list[Document], **kwargs: Any) -> str:
            return await self._collapse_chain.arun(
                input_documents=docs,
                callbacks=callbacks,
                **kwargs,
            )

        _token_max = token_max or self.token_max
        retries: int = 0
        while num_tokens is not None and num_tokens > _token_max:
            new_result_doc_list = split_list_of_docs(
                result_docs,
                length_func,
                _token_max,
                **kwargs,
            )
            result_docs = [
                await acollapse_docs(docs_, _collapse_docs_func, **kwargs)
                for docs_ in new_result_doc_list
            ]
            num_tokens = length_func(result_docs, **kwargs)
            retries += 1
            if self.collapse_max_retries and retries == self.collapse_max_retries:
                msg = f"Exceed {self.collapse_max_retries} tries to \
                        collapse document to {_token_max} tokens."
                raise ValueError(msg)
        return result_docs, {}

    @property
    def _chain_type(self) -> str:
        return "reduce_documents_chain"
