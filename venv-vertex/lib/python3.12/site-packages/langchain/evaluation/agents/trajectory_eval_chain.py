

import re
from collections.abc import Sequence
from typing import (
    Any,
    Optional,
    TypedDict,
    Union,
    cast,
)

from langchain_core.agents import AgentAction
from langchain_core.callbacks import Callbacks
from langchain_core.callbacks.manager import (
    AsyncCallbackManagerForChainRun,
    CallbackManagerForChainRun,
)
from langchain_core.exceptions import OutputParserException
from langchain_core.language_models import BaseLanguageModel
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.output_parsers import BaseOutputParser
from langchain_core.tools import BaseTool
from pydantic import ConfigDict, Field
from typing_extensions import override

from langchain.chains.llm import LLMChain
from langchain.evaluation.agents.trajectory_eval_prompt import (
    EVAL_CHAT_PROMPT,
    TOOL_FREE_EVAL_CHAT_PROMPT,
)
from langchain.evaluation.schema import AgentTrajectoryEvaluator, LLMEvalChain

_MAX_SCORE = 5


class TrajectoryEval(TypedDict):
    

    score: float
    
    reasoning: str
    


class TrajectoryOutputParser(BaseOutputParser):
    

    @property
    def _type(self) -> str:
        return "agent_trajectory"

    def parse(self, text: str) -> TrajectoryEval:
        
        if "Score:" not in text:
            msg = f"Could not find score in model eval output: {text}"
            raise OutputParserException(msg)

        reasoning, score_str = text.split("Score: ", maxsplit=1)

        reasoning, score_str = reasoning.strip(), score_str.strip()

        
        
        
        
        
        _score = re.search(r"(\d+(\.\d+)?)", score_str)
        
        if _score is None or "." in _score.group(1):
            msg = f"Score is not an integer digit in the range 1-5: {text}"
            raise OutputParserException(msg)
        score = int(_score.group(1))
        
        if not 1 <= score <= _MAX_SCORE:
            msg = f"Score is not a digit in the range 1-5: {text}"
            raise OutputParserException(msg)
        normalized_score = (score - 1) / (_MAX_SCORE - 1)
        return TrajectoryEval(score=normalized_score, reasoning=reasoning)


class TrajectoryEvalChain(AgentTrajectoryEvaluator, LLMEvalChain):
    

    agent_tools: Optional[list[BaseTool]] = None
    
    eval_chain: LLMChain
    
    output_parser: TrajectoryOutputParser = Field(
        default_factory=TrajectoryOutputParser,
    )
    
    return_reasoning: bool = False  
    

    model_config = ConfigDict(
        extra="ignore",
    )

    @property
    def requires_reference(self) -> bool:
        
        return False

    @property
    def _tools_description(self) -> str:
        
        if self.agent_tools is None:
            return ""
        return "\n\n".join(
            [
                f
                for i, tool in enumerate(self.agent_tools, 1)
            ],
        )

    @staticmethod
    def get_agent_trajectory(
        steps: Union[str, Sequence[tuple[AgentAction, str]]],
    ) -> str:
        
        if isinstance(steps, str):
            return steps

        return "\n\n".join(
            [
                f
                for i, (action, output) in enumerate(steps, 1)
            ],
        )

    @staticmethod
    def _format_reference(reference: Optional[str]) -> str:
        
        if not reference:
            return ""
        return f

    @classmethod
    def from_llm(
        cls,
        llm: BaseLanguageModel,
        agent_tools: Optional[Sequence[BaseTool]] = None,
        output_parser: Optional[TrajectoryOutputParser] = None,
        **kwargs: Any,
    ) -> "TrajectoryEvalChain":
        
        if not isinstance(llm, BaseChatModel):
            msg = "Only chat models supported by the current trajectory eval"
            raise NotImplementedError(msg)
        prompt = EVAL_CHAT_PROMPT if agent_tools else TOOL_FREE_EVAL_CHAT_PROMPT
        eval_chain = LLMChain(llm=llm, prompt=prompt)
        return cls(
            agent_tools=agent_tools,  
            eval_chain=eval_chain,
            output_parser=output_parser or TrajectoryOutputParser(),
            **kwargs,
        )

    @property
    def input_keys(self) -> list[str]:
        
        return ["question", "agent_trajectory", "answer", "reference"]

    @property
    def output_keys(self) -> list[str]:
        
        return ["score", "reasoning"]

    def prep_inputs(self, inputs: Union[dict[str, Any], Any]) -> dict[str, str]:
        
        inputs["reference"] = self._format_reference(inputs.get("reference"))
        return super().prep_inputs(inputs)

    def _call(
        self,
        inputs: dict[str, str],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> dict[str, Any]:
        
        chain_input = {**inputs}
        if self.agent_tools:
            chain_input["tool_descriptions"] = self._tools_description
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        raw_output = self.eval_chain.run(
            chain_input,
            callbacks=_run_manager.get_child(),
        )
        return cast(dict, self.output_parser.parse(raw_output))

    async def _acall(
        self,
        inputs: dict[str, str],
        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,
    ) -> dict[str, Any]:
        
        chain_input = {**inputs}
        if self.agent_tools:
            chain_input["tool_descriptions"] = self._tools_description
        _run_manager = run_manager or AsyncCallbackManagerForChainRun.get_noop_manager()
        raw_output = await self.eval_chain.arun(
            chain_input,
            callbacks=_run_manager.get_child(),
        )
        return cast(dict, self.output_parser.parse(raw_output))

    @override
    def _evaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        input: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        reference: Optional[str] = None,
        callbacks: Callbacks = None,
        tags: Optional[list[str]] = None,
        metadata: Optional[dict[str, Any]] = None,
        include_run_info: bool = False,
        **kwargs: Any,
    ) -> dict:
        
        inputs = {
            "question": input,
            "agent_trajectory": self.get_agent_trajectory(agent_trajectory),
            "answer": prediction,
            "reference": reference,
        }
        return self.__call__(
            inputs=inputs,
            callbacks=callbacks,
            tags=tags,
            metadata=metadata,
            include_run_info=include_run_info,
            return_only_outputs=True,
        )

    @override
    async def _aevaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        input: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        reference: Optional[str] = None,
        callbacks: Callbacks = None,
        tags: Optional[list[str]] = None,
        metadata: Optional[dict[str, Any]] = None,
        include_run_info: bool = False,
        **kwargs: Any,
    ) -> dict:
        
        inputs = {
            "question": input,
            "agent_trajectory": self.get_agent_trajectory(agent_trajectory),
            "answer": prediction,
            "reference": reference,
        }
        return await self.acall(
            inputs=inputs,
            callbacks=callbacks,
            tags=tags,
            metadata=metadata,
            include_run_info=include_run_info,
            return_only_outputs=True,
        )
