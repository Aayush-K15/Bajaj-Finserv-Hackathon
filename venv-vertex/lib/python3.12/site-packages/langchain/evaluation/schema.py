

from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from collections.abc import Sequence
from enum import Enum
from typing import Any, Optional, Union
from warnings import warn

from langchain_core.agents import AgentAction
from langchain_core.language_models import BaseLanguageModel
from langchain_core.runnables.config import run_in_executor

from langchain.chains.base import Chain

logger = logging.getLogger(__name__)


class EvaluatorType(str, Enum):
    

    QA = "qa"
    
    COT_QA = "cot_qa"
    
    CONTEXT_QA = "context_qa"
    
    PAIRWISE_STRING = "pairwise_string"
    
    SCORE_STRING = "score_string"
    
    LABELED_PAIRWISE_STRING = "labeled_pairwise_string"
    
    LABELED_SCORE_STRING = "labeled_score_string"
    
    AGENT_TRAJECTORY = "trajectory"
    
    CRITERIA = "criteria"
    
    LABELED_CRITERIA = "labeled_criteria"
    
    STRING_DISTANCE = "string_distance"
    
    EXACT_MATCH = "exact_match"
    
    REGEX_MATCH = "regex_match"
    
    PAIRWISE_STRING_DISTANCE = "pairwise_string_distance"
    
    EMBEDDING_DISTANCE = "embedding_distance"
    
    PAIRWISE_EMBEDDING_DISTANCE = "pairwise_embedding_distance"
    
    JSON_VALIDITY = "json_validity"
    
    JSON_EQUALITY = "json_equality"
    
    JSON_EDIT_DISTANCE = "json_edit_distance"
    
    JSON_SCHEMA_VALIDATION = "json_schema_validation"
    


class LLMEvalChain(Chain):
    

    @classmethod
    @abstractmethod
    def from_llm(cls, llm: BaseLanguageModel, **kwargs: Any) -> LLMEvalChain:
        


class _EvalArgsMixin:
    

    @property
    def requires_reference(self) -> bool:
        
        return False

    @property
    def requires_input(self) -> bool:
        
        return False

    @property
    def _skip_input_warning(self) -> str:
        
        return f"Ignoring input in {self.__class__.__name__}, as it is not expected."

    @property
    def _skip_reference_warning(self) -> str:
        
        return (
            f"Ignoring reference in {self.__class__.__name__}, as it is not expected."
        )

    def _check_evaluation_args(
        self,
        reference: Optional[str] = None,
        input_: Optional[str] = None,
    ) -> None:
        
        if self.requires_input and input_ is None:
            msg = f"{self.__class__.__name__} requires an input string."
            raise ValueError(msg)
        if input_ is not None and not self.requires_input:
            warn(self._skip_input_warning, stacklevel=3)
        if self.requires_reference and reference is None:
            msg = f"{self.__class__.__name__} requires a reference string."
            raise ValueError(msg)
        if reference is not None and not self.requires_reference:
            warn(self._skip_reference_warning, stacklevel=3)


class StringEvaluator(_EvalArgsMixin, ABC):
    

    @property
    def evaluation_name(self) -> str:
        
        return self.__class__.__name__

    @property
    def requires_reference(self) -> bool:
        
        return False

    @abstractmethod
    def _evaluate_strings(
        self,
        *,
        prediction: Union[str, Any],
        reference: Optional[Union[str, Any]] = None,
        input: Optional[Union[str, Any]] = None,  
        **kwargs: Any,
    ) -> dict:
          

    async def _aevaluate_strings(
        self,
        *,
        prediction: Union[str, Any],
        reference: Optional[Union[str, Any]] = None,
        input: Optional[Union[str, Any]] = None,  
        **kwargs: Any,
    ) -> dict:
          
        return await run_in_executor(
            None,
            self._evaluate_strings,
            prediction=prediction,
            reference=reference,
            input=input,
            **kwargs,
        )

    def evaluate_strings(
        self,
        *,
        prediction: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          
        self._check_evaluation_args(reference=reference, input_=input)
        return self._evaluate_strings(
            prediction=prediction,
            reference=reference,
            input=input,
            **kwargs,
        )

    async def aevaluate_strings(
        self,
        *,
        prediction: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          
        self._check_evaluation_args(reference=reference, input_=input)
        return await self._aevaluate_strings(
            prediction=prediction,
            reference=reference,
            input=input,
            **kwargs,
        )


class PairwiseStringEvaluator(_EvalArgsMixin, ABC):
    

    @abstractmethod
    def _evaluate_string_pairs(
        self,
        *,
        prediction: str,
        prediction_b: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          

    async def _aevaluate_string_pairs(
        self,
        *,
        prediction: str,
        prediction_b: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          
        return await run_in_executor(
            None,
            self._evaluate_string_pairs,
            prediction=prediction,
            prediction_b=prediction_b,
            reference=reference,
            input=input,
            **kwargs,
        )

    def evaluate_string_pairs(
        self,
        *,
        prediction: str,
        prediction_b: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          
        self._check_evaluation_args(reference=reference, input_=input)
        return self._evaluate_string_pairs(
            prediction=prediction,
            prediction_b=prediction_b,
            reference=reference,
            input=input,
            **kwargs,
        )

    async def aevaluate_string_pairs(
        self,
        *,
        prediction: str,
        prediction_b: str,
        reference: Optional[str] = None,
        input: Optional[str] = None,  
        **kwargs: Any,
    ) -> dict:
          
        self._check_evaluation_args(reference=reference, input_=input)
        return await self._aevaluate_string_pairs(
            prediction=prediction,
            prediction_b=prediction_b,
            reference=reference,
            input=input,
            **kwargs,
        )


class AgentTrajectoryEvaluator(_EvalArgsMixin, ABC):
    

    @property
    def requires_input(self) -> bool:
        
        return True

    @abstractmethod
    def _evaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        input: str,  
        reference: Optional[str] = None,
        **kwargs: Any,
    ) -> dict:
        

    async def _aevaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        input: str,  
        reference: Optional[str] = None,
        **kwargs: Any,
    ) -> dict:
        
        return await run_in_executor(
            None,
            self._evaluate_agent_trajectory,
            prediction=prediction,
            agent_trajectory=agent_trajectory,
            reference=reference,
            input=input,
            **kwargs,
        )

    def evaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        input: str,  
        reference: Optional[str] = None,
        **kwargs: Any,
    ) -> dict:
        
        self._check_evaluation_args(reference=reference, input_=input)
        return self._evaluate_agent_trajectory(
            prediction=prediction,
            input=input,
            agent_trajectory=agent_trajectory,
            reference=reference,
            **kwargs,
        )

    async def aevaluate_agent_trajectory(
        self,
        *,
        prediction: str,
        agent_trajectory: Sequence[tuple[AgentAction, str]],
        input: str,  
        reference: Optional[str] = None,
        **kwargs: Any,
    ) -> dict:
        
        self._check_evaluation_args(reference=reference, input_=input)
        return await self._aevaluate_agent_trajectory(
            prediction=prediction,
            input=input,
            agent_trajectory=agent_trajectory,
            reference=reference,
            **kwargs,
        )
