

from __future__ import annotations

import sys
from typing import TYPE_CHECKING, Any

from typing_extensions import override

from langchain_core.callbacks.base import BaseCallbackHandler

if TYPE_CHECKING:
    from langchain_core.agents import AgentAction, AgentFinish
    from langchain_core.messages import BaseMessage
    from langchain_core.outputs import LLMResult


class StreamingStdOutCallbackHandler(BaseCallbackHandler):
    

    def on_llm_start(
        self, serialized: dict[str, Any], prompts: list[str], **kwargs: Any
    ) -> None:
        

    def on_chat_model_start(
        self,
        serialized: dict[str, Any],
        messages: list[list[BaseMessage]],
        **kwargs: Any,
    ) -> None:
        

    @override
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        
        sys.stdout.write(token)
        sys.stdout.flush()

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        

    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_chain_start(
        self, serialized: dict[str, Any], inputs: dict[str, Any], **kwargs: Any
    ) -> None:
        

    def on_chain_end(self, outputs: dict[str, Any], **kwargs: Any) -> None:
        

    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_tool_start(
        self, serialized: dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        

    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
        

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_text(self, text: str, **kwargs: Any) -> None:
        

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        
