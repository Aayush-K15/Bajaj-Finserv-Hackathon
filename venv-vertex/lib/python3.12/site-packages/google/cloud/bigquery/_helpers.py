















import base64
import datetime
import decimal
import json
import math
import re
import os
import textwrap
import warnings
from typing import Any, Optional, Tuple, Type, Union

from dateutil import relativedelta
from google.cloud._helpers import UTC  
from google.cloud._helpers import _date_from_iso8601_date
from google.cloud._helpers import _datetime_from_microseconds
from google.cloud._helpers import _RFC3339_MICROS
from google.cloud._helpers import _RFC3339_NO_FRACTION
from google.cloud._helpers import _to_bytes
from google.auth import credentials as ga_credentials  
from google.api_core import client_options as client_options_lib

TimeoutType = Union[float, None]

_RFC3339_MICROS_NO_ZULU = "%Y-%m-%dT%H:%M:%S.%f"
_TIMEONLY_WO_MICROS = "%H:%M:%S"
_TIMEONLY_W_MICROS = "%H:%M:%S.%f"
_PROJECT_PREFIX_PATTERN = re.compile(
    r,
    re.VERBOSE,
)



_INTERVAL_PATTERN = re.compile(
    r"(?P<calendar_sign>-?)(?P<years>\d+)-(?P<months>\d+) "
    r"(?P<days>-?\d+) "
    r"(?P<time_sign>-?)(?P<hours>\d+):(?P<minutes>\d+):(?P<seconds>\d+)\.?(?P<fraction>\d*)?$"
)
_RANGE_PATTERN = re.compile(r"\[.*, .*\)")

BIGQUERY_EMULATOR_HOST = "BIGQUERY_EMULATOR_HOST"


_DEFAULT_HOST = "https://bigquery.googleapis.com"


_DEFAULT_HOST_TEMPLATE = "https://bigquery.{UNIVERSE_DOMAIN}"


_DEFAULT_UNIVERSE = "googleapis.com"


_UNIVERSE_DOMAIN_ENV = "GOOGLE_CLOUD_UNIVERSE_DOMAIN"


_SUPPORTED_RANGE_ELEMENTS = {"TIMESTAMP", "DATETIME", "DATE"}


def _get_client_universe(
    client_options: Optional[Union[client_options_lib.ClientOptions, dict]]
) -> str:
    
    if isinstance(client_options, dict):
        client_options = client_options_lib.from_dict(client_options)
    universe = _DEFAULT_UNIVERSE
    options_universe = getattr(client_options, "universe_domain", None)
    if (
        options_universe
        and isinstance(options_universe, str)
        and len(options_universe) > 0
    ):
        universe = options_universe
    else:
        env_universe = os.getenv(_UNIVERSE_DOMAIN_ENV)
        if isinstance(env_universe, str) and len(env_universe) > 0:
            universe = env_universe
    return universe


def _validate_universe(client_universe: str, credentials: ga_credentials.Credentials):
    
    if hasattr(credentials, "universe_domain"):
        cred_universe = getattr(credentials, "universe_domain")
        if isinstance(cred_universe, str):
            if client_universe != cred_universe:
                raise ValueError(
                    "The configured universe domain "
                    f"({client_universe}) does not match the universe domain "
                    f"found in the credentials ({cred_universe}). "
                    "If you haven't configured the universe domain explicitly, "
                    f"`{_DEFAULT_UNIVERSE}` is the default."
                )


def _get_bigquery_host():
    return os.environ.get(BIGQUERY_EMULATOR_HOST, _DEFAULT_HOST)


def _not_null(value, field):
    
    return value is not None or (field is not None and field.mode != "NULLABLE")


class CellDataParser:
    

    def to_py(self, resource, field):
        def default_converter(value, field):
            _warn_unknown_field_type(field)
            return value

        converter = getattr(
            self, f"{field.field_type.lower()}_to_py", default_converter
        )
        if field.mode == "REPEATED":
            return [converter(item["v"], field) for item in resource]
        else:
            return converter(resource, field)

    def bool_to_py(self, value, field):
        
        if _not_null(value, field):
            
            
            if value is None:
                raise TypeError(f"got None for required boolean field {field}")
            return value.lower() in ("t", "true", "1")

    def boolean_to_py(self, value, field):
        
        return self.bool_to_py(value, field)

    def integer_to_py(self, value, field):
        
        if _not_null(value, field):
            return int(value)

    def int64_to_py(self, value, field):
        
        return self.integer_to_py(value, field)

    def interval_to_py(
        self, value: Optional[str], field
    ) -> Optional[relativedelta.relativedelta]:
        
        if not _not_null(value, field):
            return None
        if value is None:
            raise TypeError(f"got {value} for REQUIRED field: {repr(field)}")

        parsed = _INTERVAL_PATTERN.match(value)
        if parsed is None:
            raise ValueError(
                textwrap.dedent(
                    f
                ),
            )

        calendar_sign = -1 if parsed.group("calendar_sign") == "-" else 1
        years = calendar_sign * int(parsed.group("years"))
        months = calendar_sign * int(parsed.group("months"))
        days = int(parsed.group("days"))
        time_sign = -1 if parsed.group("time_sign") == "-" else 1
        hours = time_sign * int(parsed.group("hours"))
        minutes = time_sign * int(parsed.group("minutes"))
        seconds = time_sign * int(parsed.group("seconds"))
        fraction = parsed.group("fraction")
        microseconds = time_sign * int(fraction.ljust(6, "0")[:6]) if fraction else 0

        return relativedelta.relativedelta(
            years=years,
            months=months,
            days=days,
            hours=hours,
            minutes=minutes,
            seconds=seconds,
            microseconds=microseconds,
        )

    def float_to_py(self, value, field):
        
        if _not_null(value, field):
            return float(value)

    def float64_to_py(self, value, field):
        
        return self.float_to_py(value, field)

    def numeric_to_py(self, value, field):
        
        if _not_null(value, field):
            return decimal.Decimal(value)

    def bignumeric_to_py(self, value, field):
        
        return self.numeric_to_py(value, field)

    def string_to_py(self, value, _):
        
        return value

    def geography_to_py(self, value, _):
        
        return value

    def bytes_to_py(self, value, field):
        
        if _not_null(value, field):
            return base64.standard_b64decode(_to_bytes(value))

    def timestamp_to_py(self, value, field):
        
        if _not_null(value, field):
            
            return _datetime_from_microseconds(int(value))

    def datetime_to_py(self, value, field):
        
        if _not_null(value, field):
            if "." in value:
                
                return datetime.datetime.strptime(value, _RFC3339_MICROS_NO_ZULU)
            else:
                
                return datetime.datetime.strptime(value, _RFC3339_NO_FRACTION)
        else:
            return None

    def date_to_py(self, value, field):
        
        if _not_null(value, field):
            
            return _date_from_iso8601_date(value)

    def time_to_py(self, value, field):
        
        if _not_null(value, field):
            if len(value) == 8:  
                fmt = _TIMEONLY_WO_MICROS
            elif len(value) == 15:  
                fmt = _TIMEONLY_W_MICROS
            else:
                raise ValueError(
                    textwrap.dedent(
                        f
                    ),
                )
            return datetime.datetime.strptime(value, fmt).time()

    def record_to_py(self, value, field):
        
        if _not_null(value, field):
            record = {}
            record_iter = zip(field.fields, value["f"])
            for subfield, cell in record_iter:
                record[subfield.name] = self.to_py(cell["v"], subfield)
            return record

    def struct_to_py(self, value, field):
        
        return self.record_to_py(value, field)

    def json_to_py(self, value, field):
        
        if _not_null(value, field):
            return json.loads(value)
        else:
            return None

    def _range_element_to_py(self, value, field_element_type):
        
        
        from google.cloud.bigquery import schema

        if value == "UNBOUNDED":
            return None
        if field_element_type.element_type in _SUPPORTED_RANGE_ELEMENTS:
            return self.to_py(
                value,
                schema.SchemaField("placeholder", field_element_type.element_type),
            )
        else:
            raise ValueError(
                textwrap.dedent(
                    f
                ),
            )

    def range_to_py(self, value, field):
        
        if _not_null(value, field):
            if _RANGE_PATTERN.match(value):
                start, end = value[1:-1].split(", ")
                start = self._range_element_to_py(start, field.range_element_type)
                end = self._range_element_to_py(end, field.range_element_type)
                return {"start": start, "end": end}
            else:
                raise ValueError(
                    textwrap.dedent(
                        f
                    ),
                )


CELL_DATA_PARSER = CellDataParser()


class DataFrameCellDataParser(CellDataParser):
    

    def json_to_py(self, value, _):
        
        return value


DATA_FRAME_CELL_DATA_PARSER = DataFrameCellDataParser()


class ScalarQueryParamParser(CellDataParser):
    

    def timestamp_to_py(self, value, field):
        
        if _not_null(value, field):
            
            
            
            value = value.replace(" ", "T", 1)
            
            value = value.replace("Z", "")
            value = value.replace("+00:00", "")

            if "." in value:
                
                return datetime.datetime.strptime(
                    value, _RFC3339_MICROS_NO_ZULU
                ).replace(tzinfo=UTC)
            else:
                
                return datetime.datetime.strptime(value, _RFC3339_NO_FRACTION).replace(
                    tzinfo=UTC
                )
        else:
            return None


SCALAR_QUERY_PARAM_PARSER = ScalarQueryParamParser()


def _field_to_index_mapping(schema):
    
    return {f.name: i for i, f in enumerate(schema)}


def _row_tuple_from_json(row, schema):
    
    from google.cloud.bigquery.schema import _to_schema_fields

    schema = _to_schema_fields(schema)

    row_data = []
    for field, cell in zip(schema, row["f"]):
        row_data.append(CELL_DATA_PARSER.to_py(cell["v"], field))
    return tuple(row_data)


def _rows_from_json(values, schema):
    
    from google.cloud.bigquery import Row
    from google.cloud.bigquery.schema import _to_schema_fields

    schema = _to_schema_fields(schema)
    field_to_index = _field_to_index_mapping(schema)
    return [Row(_row_tuple_from_json(r, schema), field_to_index) for r in values]


def _int_to_json(value):
    
    if isinstance(value, int):
        value = str(value)
    return value


def _float_to_json(value) -> Union[None, str, float]:
    
    if value is None:
        return None

    if isinstance(value, str):
        value = float(value)

    return str(value) if (math.isnan(value) or math.isinf(value)) else float(value)


def _decimal_to_json(value):
    
    if isinstance(value, decimal.Decimal):
        value = str(value)
    return value


def _bool_to_json(value):
    
    if isinstance(value, bool):
        value = "true" if value else "false"
    return value


def _bytes_to_json(value):
    
    if isinstance(value, bytes):
        value = base64.standard_b64encode(value).decode("ascii")
    return value


def _json_to_json(value):
    
    if value is None:
        return None
    return json.dumps(value)


def _string_to_json(value):
    
    return value


def _timestamp_to_json_parameter(value):
    
    if isinstance(value, datetime.datetime):
        if value.tzinfo not in (None, UTC):
            
            value = value.replace(tzinfo=None) - value.utcoffset()
        value = "%s %s+00:00" % (value.date().isoformat(), value.time().isoformat())
    return value


def _timestamp_to_json_row(value):
    
    if isinstance(value, datetime.datetime):
        
        
        if value.tzinfo is not None:
            value = value.astimezone(UTC)
        value = value.strftime(_RFC3339_MICROS)
    return value


def _datetime_to_json(value):
    
    if isinstance(value, datetime.datetime):
        
        
        if value.tzinfo is not None:
            value = value.astimezone(UTC)
        value = value.strftime(_RFC3339_MICROS_NO_ZULU)
    return value


def _date_to_json(value):
    
    if isinstance(value, datetime.date):
        value = value.isoformat()
    return value


def _time_to_json(value):
    
    if isinstance(value, datetime.time):
        value = value.isoformat()
    return value


def _range_element_to_json(value, element_type=None):
    
    if value is None:
        return None
    elif isinstance(value, str):
        if value.upper() in ("UNBOUNDED", "NULL"):
            return None
        else:
            
            
            return value
    elif (
        element_type and element_type.element_type.upper() in _SUPPORTED_RANGE_ELEMENTS
    ):
        converter = _SCALAR_VALUE_TO_JSON_ROW.get(element_type.element_type.upper())
        return converter(value)
    else:
        raise ValueError(
            f"Unsupported RANGE element type {element_type}, or "
            "element type is empty. Must be DATE, DATETIME, or "
            "TIMESTAMP"
        )


def _range_field_to_json(range_element_type, value):
    
    if isinstance(value, str):
        
        if _RANGE_PATTERN.match(value):
            start, end = value[1:-1].split(", ")
        else:
            raise ValueError(f"RANGE literal {value} has incorrect format")
    elif isinstance(value, dict):
        
        start = value.get("start")
        end = value.get("end")
    else:
        raise ValueError(
            f"Unsupported type of RANGE value {value}, must be " "string or dict"
        )

    start = _range_element_to_json(start, range_element_type)
    end = _range_element_to_json(end, range_element_type)
    return {"start": start, "end": end}




_SCALAR_VALUE_TO_JSON_ROW = {
    "INTEGER": _int_to_json,
    "INT64": _int_to_json,
    "FLOAT": _float_to_json,
    "FLOAT64": _float_to_json,
    "NUMERIC": _decimal_to_json,
    "BIGNUMERIC": _decimal_to_json,
    "BOOLEAN": _bool_to_json,
    "BOOL": _bool_to_json,
    "BYTES": _bytes_to_json,
    "TIMESTAMP": _timestamp_to_json_row,
    "DATETIME": _datetime_to_json,
    "DATE": _date_to_json,
    "TIME": _time_to_json,
    "JSON": _json_to_json,
    "STRING": _string_to_json,
    
    
    
    "DECIMAL": _decimal_to_json,
    "BIGDECIMAL": _decimal_to_json,
}



_SCALAR_VALUE_TO_JSON_PARAM = _SCALAR_VALUE_TO_JSON_ROW.copy()
_SCALAR_VALUE_TO_JSON_PARAM["TIMESTAMP"] = _timestamp_to_json_parameter


def _warn_unknown_field_type(field):
    warnings.warn(
        "Unknown type '{}' for field '{}'. Behavior reading and writing this type is not officially supported and may change in the future.".format(
            field.field_type, field.name
        ),
        FutureWarning,
    )


def _scalar_field_to_json(field, row_value):
    

    def default_converter(value):
        _warn_unknown_field_type(field)
        return value

    converter = _SCALAR_VALUE_TO_JSON_ROW.get(field.field_type, default_converter)
    return converter(row_value)


def _repeated_field_to_json(field, row_value):
    
    values = []
    for item in row_value:
        values.append(_single_field_to_json(field, item))
    return values


def _record_field_to_json(fields, row_value):
    
    isdict = isinstance(row_value, dict)

    
    
    
    
    if not isdict and len(row_value) != len(fields):
        msg = "The number of row fields ({}) does not match schema length ({}).".format(
            len(row_value), len(fields)
        )
        raise ValueError(msg)

    record = {}

    if isdict:
        processed_fields = set()

    for subindex, subfield in enumerate(fields):
        subname = subfield.name
        subvalue = row_value.get(subname) if isdict else row_value[subindex]

        
        if subvalue is not None:
            record[subname] = _field_to_json(subfield, subvalue)

        if isdict:
            processed_fields.add(subname)

    
    
    
    if isdict:
        not_processed = set(row_value.keys()) - processed_fields

        for field_name in not_processed:
            value = row_value[field_name]
            if value is not None:
                record[field_name] = str(value)

    return record


def _single_field_to_json(field, row_value):
    
    if row_value is None:
        return None

    if field.field_type == "RECORD":
        return _record_field_to_json(field.fields, row_value)
    if field.field_type == "RANGE":
        return _range_field_to_json(field.range_element_type, row_value)

    return _scalar_field_to_json(field, row_value)


def _field_to_json(field, row_value):
    
    if row_value is None:
        return None

    if field.mode == "REPEATED":
        return _repeated_field_to_json(field, row_value)

    return _single_field_to_json(field, row_value)


def _snake_to_camel_case(value):
    
    words = value.split("_")
    return words[0] + "".join(map(str.capitalize, words[1:]))


def _get_sub_prop(container, keys, default=None):
    
    if isinstance(keys, str):
        keys = [keys]

    sub_val = container
    for key in keys:
        if key not in sub_val:
            return default
        sub_val = sub_val[key]
    return sub_val


def _set_sub_prop(container, keys, value):
    
    if isinstance(keys, str):
        keys = [keys]

    sub_val = container
    for key in keys[:-1]:
        if key not in sub_val:
            sub_val[key] = {}
        sub_val = sub_val[key]
    sub_val[keys[-1]] = value


def _del_sub_prop(container, keys):
    
    sub_val = container
    for key in keys[:-1]:
        if key not in sub_val:
            sub_val[key] = {}
        sub_val = sub_val[key]
    if keys[-1] in sub_val:
        del sub_val[keys[-1]]


def _int_or_none(value):
    
    if isinstance(value, int):
        return value
    if value is not None:
        return int(value)


def _str_or_none(value):
    
    if value is not None:
        return str(value)


def _split_id(full_id):
    
    with_prefix = _PROJECT_PREFIX_PATTERN.match(full_id)
    if with_prefix is None:
        parts = full_id.split(".")
    else:
        parts = with_prefix.groups()
        parts = [part for part in parts if part]
    return parts


def _parse_3_part_id(full_id, default_project=None, property_name="table_id"):
    output_project_id = default_project
    output_dataset_id = None
    output_resource_id = None
    parts = _split_id(full_id)

    if len(parts) != 2 and len(parts) != 3:
        raise ValueError(
            "{property_name} must be a fully-qualified ID in "
            'standard SQL format, e.g., "project.dataset.{property_name}", '
            "got {}".format(full_id, property_name=property_name)
        )

    if len(parts) == 2 and not default_project:
        raise ValueError(
            "When default_project is not set, {property_name} must be a "
            "fully-qualified ID in standard SQL format, "
            'e.g., "project.dataset_id.{property_name}", got {}'.format(
                full_id, property_name=property_name
            )
        )

    if len(parts) == 2:
        output_dataset_id, output_resource_id = parts
    else:
        output_project_id, output_dataset_id, output_resource_id = parts

    return output_project_id, output_dataset_id, output_resource_id


def _build_resource_from_properties(obj, filter_fields):
    
    partial = {}
    for filter_field in filter_fields:
        api_field = _get_sub_prop(obj._PROPERTY_TO_API_FIELD, filter_field)
        if api_field is None and filter_field not in obj._properties:
            raise ValueError("No property %s" % filter_field)
        elif api_field is not None:
            _set_sub_prop(partial, api_field, _get_sub_prop(obj._properties, api_field))
        else:
            
            
            partial[filter_field] = obj._properties[filter_field]

    return partial


def _verify_job_config_type(job_config, expected_type, param_name="job_config"):
    if not isinstance(job_config, expected_type):
        msg = (
            "Expected an instance of {expected_type} class for the {param_name} parameter, "
            "but received {param_name} = {job_config}"
        )
        raise TypeError(
            msg.format(
                expected_type=expected_type.__name__,
                param_name=param_name,
                job_config=job_config,
            )
        )


def _isinstance_or_raise(
    value: Any,
    dtype: Union[Type, Tuple[Type, ...]],
    none_allowed: Optional[bool] = False,
) -> Any:
    
    if none_allowed and value is None:
        return value

    if isinstance(value, dtype):
        return value

    or_none = ""
    if none_allowed:
        or_none = " (or None)"

    msg = f"Pass {value} as a '{dtype}'{or_none}. Got {type(value)}."
    raise TypeError(msg)
