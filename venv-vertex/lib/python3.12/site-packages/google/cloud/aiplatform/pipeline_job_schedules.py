
















from typing import List, Optional

from google.auth import credentials as auth_credentials
from google.cloud.aiplatform import base
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import (
    PipelineJob,
)
from google.cloud.aiplatform.schedules import (
    _Schedule,
)
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.compat.types import (
    schedule as gca_schedule,
)
from google.cloud.aiplatform.constants import (
    schedule as schedule_constants,
)

from google.protobuf import field_mask_pb2 as field_mask

_LOGGER = base.Logger(__name__)


_VALID_NAME_PATTERN = schedule_constants._VALID_NAME_PATTERN


_VALID_AR_URL = schedule_constants._VALID_AR_URL


_VALID_HTTPS_URL = schedule_constants._VALID_HTTPS_URL

_SCHEDULE_ERROR_STATES = schedule_constants._SCHEDULE_ERROR_STATES


class PipelineJobSchedule(
    _Schedule,
):
    def __init__(
        self,
        pipeline_job: PipelineJob,
        display_name: str,
        credentials: Optional[auth_credentials.Credentials] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
    ):
        
        if not display_name:
            display_name = self.__class__._generate_display_name()
        utils.validate_display_name(display_name)

        project = project or pipeline_job.project
        location = location or pipeline_job.location
        super().__init__(credentials=credentials, project=project, location=location)

        self._parent = initializer.global_config.common_location_path(
            project=project, location=location
        )

        create_pipeline_job_request = {
            "parent": self._parent,
            "pipeline_job": {
                "runtime_config": pipeline_job.runtime_config,
                "pipeline_spec": pipeline_job.pipeline_spec,
            },
        }
        if "template_uri" in pipeline_job._gca_resource:
            create_pipeline_job_request["pipeline_job"][
                "template_uri"
            ] = pipeline_job._gca_resource.template_uri
        if "labels" in pipeline_job._gca_resource:
            create_pipeline_job_request["pipeline_job"][
                "labels"
            ] = pipeline_job._gca_resource.labels
        if "encryption_spec" in pipeline_job._gca_resource:
            create_pipeline_job_request["pipeline_job"][
                "encryption_spec"
            ] = pipeline_job._gca_resource.encryption_spec
        if "reserved_ip_ranges" in pipeline_job._gca_resource:
            create_pipeline_job_request["pipeline_job"][
                "reserved_ip_ranges"
            ] = pipeline_job._gca_resource.reserved_ip_ranges
        pipeline_job_schedule_args = {
            "display_name": display_name,
            "create_pipeline_job_request": create_pipeline_job_request,
        }

        self._gca_resource = gca_schedule.Schedule(**pipeline_job_schedule_args)

    def create(
        self,
        cron: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        allow_queueing: bool = False,
        max_run_count: Optional[int] = None,
        max_concurrent_run_count: int = 1,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        create_request_timeout: Optional[float] = None,
    ) -> None:
        
        network = network or initializer.global_config.network

        self._create(
            cron=cron,
            start_time=start_time,
            end_time=end_time,
            allow_queueing=allow_queueing,
            max_run_count=max_run_count,
            max_concurrent_run_count=max_concurrent_run_count,
            service_account=service_account,
            network=network,
            create_request_timeout=create_request_timeout,
        )

    def _create(
        self,
        cron: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        allow_queueing: bool = False,
        max_run_count: Optional[int] = None,
        max_concurrent_run_count: int = 1,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        create_request_timeout: Optional[float] = None,
    ) -> None:
        
        if cron:
            self._gca_resource.cron = cron
        if start_time:
            self._gca_resource.start_time = start_time
        if end_time:
            self._gca_resource.end_time = end_time
        if allow_queueing:
            self._gca_resource.allow_queueing = allow_queueing
        if max_run_count:
            self._gca_resource.max_run_count = max_run_count
        if max_concurrent_run_count:
            self._gca_resource.max_concurrent_run_count = max_concurrent_run_count

        service_account = service_account or initializer.global_config.service_account
        network = network or initializer.global_config.network

        if service_account:
            self._gca_resource.create_pipeline_job_request.pipeline_job.service_account = (
                service_account
            )

        if network:
            self._gca_resource.create_pipeline_job_request.pipeline_job.network = (
                network
            )

        _LOGGER.log_create_with_lro(self.__class__)

        self._gca_resource = self.api_client.create_schedule(
            parent=self._parent,
            schedule=self._gca_resource,
            timeout=create_request_timeout,
        )

        _LOGGER.log_create_complete_with_getter(
            self.__class__, self._gca_resource, "schedule"
        )

        _LOGGER.info("View Schedule:\n%s" % self._dashboard_uri())

    @classmethod
    def list(
        cls,
        filter: Optional[str] = None,
        order_by: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ) -> List["PipelineJobSchedule"]:
        
        return cls._list_with_local_order(
            filter=filter,
            order_by=order_by,
            project=project,
            location=location,
            credentials=credentials,
        )

    def list_jobs(
        self,
        filter: Optional[str] = None,
        order_by: Optional[str] = None,
        enable_simple_view: bool = True,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ) -> List[PipelineJob]:
        
        list_filter = f"schedule_name={self._gca_resource.name}"
        if filter:
            list_filter = list_filter + f" AND {filter}"

        return PipelineJob.list(
            filter=list_filter,
            order_by=order_by,
            enable_simple_view=enable_simple_view,
            project=project,
            location=location,
            credentials=credentials,
        )

    def update(
        self,
        display_name: Optional[str] = None,
        cron: Optional[str] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        allow_queueing: Optional[bool] = None,
        max_run_count: Optional[int] = None,
        max_concurrent_run_count: Optional[int] = None,
    ) -> None:
        
        pipeline_job_schedule = self._gca_resource
        if pipeline_job_schedule.state in _SCHEDULE_ERROR_STATES:
            raise RuntimeError(
                "Not updating PipelineJobSchedule: PipelineJobSchedule must be active or completed."
            )

        updated_fields = []
        if display_name is not None:
            updated_fields.append("display_name")
            setattr(pipeline_job_schedule, "display_name", display_name)
        if cron is not None:
            updated_fields.append("cron")
            setattr(pipeline_job_schedule, "cron", cron)
        if start_time is not None:
            updated_fields.append("start_time")
            setattr(pipeline_job_schedule, "start_time", start_time)
        if end_time is not None:
            updated_fields.append("end_time")
            setattr(pipeline_job_schedule, "end_time", end_time)
        if allow_queueing is not None:
            updated_fields.append("allow_queueing")
            setattr(pipeline_job_schedule, "allow_queueing", allow_queueing)
        if max_run_count is not None:
            updated_fields.append("max_run_count")
            setattr(pipeline_job_schedule, "max_run_count", max_run_count)
        if max_concurrent_run_count is not None:
            updated_fields.append("max_concurrent_run_count")
            setattr(
                pipeline_job_schedule,
                "max_concurrent_run_count",
                max_concurrent_run_count,
            )

        update_mask = field_mask.FieldMask(paths=updated_fields)
        self.api_client.update_schedule(
            schedule=pipeline_job_schedule,
            update_mask=update_mask,
        )
