
















from typing import Optional, List, Union

from google.auth import credentials as auth_credentials
import grpc

from google.cloud import aiplatform
from google.cloud.aiplatform import base
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform._pipeline_based_service import (
    pipeline_based_service,
)
from google.cloud.aiplatform import model_evaluation
from google.cloud.aiplatform import pipeline_jobs
from google.cloud.aiplatform.utils import _ipython_utils

from google.cloud.aiplatform.compat.types import (
    pipeline_state_v1 as gca_pipeline_state_v1,
    pipeline_job_v1 as gca_pipeline_job_v1,
    execution_v1 as gca_execution_v1,
)

_LOGGER = base.Logger(__name__)

_PIPELINE_TEMPLATE_ARTIFACT_REGISTRY_TAG = "1.0.0"
_BASE_URI = (
    "base_uri",
    "https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation",
)
_TAG = ("tag", _PIPELINE_TEMPLATE_ARTIFACT_REGISTRY_TAG)
_MODEL_EVAL_TEMPLATE_REF = frozenset((_BASE_URI, _TAG))


class _ModelEvaluationJob(pipeline_based_service._VertexAiPipelineBasedService):
    

    _template_ref = _MODEL_EVAL_TEMPLATE_REF

    _creation_log_message = "Created PipelineJob for your Model Evaluation."

    _component_identifier = "fpc-model-evaluation"

    _template_name_identifier = None

    @property
    def _metadata_output_artifact(self) -> Optional[str]:
        
        if self.state != gca_pipeline_state_v1.PipelineState.PIPELINE_STATE_SUCCEEDED:
            return
        for task in self.backing_pipeline_job._gca_resource.job_detail.task_details:
            if task.task_name == self.backing_pipeline_job.name:
                return task.outputs["evaluation_metrics"].artifacts[0].name

    def __init__(
        self,
        evaluation_pipeline_run_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ):
        
        super().__init__(
            pipeline_job_name=evaluation_pipeline_run_name,
            project=project,
            location=location,
            credentials=credentials,
        )

    @staticmethod
    def _get_template_url(
        model_type: str,
        feature_attributions: bool,
        prediction_type: str,
    ) -> str:
        

        
        
        
        
        
        model_type_uri_str = "automl-tabular" if model_type == "automl_tabular" else ""
        feature_attributions_uri_str = (
            "feature-attribution" if feature_attributions else ""
        )

        template_ref_dict = dict(_ModelEvaluationJob._template_ref)

        uri_parts = [
            template_ref_dict["base_uri"],
            model_type_uri_str,
            feature_attributions_uri_str,
            prediction_type,
            "pipeline/" + template_ref_dict["tag"],
        ]
        template_url = "-".join(filter(None, uri_parts))

        return template_url

    @classmethod
    def submit(
        cls,
        model_name: Union[str, "aiplatform.Model"],
        prediction_type: str,
        target_field_name: str,
        pipeline_root: str,
        model_type: str,
        gcs_source_uris: Optional[List[str]] = None,
        bigquery_source_uri: Optional[str] = None,
        batch_predict_bigquery_destination_output_uri: Optional[str] = None,
        class_labels: Optional[List[str]] = None,
        prediction_label_column: Optional[str] = None,
        prediction_score_column: Optional[str] = None,
        generate_feature_attributions: Optional[bool] = False,
        instances_format: Optional[str] = "jsonl",
        evaluation_pipeline_display_name: Optional[str] = None,
        evaluation_metrics_display_name: Optional[str] = None,
        job_id: Optional[str] = None,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        encryption_spec_key_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        experiment: Optional[Union[str, "aiplatform.Experiment"]] = None,
        enable_caching: Optional[bool] = None,
    ) -> "_ModelEvaluationJob":
        
        service_account = service_account or initializer.global_config.service_account

        if isinstance(model_name, aiplatform.Model):
            model_resource_name = model_name.versioned_resource_name
        else:
            model_resource_name = aiplatform.Model(
                model_name=model_name,
                project=project,
                location=location,
                credentials=credentials,
            ).versioned_resource_name

        if not evaluation_pipeline_display_name:
            evaluation_pipeline_display_name = cls._generate_display_name()

        template_params = {
            "batch_predict_instances_format": instances_format,
            "model_name": model_resource_name,
            "evaluation_display_name": evaluation_metrics_display_name,
            "project": project or initializer.global_config.project,
            "location": location or initializer.global_config.location,
            "batch_predict_gcs_destination_output_uri": pipeline_root,
            "target_field_name": target_field_name,
            "encryption_spec_key_name": encryption_spec_key_name,
        }

        if bigquery_source_uri:
            template_params["batch_predict_predictions_format"] = "bigquery"
            template_params["batch_predict_bigquery_source_uri"] = bigquery_source_uri
            template_params[
                "batch_predict_bigquery_destination_output_uri"
            ] = batch_predict_bigquery_destination_output_uri
        elif gcs_source_uris:
            template_params["batch_predict_gcs_source_uris"] = gcs_source_uris

        if prediction_type == "classification" and model_type == "other":
            template_params["evaluation_class_labels"] = class_labels

        if prediction_label_column:
            template_params[
                "evaluation_prediction_label_column"
            ] = prediction_label_column

        if prediction_score_column:
            template_params[
                "evaluation_prediction_score_column"
            ] = prediction_score_column

        
        if service_account is not None:
            template_params["dataflow_service_account"] = service_account

        template_url = cls._get_template_url(
            model_type,
            generate_feature_attributions,
            prediction_type,
        )

        eval_pipeline_run = cls._create_and_submit_pipeline_job(
            template_params=template_params,
            template_path=template_url,
            pipeline_root=pipeline_root,
            display_name=evaluation_pipeline_display_name,
            job_id=job_id,
            service_account=service_account,
            network=network,
            encryption_spec_key_name=encryption_spec_key_name,
            project=project,
            location=location,
            credentials=credentials,
            experiment=experiment,
            enable_caching=enable_caching,
        )

        _LOGGER.info(
            f"{_ModelEvaluationJob._creation_log_message} View it in the console: {eval_pipeline_run.pipeline_console_uri}"
        )

        return eval_pipeline_run

    def get_model_evaluation(
        self,
    ) -> Optional["model_evaluation.ModelEvaluation"]:
        
        eval_job_state = self.backing_pipeline_job.state

        if eval_job_state in pipeline_jobs._PIPELINE_ERROR_STATES:
            raise RuntimeError(
                f"Evaluation job failed. For more details see the logs: {self.pipeline_console_uri}"
            )
        if eval_job_state not in pipeline_jobs._PIPELINE_COMPLETE_STATES:
            _LOGGER.info(
                f"Your evaluation job is still in progress. For more details see the logs {self.pipeline_console_uri}"
            )
            return

        for component in self.backing_pipeline_job.task_details:
            
            if not component.task_name == self.backing_pipeline_job.name:
                continue

            
            if (
                component.state
                not in (
                    gca_pipeline_job_v1.PipelineTaskDetail.State.SUCCEEDED,
                    gca_pipeline_job_v1.PipelineTaskDetail.State.SKIPPED,
                )
                and component.execution.state != gca_execution_v1.Execution.State.CACHED
            ):
                continue

            if "output:evaluation_resource_name" not in component.execution.metadata:
                continue

            eval_resource_name = component.execution.metadata[
                "output:evaluation_resource_name"
            ]

            eval_resource = model_evaluation.ModelEvaluation(
                evaluation_name=eval_resource_name,
                credentials=self.credentials,
            )
            _ipython_utils.display_model_evaluation_button(eval_resource)
            return eval_resource

    def wait(self) -> None:
        
        super().wait()

        try:
            self.get_model_evaluation()
        except grpc.RpcError as e:
            _LOGGER.error("Get model evaluation call failed with error %s", e)
