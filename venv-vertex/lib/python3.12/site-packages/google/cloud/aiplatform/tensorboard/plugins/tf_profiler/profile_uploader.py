

















from collections import defaultdict
import datetime
import functools
import os
import re
from typing import DefaultDict, Dict, Generator, List, Optional, Set, Tuple

from google.cloud import storage
from google.cloud.aiplatform.compat.services import tensorboard_service_client
from google.cloud.aiplatform.compat.types import tensorboard_data
from google.cloud.aiplatform.compat.types import tensorboard_service
from google.cloud.aiplatform.compat.types import tensorboard_time_series
from google.cloud.aiplatform.tensorboard import upload_tracker
from google.cloud.aiplatform.tensorboard import uploader_constants
from google.cloud.aiplatform.tensorboard import uploader_utils
import grpc
import tensorflow as tf

from google.protobuf import timestamp_pb2 as timestamp
from tensorboard.util import tb_logging


TensorboardServiceClient = tensorboard_service_client.TensorboardServiceClient

logger = tb_logging.get_logger()


class ProfileRequestSender(uploader_utils.RequestSender):
    

    PLUGIN_NAME = "profile"
    PROFILE_PATH = "plugins/profile"

    def __init__(
        self,
        experiment_resource_name: str,
        api: TensorboardServiceClient,
        upload_limits: uploader_constants.UploadLimits,
        blob_rpc_rate_limiter: uploader_utils.RateLimiter,
        blob_storage_bucket: storage.Bucket,
        blob_storage_folder: str,
        tracker: upload_tracker.UploadTracker,
        logdir: str,
        source_bucket: Optional[storage.Bucket],
    ):
        
        self._experiment_resource_name = experiment_resource_name
        self._api = api
        self._logdir = logdir
        self._tag_metadata = {}
        self._tracker = tracker
        self._one_platform_resource_manager = uploader_utils.OnePlatformResourceManager(
            experiment_resource_name=experiment_resource_name, api=api
        )

        self._run_to_file_request_sender: Dict[str, _FileRequestSender] = {}
        self._run_to_profile_loaders: Dict[str, _ProfileSessionLoader] = {}

        self._file_request_sender_factory = functools.partial(
            _FileRequestSender,
            api=api,
            rpc_rate_limiter=blob_rpc_rate_limiter,
            max_blob_request_size=upload_limits.max_blob_request_size,
            max_blob_size=upload_limits.max_blob_size,
            blob_storage_bucket=blob_storage_bucket,
            source_bucket=source_bucket,
            blob_storage_folder=blob_storage_folder,
            tracker=self._tracker,
        )

    def _is_valid_event(self, run_name: str) -> bool:
        

        return tf.io.gfile.isdir(self._profile_dir(run_name))

    def _profile_dir(self, run_name: str) -> str:
        
        if run_name is None or run_name == uploader_utils.DEFAULT_PROFILE_RUN_NAME:
            return os.path.join(self._logdir, self.PROFILE_PATH)
        return os.path.join(self._logdir, run_name, self.PROFILE_PATH)

    def send_request(self, run_name: str):
        

        if not self._is_valid_event(run_name):
            logger.debug("No such profile run for %s", run_name)
            return

        
        
        if run_name not in self._run_to_profile_loaders:
            self._run_to_profile_loaders[run_name] = _ProfileSessionLoader(
                self._profile_dir(run_name)
            )

        experiment_run_name = uploader_utils.reformat_run_name(run_name)
        tb_run = self._one_platform_resource_manager.get_run_resource_name(
            experiment_run_name
        )

        if run_name not in self._run_to_file_request_sender:
            self._run_to_file_request_sender[
                run_name
            ] = self._file_request_sender_factory(tb_run)

        
        
        for prof_session, files in self._run_to_profile_loaders[
            run_name
        ].prof_sessions_to_files():
            event_time = datetime.datetime.strptime(prof_session, "%Y_%m_%d_%H_%M_%S")
            event_timestamp = timestamp.Timestamp().FromDatetime(event_time)

            
            self._run_to_file_request_sender[run_name].add_files(
                files=files,
                tag=prof_session,
                plugin=self.PLUGIN_NAME,
                event_timestamp=event_timestamp,
            )


class _ProfileSessionLoader(object):
    

    
    PROF_PATH_REGEX = r".*\/plugins\/profile\/[0-9]{4}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}_[0-9]{2}\/?$"

    def __init__(
        self,
        path: str,
    ):
        
        self._path = path
        self._prof_session_to_files: DefaultDict[str, Set[str]] = defaultdict(set)

    def _path_filter(self, path: str) -> bool:
        
        return tf.io.gfile.isdir(path) and re.match(self.PROF_PATH_REGEX, path)

    def _path_to_files(self, prof_session: str, path: str) -> List[str]:
        

        files = []
        for prof_file in tf.io.gfile.listdir(path):
            full_file_path = os.path.join(path, prof_file)
            if full_file_path not in self._prof_session_to_files[prof_session]:
                files.append(full_file_path)

        self._prof_session_to_files[prof_session].update(files)
        return files

    def prof_sessions_to_files(self) -> Generator[Tuple[str, List[str]], None, None]:
        

        prof_sessions = tf.io.gfile.listdir(self._path)

        for prof_session in prof_sessions:
            
            prof_session = (
                prof_session if not prof_session.endswith("/") else prof_session[:-1]
            )

            full_path = os.path.join(self._path, prof_session)
            if not self._path_filter(full_path):
                continue

            files = self._path_to_files(prof_session, full_path)

            if files:
                yield (prof_session, files)


class _FileRequestSender(object):
    

    def __init__(
        self,
        run_resource_id: str,
        api: TensorboardServiceClient,
        rpc_rate_limiter: uploader_utils.RateLimiter,
        max_blob_request_size: int,
        max_blob_size: int,
        blob_storage_bucket: storage.Bucket,
        blob_storage_folder: str,
        tracker: upload_tracker.UploadTracker,
        source_bucket: Optional[storage.Bucket] = None,
    ):
        
        self._run_resource_id = run_resource_id
        self._api = api
        self._rpc_rate_limiter = rpc_rate_limiter
        self._max_blob_request_size = max_blob_request_size
        self._max_blob_size = max_blob_size
        self._tracker = tracker
        self._time_series_resource_manager = uploader_utils.TimeSeriesResourceManager(
            run_resource_id, api
        )

        self._bucket = blob_storage_bucket
        self._folder = blob_storage_folder
        self._source_bucket = source_bucket

        self._new_request()

    def _new_request(self):
        
        self._files = []
        self._tag = None
        self._plugin = None
        self._event_timestamp = None

    def add_files(
        self,
        files: List[str],
        tag: str,
        plugin: str,
        event_timestamp: timestamp.Timestamp,
    ):
        

        for prof_file in files:
            if not tf.io.gfile.exists(prof_file):
                logger.warning(
                    "The file provided does not exist. "
                    "Will not be uploading file %s.",
                    prof_file,
                )
            else:
                self._files.append(prof_file)

        self._tag = tag
        self._plugin = plugin
        self._event_timestamp = event_timestamp
        self.flush()
        self._new_request()

    def flush(self):
        
        if not self._files:
            return

        time_series_proto = self._time_series_resource_manager.get_or_create(
            self._tag,
            lambda: tensorboard_time_series.TensorboardTimeSeries(
                display_name=self._tag,
                value_type=tensorboard_time_series.TensorboardTimeSeries.ValueType.BLOB_SEQUENCE,
                plugin_name=self._plugin,
            ),
        )
        m = re.match(
            ".*/tensorboards/(.*)/experiments/(.*)/runs/(.*)/timeSeries/(.*)",
            time_series_proto.name,
        )
        blob_path_prefix = "tensorboard-{}/{}/{}/{}".format(m[1], m[2], m[3], m[4])
        blob_path_prefix = (
            "{}/{}".format(self._folder, blob_path_prefix)
            if self._folder
            else blob_path_prefix
        )
        sent_blob_ids = []

        for prof_file in self._files:
            self._rpc_rate_limiter.tick()
            file_size = tf.io.gfile.stat(prof_file).length
            with self._tracker.blob_tracker(file_size) as blob_tracker:
                if not self._file_too_large(prof_file):
                    blob_id = self._upload(prof_file, blob_path_prefix)
                    sent_blob_ids.append(str(blob_id))
                    blob_tracker.mark_uploaded(blob_id is not None)

        data_point = tensorboard_data.TimeSeriesDataPoint(
            blobs=tensorboard_data.TensorboardBlobSequence(
                values=[
                    tensorboard_data.TensorboardBlob(id=blob_id)
                    for blob_id in sent_blob_ids
                ]
            ),
            wall_time=self._event_timestamp,
        )

        time_series_data_proto = tensorboard_data.TimeSeriesData(
            tensorboard_time_series_id=time_series_proto.name.split("/")[-1],
            value_type=tensorboard_time_series.TensorboardTimeSeries.ValueType.BLOB_SEQUENCE,
            values=[data_point],
        )
        request = tensorboard_service.WriteTensorboardRunDataRequest(
            time_series_data=[time_series_data_proto]
        )

        _prune_empty_time_series_from_blob(request)
        if not request.time_series_data:
            return

        with uploader_utils.request_logger(request):
            try:
                self._api.write_tensorboard_run_data(
                    tensorboard_run=self._run_resource_id,
                    time_series_data=request.time_series_data,
                )
            except grpc.RpcError as e:
                logger.error("Upload call failed with error %s", e)

    def _file_too_large(self, filename: str) -> bool:
        

        file_size = tf.io.gfile.stat(filename).length
        if file_size > self._max_blob_size:
            logger.warning(
                "Blob too large; skipping.  Size %d exceeds limit of %d bytes.",
                file_size,
                self._max_blob_size,
            )
            return True
        return False

    def _upload(self, filename: str, blob_path_prefix: Optional[str] = None) -> str:
        
        blob_id = os.path.basename(filename)
        blob_path = (
            "{}/{}".format(blob_path_prefix, blob_id) if blob_path_prefix else blob_id
        )

        
        if self._source_bucket:
            self._copy_between_buckets(filename, blob_path)
        else:
            self._upload_from_local(filename, blob_path)

        return blob_id

    def _copy_between_buckets(self, filename: str, blob_path: str):
        
        blob_name = _get_blob_from_file(filename)

        source_blob = self._source_bucket.blob(blob_name)

        self._source_bucket.copy_blob(
            source_blob,
            self._bucket,
            blob_path,
        )

    def _upload_from_local(self, filename: str, blob_path: str):
        
        blob = self._bucket.blob(blob_path)
        blob.upload_from_filename(filename)


def _get_blob_from_file(fp: str) -> Optional[str]:
    
    m = re.match(r"gs:\/\/.*?\/(.*)", fp)
    if not m:
        logger.warning("Could not get the blob name from file %s", fp)
        return None
    return m[1]


def _prune_empty_time_series_from_blob(
    request: tensorboard_service.WriteTensorboardRunDataRequest,
):
    
    for time_series_idx, time_series_data in reversed(
        list(enumerate(request.time_series_data))
    ):
        if not any(x.blobs for x in time_series_data.values):
            del request.time_series_data[time_series_idx]
