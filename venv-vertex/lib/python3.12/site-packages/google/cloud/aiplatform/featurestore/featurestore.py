
















from typing import Dict, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union
import uuid

from google.auth import credentials as auth_credentials
from google.protobuf import field_mask_pb2
from google.protobuf import timestamp_pb2

from google.cloud.aiplatform import base
from google.cloud.aiplatform.compat.types import (
    feature_selector as gca_feature_selector,
    featurestore as gca_featurestore,
    featurestore_service as gca_featurestore_service,
    io as gca_io,
)
from google.cloud.aiplatform import featurestore
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.utils import (
    featurestore_utils,
    resource_manager_utils,
)

if TYPE_CHECKING:
    from google.cloud import bigquery

_LOGGER = base.Logger(__name__)


class Featurestore(base.VertexAiResourceNounWithFutureManager):
    

    client_class = utils.FeaturestoreClientWithOverride

    _resource_noun = "featurestores"
    _getter_method = "get_featurestore"
    _list_method = "list_featurestores"
    _delete_method = "delete_featurestore"
    _parse_resource_name_method = "parse_featurestore_path"
    _format_resource_name_method = "featurestore_path"

    @staticmethod
    def _resource_id_validator(resource_id: str):
        
        featurestore_utils.validate_id(resource_id)

    def __init__(
        self,
        featurestore_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ):
        

        super().__init__(
            project=project,
            location=location,
            credentials=credentials,
            resource_name=featurestore_name,
        )
        self._gca_resource = self._get_gca_resource(resource_name=featurestore_name)

    def get_entity_type(self, entity_type_id: str) -> "featurestore.EntityType":
        
        self.wait()
        return self._get_entity_type(entity_type_id=entity_type_id)

    def _get_entity_type(self, entity_type_id: str) -> "featurestore.EntityType":
        
        featurestore_name_components = self._parse_resource_name(self.resource_name)
        return featurestore.EntityType(
            entity_type_name=featurestore.EntityType._format_resource_name(
                project=featurestore_name_components["project"],
                location=featurestore_name_components["location"],
                featurestore=featurestore_name_components["featurestore"],
                entity_type=entity_type_id,
            )
        )

    def update(
        self,
        labels: Optional[Dict[str, str]] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        update_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        

        return self._update(
            labels=labels,
            request_metadata=request_metadata,
            update_request_timeout=update_request_timeout,
        )

    
    def update_online_store(
        self,
        fixed_node_count: int,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        update_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        
        return self._update(
            fixed_node_count=fixed_node_count,
            request_metadata=request_metadata,
            update_request_timeout=update_request_timeout,
        )

    def _update(
        self,
        labels: Optional[Dict[str, str]] = None,
        fixed_node_count: Optional[int] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        update_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        
        self.wait()
        update_mask = list()

        if labels:
            utils.validate_labels(labels)
            update_mask.append("labels")

        if fixed_node_count is not None:
            update_mask.append("online_serving_config.fixed_node_count")

        update_mask = field_mask_pb2.FieldMask(paths=update_mask)

        gapic_featurestore = gca_featurestore.Featurestore(
            name=self.resource_name,
            labels=labels,
            online_serving_config=gca_featurestore.Featurestore.OnlineServingConfig(
                fixed_node_count=fixed_node_count
            ),
        )

        _LOGGER.log_action_start_against_resource(
            "Updating",
            "featurestore",
            self,
        )

        update_featurestore_lro = self.api_client.update_featurestore(
            featurestore=gapic_featurestore,
            update_mask=update_mask,
            metadata=request_metadata,
            timeout=update_request_timeout,
        )

        _LOGGER.log_action_started_against_resource_with_lro(
            "Update", "featurestore", self.__class__, update_featurestore_lro
        )

        update_featurestore_lro.result()

        _LOGGER.log_action_completed_against_resource("featurestore", "updated", self)

        return self

    def list_entity_types(
        self,
        filter: Optional[str] = None,
        order_by: Optional[str] = None,
    ) -> List["featurestore.EntityType"]:
        
        self.wait()
        return featurestore.EntityType.list(
            featurestore_name=self.resource_name,
            filter=filter,
            order_by=order_by,
        )

    @base.optional_sync()
    def delete_entity_types(
        self,
        entity_type_ids: List[str],
        sync: bool = True,
        force: bool = False,
    ) -> None:
        
        entity_types = []
        for entity_type_id in entity_type_ids:
            entity_type = self._get_entity_type(entity_type_id=entity_type_id)
            entity_type.delete(force=force, sync=False)
            entity_types.append(entity_type)

        for entity_type in entity_types:
            entity_type.wait()

    @base.optional_sync()
    def delete(self, sync: bool = True, force: bool = False) -> None:
        
        _LOGGER.log_action_start_against_resource("Deleting", "", self)
        lro = getattr(self.api_client, self._delete_method)(
            name=self.resource_name, force=force
        )
        _LOGGER.log_action_started_against_resource_with_lro(
            "Delete", "", self.__class__, lro
        )
        lro.result()
        _LOGGER.log_action_completed_against_resource("deleted.", "", self)

    @classmethod
    @base.optional_sync()
    def create(
        cls,
        featurestore_id: str,
        online_store_fixed_node_count: Optional[int] = None,
        labels: Optional[Dict[str, str]] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        encryption_spec_key_name: Optional[str] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        
        gapic_featurestore = gca_featurestore.Featurestore(
            online_serving_config=gca_featurestore.Featurestore.OnlineServingConfig(
                fixed_node_count=online_store_fixed_node_count
            )
        )

        if labels:
            utils.validate_labels(labels)
            gapic_featurestore.labels = labels

        if encryption_spec_key_name:
            gapic_featurestore.encryption_spec = (
                initializer.global_config.get_encryption_spec(
                    encryption_spec_key_name=encryption_spec_key_name
                )
            )

        api_client = cls._instantiate_client(location=location, credentials=credentials)

        created_featurestore_lro = api_client.create_featurestore(
            parent=initializer.global_config.common_location_path(
                project=project, location=location
            ),
            featurestore=gapic_featurestore,
            featurestore_id=featurestore_id,
            metadata=request_metadata,
            timeout=create_request_timeout,
        )

        _LOGGER.log_create_with_lro(cls, created_featurestore_lro)

        created_featurestore = created_featurestore_lro.result()

        _LOGGER.log_create_complete(cls, created_featurestore, "featurestore")

        featurestore_obj = cls(
            featurestore_name=created_featurestore.name,
            project=project,
            location=location,
            credentials=credentials,
        )

        return featurestore_obj

    def create_entity_type(
        self,
        entity_type_id: str,
        description: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "featurestore.EntityType":
        
        self.wait()
        return featurestore.EntityType.create(
            entity_type_id=entity_type_id,
            featurestore_name=self.resource_name,
            description=description,
            labels=labels,
            request_metadata=request_metadata,
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    def _batch_read_feature_values(
        self,
        batch_read_feature_values_request: gca_featurestore_service.BatchReadFeatureValuesRequest,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        serve_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        

        _LOGGER.log_action_start_against_resource(
            "Serving",
            "feature values",
            self,
        )

        batch_read_lro = self.api_client.batch_read_feature_values(
            request=batch_read_feature_values_request,
            metadata=request_metadata,
            timeout=serve_request_timeout,
        )

        _LOGGER.log_action_started_against_resource_with_lro(
            "Serve", "feature values", self.__class__, batch_read_lro
        )

        batch_read_lro.result()

        _LOGGER.log_action_completed_against_resource("feature values", "served", self)

        return self

    @staticmethod
    def _validate_and_get_read_instances(
        read_instances_uri: str,
    ) -> Union[gca_io.BigQuerySource, gca_io.CsvSource]:
        
        if not (
            read_instances_uri.startswith("bq://")
            or read_instances_uri.startswith("gs://")
        ):
            raise ValueError(
                "The read_instances_uri should be a single uri starts with either 'bq://' or 'gs://'."
            )

        if read_instances_uri.startswith("bq://"):
            return gca_io.BigQuerySource(input_uri=read_instances_uri)
        if read_instances_uri.startswith("gs://"):
            return gca_io.CsvSource(
                gcs_source=gca_io.GcsSource(uris=[read_instances_uri])
            )

    def _validate_and_get_batch_read_feature_values_request(
        self,
        featurestore_name: str,
        serving_feature_ids: Dict[str, List[str]],
        destination: Union[
            gca_io.BigQueryDestination,
            gca_io.CsvDestination,
            gca_io.TFRecordDestination,
        ],
        read_instances: Union[gca_io.BigQuerySource, gca_io.CsvSource],
        pass_through_fields: Optional[List[str]] = None,
        feature_destination_fields: Optional[Dict[str, str]] = None,
        start_time: [timestamp_pb2.Timestamp] = None,
    ) -> gca_featurestore_service.BatchReadFeatureValuesRequest:
        
        featurestore_name_components = self._parse_resource_name(featurestore_name)

        feature_destination_fields = feature_destination_fields or {}

        entity_type_specs = []
        for entity_type_id, feature_ids in serving_feature_ids.items():
            destination_feature_settings = []
            for feature_id in feature_ids:
                feature_resource_name = featurestore.Feature._format_resource_name(
                    project=featurestore_name_components["project"],
                    location=featurestore_name_components["location"],
                    featurestore=featurestore_name_components["featurestore"],
                    entity_type=entity_type_id,
                    feature=feature_id,
                )

                feature_destination_field = feature_destination_fields.get(
                    feature_resource_name
                )
                if feature_destination_field:
                    destination_feature_setting_proto = (
                        gca_featurestore_service.DestinationFeatureSetting(
                            feature_id=feature_id,
                            destination_field=feature_destination_field,
                        )
                    )
                    destination_feature_settings.append(
                        destination_feature_setting_proto
                    )

            entity_type_spec = (
                gca_featurestore_service.BatchReadFeatureValuesRequest.EntityTypeSpec(
                    entity_type_id=entity_type_id,
                    feature_selector=gca_feature_selector.FeatureSelector(
                        id_matcher=gca_feature_selector.IdMatcher(ids=feature_ids)
                    ),
                    settings=destination_feature_settings or None,
                )
            )
            entity_type_specs.append(entity_type_spec)

        batch_read_feature_values_request = (
            gca_featurestore_service.BatchReadFeatureValuesRequest(
                featurestore=featurestore_name,
                entity_type_specs=entity_type_specs,
            )
        )

        if isinstance(destination, gca_io.BigQueryDestination):
            batch_read_feature_values_request.destination = (
                gca_featurestore_service.FeatureValueDestination(
                    bigquery_destination=destination
                )
            )
        elif isinstance(destination, gca_io.CsvDestination):
            batch_read_feature_values_request.destination = (
                gca_featurestore_service.FeatureValueDestination(
                    csv_destination=destination
                )
            )
        elif isinstance(destination, gca_io.TFRecordDestination):
            batch_read_feature_values_request.destination = (
                gca_featurestore_service.FeatureValueDestination(
                    tfrecord_destination=destination
                )
            )

        if isinstance(read_instances, gca_io.BigQuerySource):
            batch_read_feature_values_request.bigquery_read_instances = read_instances
        elif isinstance(read_instances, gca_io.CsvSource):
            batch_read_feature_values_request.csv_read_instances = read_instances

        if pass_through_fields is not None:
            batch_read_feature_values_request.pass_through_fields = [
                gca_featurestore_service.BatchReadFeatureValuesRequest.PassThroughField(
                    field_name=pass_through_field
                )
                for pass_through_field in pass_through_fields
            ]

        if start_time is not None:
            batch_read_feature_values_request.start_time = start_time

        return batch_read_feature_values_request

    @base.optional_sync(return_input_arg="self")
    def batch_serve_to_bq(
        self,
        bq_destination_output_uri: str,
        serving_feature_ids: Dict[str, List[str]],
        read_instances_uri: str,
        pass_through_fields: Optional[List[str]] = None,
        feature_destination_fields: Optional[Dict[str, str]] = None,
        start_time: Optional[timestamp_pb2.Timestamp] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        serve_request_timeout: Optional[float] = None,
        sync: bool = True,
    ) -> "Featurestore":
        
        read_instances = self._validate_and_get_read_instances(read_instances_uri)

        batch_read_feature_values_request = (
            self._validate_and_get_batch_read_feature_values_request(
                featurestore_name=self.resource_name,
                serving_feature_ids=serving_feature_ids,
                destination=gca_io.BigQueryDestination(
                    output_uri=bq_destination_output_uri
                ),
                feature_destination_fields=feature_destination_fields,
                read_instances=read_instances,
                pass_through_fields=pass_through_fields,
                start_time=start_time,
            )
        )

        return self._batch_read_feature_values(
            batch_read_feature_values_request=batch_read_feature_values_request,
            request_metadata=request_metadata,
            serve_request_timeout=serve_request_timeout,
        )

    @base.optional_sync(return_input_arg="self")
    def batch_serve_to_gcs(
        self,
        gcs_destination_output_uri_prefix: str,
        gcs_destination_type: str,
        serving_feature_ids: Dict[str, List[str]],
        read_instances_uri: str,
        pass_through_fields: Optional[List[str]] = None,
        feature_destination_fields: Optional[Dict[str, str]] = None,
        start_time: Optional[timestamp_pb2.Timestamp] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        sync: bool = True,
        serve_request_timeout: Optional[float] = None,
    ) -> "Featurestore":
        
        destination = None
        if gcs_destination_type not in featurestore_utils.GCS_DESTINATION_TYPE:
            raise ValueError(
                "Only %s are supported gcs_destination_type, not `%s`. "
                % (
                    "`" + "`, `".join(featurestore_utils.GCS_DESTINATION_TYPE) + "`",
                    gcs_destination_type,
                )
            )

        gcs_destination = gca_io.GcsDestination(
            output_uri_prefix=gcs_destination_output_uri_prefix
        )
        if gcs_destination_type == "csv":
            destination = gca_io.CsvDestination(gcs_destination=gcs_destination)
        if gcs_destination_type == "tfrecord":
            destination = gca_io.TFRecordDestination(gcs_destination=gcs_destination)

        read_instances = self._validate_and_get_read_instances(read_instances_uri)

        batch_read_feature_values_request = (
            self._validate_and_get_batch_read_feature_values_request(
                featurestore_name=self.resource_name,
                serving_feature_ids=serving_feature_ids,
                destination=destination,
                feature_destination_fields=feature_destination_fields,
                read_instances=read_instances,
                pass_through_fields=pass_through_fields,
                start_time=start_time,
            )
        )

        return self._batch_read_feature_values(
            batch_read_feature_values_request=batch_read_feature_values_request,
            request_metadata=request_metadata,
            serve_request_timeout=serve_request_timeout,
        )

    def batch_serve_to_df(
        self,
        serving_feature_ids: Dict[str, List[str]],
        read_instances_df: "pd.DataFrame",  
        pass_through_fields: Optional[List[str]] = None,
        feature_destination_fields: Optional[Dict[str, str]] = None,
        start_time: Optional[timestamp_pb2.Timestamp] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        serve_request_timeout: Optional[float] = None,
        bq_dataset_id: Optional[str] = None,
    ) -> "pd.DataFrame":  
        
        try:
            from google.cloud import bigquery_storage
        except ImportError:
            raise ImportError(
                f"Google-Cloud-Bigquery-Storage is not installed. Please install google-cloud-bigquery-storage to use "
                f"{self.batch_serve_to_df.__name__}"
            )

        try:
            import pyarrow  
        except ImportError:
            raise ImportError(
                f"Pyarrow is not installed. Please install pyarrow to use "
                f"{self.batch_serve_to_df.__name__}"
            )

        try:
            import pandas as pd
        except ImportError:
            raise ImportError(
                f"Pandas is not installed. Please install pandas to use "
                f"{self.batch_serve_to_df.__name__}"
            )

        
        from google.cloud import bigquery  

        bigquery_client = bigquery.Client(
            project=self.project, credentials=self.credentials
        )

        self.wait()
        featurestore_name_components = self._parse_resource_name(self.resource_name)

        
        if bq_dataset_id is None:
            temp_bq_full_dataset_id = self._get_ephemeral_bq_full_dataset_id(
                featurestore_name_components["featurestore"],
                featurestore_name_components["project"],
            )
            temp_bq_dataset = self._create_ephemeral_bq_dataset(
                bigquery_client, temp_bq_full_dataset_id
            )
            temp_bq_batch_serve_table_name = "batch_serve"
            temp_bq_read_instances_table_name = "read_instances"

        
        else:
            temp_bq_full_dataset_id = bq_dataset_id
            temp_bq_dataset = bigquery.Dataset(dataset_ref=temp_bq_full_dataset_id)
            temp_bq_batch_serve_table_name = f"tmp_batch_serve_{uuid.uuid4()}".replace(
                "-", "_"
            )
            temp_bq_read_instances_table_name = (
                f"tmp_read_instances_{uuid.uuid4()}".replace("-", "_")
            )

        temp_bq_batch_serve_table_id = (
            f"{temp_bq_full_dataset_id}.{temp_bq_batch_serve_table_name}"
        )

        temp_bq_read_instances_table_id = (
            f"{temp_bq_full_dataset_id}.{temp_bq_read_instances_table_name}"
        )

        try:

            job = bigquery_client.load_table_from_dataframe(
                dataframe=read_instances_df,
                destination=temp_bq_read_instances_table_id,
            )
            job.result()

            self.batch_serve_to_bq(
                bq_destination_output_uri=f"bq://{temp_bq_batch_serve_table_id}",
                serving_feature_ids=serving_feature_ids,
                read_instances_uri=f"bq://{temp_bq_read_instances_table_id}",
                pass_through_fields=pass_through_fields,
                feature_destination_fields=feature_destination_fields,
                request_metadata=request_metadata,
                serve_request_timeout=serve_request_timeout,
                start_time=start_time,
            )

            bigquery_storage_read_client = bigquery_storage.BigQueryReadClient(
                credentials=self.credentials
            )
            read_session_proto = bigquery_storage_read_client.create_read_session(
                parent=f"projects/{self.project}",
                read_session=bigquery_storage.types.ReadSession(
                    table="projects/{project}/datasets/{dataset}/tables/{table}".format(
                        project=self.project,
                        dataset=temp_bq_dataset.dataset_id,
                        table=temp_bq_batch_serve_table_name,
                    ),
                    data_format=bigquery_storage.types.DataFormat.ARROW,
                ),
            )

            frames = []
            for stream in read_session_proto.streams:
                reader = bigquery_storage_read_client.read_rows(stream.name)
                for message in reader.rows().pages:
                    frames.append(message.to_dataframe())

        finally:
            
            if bq_dataset_id is None:
                bigquery_client.delete_dataset(
                    dataset=temp_bq_dataset.dataset_id,
                    delete_contents=True,
                )

            
            else:
                bigquery_client.delete_table(temp_bq_batch_serve_table_id)
                bigquery_client.delete_table(temp_bq_read_instances_table_id)

        return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(frames)

    def _get_ephemeral_bq_full_dataset_id(
        self, featurestore_id: str, project_number: str
    ) -> str:
        
        temp_bq_dataset_name = f"temp_{featurestore_id}_{uuid.uuid4()}".replace(
            "-", "_"
        )

        project_id = resource_manager_utils.get_project_id(
            project_number=project_number,
            credentials=self.credentials,
        )

        return f"{project_id}.{temp_bq_dataset_name}"[:1024]

    def _create_ephemeral_bq_dataset(
        self, bigquery_client: "bigquery.Client", dataset_id: str
    ) -> "bigquery.Dataset":
        
        
        from google.cloud import bigquery  

        temp_bq_dataset = bigquery.Dataset(dataset_ref=dataset_id)
        temp_bq_dataset.location = self.location

        return bigquery_client.create_dataset(temp_bq_dataset)
