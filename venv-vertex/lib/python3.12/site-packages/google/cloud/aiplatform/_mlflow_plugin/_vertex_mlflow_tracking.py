
















from collections import defaultdict
from typing import Any, Dict, List, NamedTuple, Optional, Union

from mlflow import entities as mlflow_entities
from mlflow.store.tracking import abstract_store
from mlflow import exceptions as mlflow_exceptions

from google.cloud import aiplatform
from google.cloud.aiplatform import base
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.compat.types import execution as execution_v1

_LOGGER = base.Logger(__name__)



_MLFLOW_RUN_TO_VERTEX_RUN_STATUS = {
    mlflow_entities.RunStatus.FINISHED: execution_v1.Execution.State.COMPLETE,
    mlflow_entities.RunStatus.FAILED: execution_v1.Execution.State.FAILED,
    mlflow_entities.RunStatus.RUNNING: execution_v1.Execution.State.RUNNING,
    mlflow_entities.RunStatus.KILLED: execution_v1.Execution.State.CANCELLED,
    mlflow_entities.RunStatus.SCHEDULED: execution_v1.Execution.State.NEW,
}
mlflow_to_vertex_run_default = defaultdict(
    lambda: execution_v1.Execution.State.STATE_UNSPECIFIED
)
for mlflow_status in _MLFLOW_RUN_TO_VERTEX_RUN_STATUS:
    mlflow_to_vertex_run_default[mlflow_status] = _MLFLOW_RUN_TO_VERTEX_RUN_STATUS[
        mlflow_status
    ]


_VERTEX_RUN_TO_MLFLOW_RUN_STATUS = {
    v: k for k, v in _MLFLOW_RUN_TO_VERTEX_RUN_STATUS.items()
}
vertex_run_to_mflow_default = defaultdict(lambda: mlflow_entities.RunStatus.FAILED)
for vertex_status in _VERTEX_RUN_TO_MLFLOW_RUN_STATUS:
    vertex_run_to_mflow_default[vertex_status] = _VERTEX_RUN_TO_MLFLOW_RUN_STATUS[
        vertex_status
    ]

_MLFLOW_TERMINAL_RUN_STATES = [
    mlflow_entities.RunStatus.FINISHED,
    mlflow_entities.RunStatus.FAILED,
    mlflow_entities.RunStatus.KILLED,
]


class _RunTracker(NamedTuple):
    

    autocreate: bool
    experiment_run: "aiplatform.ExperimentRun"


class _VertexMlflowTracking(abstract_store.AbstractStore):
    

    def _to_mlflow_metric(
        self,
        vertex_metrics: Dict[str, Union[float, int, str]],
    ) -> Optional[List[mlflow_entities.Metric]]:
        

        mlflow_metrics = []

        if vertex_metrics:
            for metric_key in vertex_metrics:
                mlflow_metric = mlflow_entities.Metric(
                    key=metric_key,
                    value=vertex_metrics[metric_key],
                    step=0,
                    timestamp=0,
                )
                mlflow_metrics.append(mlflow_metric)
        else:
            return None

        return mlflow_metrics

    def _to_mlflow_params(
        self, vertex_params: Dict[str, Union[float, int, str]]
    ) -> Optional[mlflow_entities.Param]:
        

        mlflow_params = []

        if vertex_params:
            for param_key in vertex_params:
                mlflow_param = mlflow_entities.Param(
                    key=param_key, value=vertex_params[param_key]
                )
                mlflow_params.append(mlflow_param)
        else:
            return None

        return mlflow_params

    def _to_mlflow_entity(
        self,
        vertex_exp: "aiplatform.Experiment",
        vertex_run: "aiplatform.ExperimentRun",
    ) -> mlflow_entities.Run:
        

        run_info = mlflow_entities.RunInfo(
            run_id=f"{vertex_exp.name}-{vertex_run.name}",
            run_uuid=f"{vertex_exp.name}-{vertex_run.name}",
            experiment_id=vertex_exp.name,
            user_id="",
            status=vertex_run_to_mflow_default[vertex_run.state],
            start_time=1,
            end_time=2,
            lifecycle_stage=mlflow_entities.LifecycleStage.ACTIVE,
            artifact_uri="file:///tmp/",  
        )

        run_data = mlflow_entities.RunData(
            metrics=self._to_mlflow_metric(vertex_run.get_metrics()),
            params=self._to_mlflow_params(vertex_run.get_params()),
            tags={},
        )

        return mlflow_entities.Run(run_info=run_info, run_data=run_data)

    def __init__(self, store_uri: Optional[str], artifact_uri: Optional[str]) -> None:
        

        self._run_map = {}
        self._vertex_experiment = None
        self._nested_run_tracker = {}
        super(_VertexMlflowTracking, self).__init__()

    @property
    def run_map(self) -> Dict[str, Any]:
        return self._run_map

    @property
    def vertex_experiment(self) -> "aiplatform.Experiment":
        return self._vertex_experiment

    def create_run(
        self,
        experiment_id: str,
        user_id: str,
        start_time: str,
        tags: List[mlflow_entities.RunTag],
        run_name: str,
    ) -> mlflow_entities.Run:
        

        self._vertex_experiment = (
            aiplatform.metadata.metadata._experiment_tracker.experiment
        )

        currently_active_run = (
            aiplatform.metadata.metadata._experiment_tracker.experiment_run
        )

        parent_run_id = None

        for tag in tags:
            if tag.key == "mlflow.parentRunId" and tag.value is not None:
                parent_run_id = tag.value
                if parent_run_id in self._nested_run_tracker:
                    self._nested_run_tracker[parent_run_id] += 1
                else:
                    self._nested_run_tracker[parent_run_id] = 1
                    _LOGGER.warning(
                        f"This model creates nested runs. No additional ExperimentRun resources will be created for nested runs, summary metrics and parameters will be logged to the parent ExperimentRun: {parent_run_id}."
                    )

        if currently_active_run:
            if (
                f"{currently_active_run.resource_id}" in self._run_map
                and not parent_run_id
            ):
                _LOGGER.warning(
                    "Metrics and parameters have already been logged to this run. Call aiplatform.end_run() to end the current run before training a new model."
                )
                raise mlflow_exceptions.MlflowException(
                    "Metrics and parameters have already been logged to this run. Call aiplatform.end_run() to end the current run before training a new model."
                )
            elif not parent_run_id:
                run_tracker = _RunTracker(
                    autocreate=False, experiment_run=currently_active_run
                )
                current_run_id = currently_active_run.name

            
            else:
                raise mlflow_exceptions.MlflowException(
                    f"This model creates nested runs. No additional ExperimentRun resources will be created for nested runs, summary metrics and parameters will be logged to the {parent_run_id}: ExperimentRun."
                )

        
        else:
            framework = ""

            for tag in tags:
                if tag.key == "mlflow.autologging":
                    framework = tag.value

            current_run_id = f"{framework}-{utils.timestamped_unique_name()}"
            currently_active_run = aiplatform.start_run(run=current_run_id)
            run_tracker = _RunTracker(
                autocreate=True, experiment_run=currently_active_run
            )

        self._run_map[currently_active_run.resource_id] = run_tracker

        return self._to_mlflow_entity(
            vertex_exp=self._vertex_experiment,
            vertex_run=run_tracker.experiment_run,
        )

    def update_run_info(
        self,
        run_id: str,
        run_status: mlflow_entities.RunStatus,
        end_time: int,
        run_name: str,
    ) -> mlflow_entities.RunInfo:
        

        
        
        
        
        
        

        if (
            self._run_map[run_id].autocreate
            and run_status in _MLFLOW_TERMINAL_RUN_STATES
            and self._run_map[run_id].experiment_run
            is aiplatform.metadata.metadata._experiment_tracker.experiment_run
        ):
            aiplatform.metadata.metadata._experiment_tracker.end_run(
                state=execution_v1.Execution.State.COMPLETE
            )
        elif (
            self._run_map[run_id].autocreate
            or run_status not in _MLFLOW_TERMINAL_RUN_STATES
        ):
            self._run_map[run_id].experiment_run.update_state(
                state=mlflow_to_vertex_run_default[run_status]
            )

        return mlflow_entities.RunInfo(
            run_uuid=run_id,
            run_id=run_id,
            status=run_status,
            end_time=end_time,
            experiment_id=self._vertex_experiment,
            user_id="",
            start_time=1,
            lifecycle_stage=mlflow_entities.LifecycleStage.ACTIVE,
            artifact_uri="file:///tmp/",
        )

    def log_batch(
        self,
        run_id: str,
        metrics: List[mlflow_entities.Metric],
        params: List[mlflow_entities.Param],
        tags: List[mlflow_entities.RunTag],
    ) -> None:
        

        summary_metrics = {}
        summary_params = {}
        time_series_metrics = {}

        
        vertex_run = self._run_map[run_id].experiment_run

        for metric in metrics:
            if metric.step:
                if metric.step not in time_series_metrics:
                    time_series_metrics[metric.step] = {metric.key: metric.value}
                else:
                    time_series_metrics[metric.step][metric.key] = metric.value
            else:
                summary_metrics[metric.key] = metric.value

        for param in params:
            summary_params[param.key] = param.value

        if summary_metrics:
            vertex_run.log_metrics(metrics=summary_metrics)

        if summary_params:
            vertex_run.log_params(params=summary_params)

        
        if time_series_metrics:
            for step in time_series_metrics:
                vertex_run.log_time_series_metrics(time_series_metrics[step], step)

    def get_run(self, run_id: str) -> mlflow_entities.Run:
        
        return self._to_mlflow_entity(
            vertex_exp=self._vertex_experiment,
            vertex_run=self._run_map[run_id].experiment_run,
        )
