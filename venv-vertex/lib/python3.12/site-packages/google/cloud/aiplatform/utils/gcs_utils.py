
















import datetime
import glob
import logging
import os
import pathlib
import tempfile
from typing import Optional, TYPE_CHECKING

from google.auth import credentials as auth_credentials
from google.cloud import storage

from google.cloud.aiplatform import initializer
from google.cloud.aiplatform.utils import resource_manager_utils

if TYPE_CHECKING:
    import pandas

_logger = logging.getLogger(__name__)


def upload_to_gcs(
    source_path: str,
    destination_uri: str,
    project: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
):
    
    source_path_obj = pathlib.Path(source_path)
    if not source_path_obj.exists():
        raise RuntimeError(f"Source path does not exist: {source_path}")

    project = project or initializer.global_config.project
    credentials = credentials or initializer.global_config.credentials

    storage_client = storage.Client(project=project, credentials=credentials)
    if source_path_obj.is_dir():
        source_file_paths = glob.glob(
            pathname=str(source_path_obj / "**"), recursive=True
        )
        for source_file_path in source_file_paths:
            source_file_path_obj = pathlib.Path(source_file_path)
            if source_file_path_obj.is_dir():
                continue
            source_file_relative_path_obj = source_file_path_obj.relative_to(
                source_path_obj
            )
            source_file_relative_posix_path = source_file_relative_path_obj.as_posix()
            destination_file_uri = (
                destination_uri.rstrip("/") + "/" + source_file_relative_posix_path
            )
            _logger.debug(f'Uploading "{source_file_path}" to "{destination_file_uri}"')
            destination_blob = storage.Blob.from_string(
                destination_file_uri, client=storage_client
            )
            destination_blob.upload_from_filename(filename=source_file_path)
    else:
        source_file_path = source_path
        destination_file_uri = destination_uri
        _logger.debug(f'Uploading "{source_file_path}" to "{destination_file_uri}"')
        destination_blob = storage.Blob.from_string(
            destination_file_uri, client=storage_client
        )
        destination_blob.upload_from_filename(filename=source_file_path)


def stage_local_data_in_gcs(
    data_path: str,
    staging_gcs_dir: Optional[str] = None,
    project: Optional[str] = None,
    location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
) -> str:
    
    data_path_obj = pathlib.Path(data_path)

    if not data_path_obj.exists():
        raise RuntimeError(f"Local data does not exist: data_path='{data_path}'")

    staging_gcs_dir = staging_gcs_dir or initializer.global_config.staging_bucket
    if not staging_gcs_dir:
        project = project or initializer.global_config.project
        location = location or initializer.global_config.location
        credentials = credentials or initializer.global_config.credentials
        
        
        
        
        
        
        staging_bucket_name = project + "-vertex-staging-" + location
        client = storage.Client(project=project, credentials=credentials)
        staging_bucket = storage.Bucket(client=client, name=staging_bucket_name)
        if not staging_bucket.exists():
            _logger.info(f'Creating staging GCS bucket "{staging_bucket_name}"')
            staging_bucket = client.create_bucket(
                bucket_or_name=staging_bucket,
                project=project,
                location=location,
            )
        staging_gcs_dir = "gs://" + staging_bucket_name

    timestamp = datetime.datetime.now().isoformat(sep="-", timespec="milliseconds")
    staging_gcs_subdir = (
        staging_gcs_dir.rstrip("/") + "/vertex_ai_auto_staging/" + timestamp
    )

    staged_data_uri = staging_gcs_subdir
    if data_path_obj.is_file():
        staged_data_uri = staging_gcs_subdir + "/" + data_path_obj.name

    _logger.info(f'Uploading "{data_path}" to "{staged_data_uri}"')
    upload_to_gcs(
        source_path=data_path,
        destination_uri=staged_data_uri,
        project=project,
        credentials=credentials,
    )

    return staged_data_uri


def generate_gcs_directory_for_pipeline_artifacts(
    project: Optional[str] = None,
    location: Optional[str] = None,
):
    
    project = project or initializer.global_config.project
    location = location or initializer.global_config.location

    pipelines_bucket_name = project + "-vertex-pipelines-" + location
    output_artifacts_gcs_dir = "gs://" + pipelines_bucket_name + "/output_artifacts/"
    return output_artifacts_gcs_dir


def create_gcs_bucket_for_pipeline_artifacts_if_it_does_not_exist(
    output_artifacts_gcs_dir: Optional[str] = None,
    service_account: Optional[str] = None,
    project: Optional[str] = None,
    location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
):
    
    project = project or initializer.global_config.project
    location = location or initializer.global_config.location
    service_account = service_account or initializer.global_config.service_account
    credentials = credentials or initializer.global_config.credentials

    output_artifacts_gcs_dir = (
        output_artifacts_gcs_dir
        or generate_gcs_directory_for_pipeline_artifacts(
            project=project,
            location=location,
        )
    )

    
    storage_client = storage.Client(
        project=project,
        credentials=credentials,
    )

    pipelines_bucket = storage.Bucket.from_string(
        uri=output_artifacts_gcs_dir,
        client=storage_client,
    )

    if not pipelines_bucket.exists():
        _logger.info(
            f'Creating GCS bucket for Vertex Pipelines: "{pipelines_bucket.name}"'
        )
        pipelines_bucket = storage_client.create_bucket(
            bucket_or_name=pipelines_bucket,
            project=project,
            location=location,
        )
        
        
        
        
        if not service_account:
            
            project_number = resource_manager_utils.get_project_number(project)
            service_account = f"{project_number}-compute@developer.gserviceaccount.com"
        bucket_iam_policy = pipelines_bucket.get_iam_policy()
        bucket_iam_policy.setdefault("roles/storage.objectCreator", set()).add(
            f"serviceAccount:{service_account}"
        )
        bucket_iam_policy.setdefault("roles/storage.objectViewer", set()).add(
            f"serviceAccount:{service_account}"
        )
        pipelines_bucket.set_iam_policy(bucket_iam_policy)
    return output_artifacts_gcs_dir


def download_file_from_gcs(
    source_file_uri: str,
    destination_file_path: str,
    project: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
):
    
    project = project or initializer.global_config.project
    credentials = credentials or initializer.global_config.credentials

    storage_client = storage.Client(project=project, credentials=credentials)
    source_blob = storage.Blob.from_string(source_file_uri, client=storage_client)

    _logger.debug(f'Downloading "{source_file_uri}" to "{destination_file_path}"')

    source_blob.download_to_filename(filename=destination_file_path)


def download_from_gcs(
    source_uri: str,
    destination_path: str,
    project: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
):
    
    project = project or initializer.global_config.project
    credentials = credentials or initializer.global_config.credentials

    storage_client = storage.Client(project=project, credentials=credentials)

    validate_gcs_path(source_uri)
    bucket_name, prefix = source_uri.replace("gs://", "").split("/", maxsplit=1)

    blobs = storage_client.list_blobs(bucket_or_name=bucket_name, prefix=prefix)
    for blob in blobs:
        
        
        if not blob.name.endswith("/"):
            rel_path = os.path.relpath(blob.name, prefix)
            filename = (
                destination_path
                if rel_path == "."
                else os.path.join(destination_path, rel_path)
            )
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            blob.download_to_filename(filename=filename)


def _upload_pandas_df_to_gcs(
    df: "pandas.DataFrame", upload_gcs_path: str, file_format: str = "jsonl"
) -> None:
    

    with tempfile.TemporaryDirectory() as temp_dir:
        local_dataset_path = os.path.join(temp_dir, "dataset.jsonl")

        if file_format == "jsonl":
            df.to_json(path_or_buf=local_dataset_path, orient="records", lines=True)
        else:
            raise ValueError(f"Unsupported file format: {file_format}")

        storage_client = storage.Client(
            project=initializer.global_config.project,
            credentials=initializer.global_config.credentials,
        )
        storage.Blob.from_string(
            uri=upload_gcs_path, client=storage_client
        ).upload_from_filename(filename=local_dataset_path)


def validate_gcs_path(gcs_path: str) -> None:
    
    if not gcs_path.startswith("gs://"):
        raise ValueError(
            f"Invalid GCS path {gcs_path}. Please provide a valid GCS path starting with 'gs://'"
        )
