
















import abc
import logging
from typing import (
    Any,
    Dict,
    FrozenSet,
    Optional,
    List,
    Tuple,
    Union,
)

from google.auth import credentials as auth_credentials

from google.cloud import aiplatform
from google.cloud.aiplatform import base
from google.cloud.aiplatform import pipeline_jobs
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.compat.types import (
    pipeline_state as gca_pipeline_state,
)
from google.cloud.aiplatform.constants import pipeline as pipeline_constants

_PIPELINE_COMPLETE_STATES = pipeline_constants._PIPELINE_COMPLETE_STATES


class _VertexAiPipelineBasedService(base.VertexAiStatefulResource):
    

    client_class = utils.PipelineJobClientWithOverride
    _resource_noun = "pipelineJob"
    _delete_method = "delete_pipeline_job"
    _getter_method = "get_pipeline_job"
    _list_method = "list_pipeline_jobs"
    _parse_resource_name_method = "parse_pipeline_job_path"
    _format_resource_name_method = "pipeline_job_path"

    _valid_done_states = _PIPELINE_COMPLETE_STATES

    @property
    @classmethod
    @abc.abstractmethod
    def _template_ref(cls) -> FrozenSet[Tuple[str, str]]:
        
        pass

    @property
    @classmethod
    @abc.abstractmethod
    def _creation_log_message(cls) -> str:
        
        pass

    @property
    @classmethod
    @abc.abstractmethod
    def _component_identifier(cls) -> str:
        
        pass

    @property
    @classmethod
    @abc.abstractmethod
    def _template_name_identifier(cls) -> Optional[str]:
        
        pass

    @classmethod
    @abc.abstractmethod
    def submit(self) -> "_VertexAiPipelineBasedService":
        
        pass

    
    @property
    @abc.abstractmethod
    def _metadata_output_artifact(self) -> Optional[str]:
        
        pass

    @property
    def backing_pipeline_job(self) -> "pipeline_jobs.PipelineJob":
        
        return pipeline_jobs.PipelineJob.get(resource_name=self.resource_name)

    @property
    def pipeline_console_uri(self) -> Optional[str]:
        
        if self.backing_pipeline_job:
            return self.backing_pipeline_job._dashboard_uri()

    @property
    def state(self) -> Optional[gca_pipeline_state.PipelineState]:
        
        if self.backing_pipeline_job:
            return self.backing_pipeline_job.state
        return None

    @classmethod
    def _does_pipeline_template_match_service(
        cls, pipeline_job: "pipeline_jobs.PipelineJob"
    ) -> bool:
        

        valid_schema_titles = ["system.Run", "system.DagExecution"]

        
        
        
        
        for component in pipeline_job.task_details:
            if not (
                "name" in component.execution
                and component.execution.schema_title in valid_schema_titles
            ):
                continue

            execution_resource = aiplatform.Execution.get(
                component.execution.name, credentials=pipeline_job.credentials
            )

            
            if (
                "component_type" in execution_resource.metadata
                and execution_resource.metadata.get("component_type")
                == cls._component_identifier
            ):
                
                if cls._template_name_identifier is None or (
                    pipeline_job.pipeline_spec is not None
                    and cls._template_name_identifier
                    == pipeline_job.pipeline_spec["pipelineInfo"]["name"]
                ):
                    return True
        return False

    
    
    @classmethod
    def _validate_pipeline_template_matches_service(
        cls, pipeline_job: "pipeline_jobs.PipelineJob"
    ):
        

        if not cls._does_pipeline_template_match_service(pipeline_job):
            raise ValueError(
                f"The provided pipeline template is not compatible with {cls.__name__}"
            )

    def __init__(
        self,
        pipeline_job_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ):
        

        super().__init__(
            project=project,
            location=location,
            credentials=credentials,
            resource_name=pipeline_job_name,
        )

        job_resource = pipeline_jobs.PipelineJob.get(
            resource_name=pipeline_job_name, credentials=credentials
        )

        self._validate_pipeline_template_matches_service(job_resource)

        self._gca_resource = job_resource._gca_resource

    @classmethod
    def _create_and_submit_pipeline_job(
        cls,
        template_params: Dict[str, Any],
        template_path: str,
        pipeline_root: Optional[str] = None,
        display_name: Optional[str] = None,
        job_id: Optional[str] = None,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        encryption_spec_key_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        experiment: Optional[Union[str, "aiplatform.Experiment"]] = None,
        enable_caching: Optional[bool] = None,
    ) -> "_VertexAiPipelineBasedService":
        

        if not display_name:
            display_name = cls._generate_display_name()

        self = cls._empty_constructor(
            project=project,
            location=location,
            credentials=credentials,
        )

        service_pipeline_job = pipeline_jobs.PipelineJob(
            display_name=display_name,
            template_path=template_path,
            job_id=job_id,
            pipeline_root=pipeline_root,
            parameter_values=template_params,
            encryption_spec_key_name=encryption_spec_key_name,
            project=project,
            location=location,
            credentials=credentials,
            enable_caching=enable_caching,
        )

        
        
        
        logging.getLogger("google.cloud.aiplatform.pipeline_jobs").setLevel(
            logging.WARNING
        )

        service_pipeline_job.submit(
            service_account=service_account,
            network=network,
            experiment=experiment,
        )

        logging.getLogger("google.cloud.aiplatform.pipeline_jobs").setLevel(
            logging.INFO
        )

        self._gca_resource = service_pipeline_job.gca_resource

        return self

    @classmethod
    def list(
        cls,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[str] = None,
    ) -> List["_VertexAiPipelineBasedService"]:
        

        filter_str = f"metadata.component_type.string_value={cls._component_identifier}"

        filtered_pipeline_executions = aiplatform.Execution.list(
            filter=filter_str, credentials=credentials
        )

        service_pipeline_jobs = []

        for pipeline_execution in filtered_pipeline_executions:
            if "pipeline_job_resource_name" in pipeline_execution.metadata:
                
                
                
                
                
                try:
                    service_pipeline_job = cls(
                        pipeline_execution.metadata["pipeline_job_resource_name"],
                        project=project,
                        location=location,
                        credentials=credentials,
                    )
                    service_pipeline_jobs.append(service_pipeline_job)
                except ValueError:
                    continue

        return service_pipeline_jobs

    def wait(self):
        
        pipeline_run = self.backing_pipeline_job

        if pipeline_run._latest_future is None:
            pipeline_run._block_until_complete()
        else:
            pipeline_run.wait()
