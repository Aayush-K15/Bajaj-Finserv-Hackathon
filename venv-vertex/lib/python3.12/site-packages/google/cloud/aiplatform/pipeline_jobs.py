
















import datetime
import logging
import re
import tempfile
import time
from typing import Any, Callable, Dict, List, Optional, Union
from google.api_core import operation

from google.auth import credentials as auth_credentials
from google.cloud import aiplatform
from google.cloud import aiplatform_v1
from google.cloud.aiplatform import base
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.constants import pipeline as pipeline_constants
from google.cloud.aiplatform.metadata import artifact
from google.cloud.aiplatform.metadata import constants as metadata_constants
from google.cloud.aiplatform.metadata import context
from google.cloud.aiplatform.metadata import execution
from google.cloud.aiplatform.metadata import experiment_resources
from google.cloud.aiplatform.metadata import utils as metadata_utils
from google.cloud.aiplatform.utils import gcs_utils
from google.cloud.aiplatform.utils import pipeline_utils
from google.cloud.aiplatform.utils import yaml_utils
from google.protobuf import field_mask_pb2 as field_mask
from google.protobuf import json_format

from google.cloud.aiplatform.compat.types import (
    pipeline_job as gca_pipeline_job,
    pipeline_state as gca_pipeline_state,
)
from google.cloud.aiplatform_v1.types import (
    pipeline_service as PipelineServiceV1,
)
from google.cloud.aiplatform_v1.services.pipeline_service import (
    PipelineServiceClient as PipelineServiceClientGa,
)

_LOGGER = base.Logger(__name__)

_PIPELINE_COMPLETE_STATES = pipeline_constants._PIPELINE_COMPLETE_STATES

_PIPELINE_ERROR_STATES = pipeline_constants._PIPELINE_ERROR_STATES


_VALID_NAME_PATTERN = pipeline_constants._VALID_NAME_PATTERN


_VALID_AR_URL = pipeline_constants._VALID_AR_URL


_VALID_HTTPS_URL = pipeline_constants._VALID_HTTPS_URL

_READ_MASK_FIELDS = pipeline_constants._READ_MASK_FIELDS


_JOB_WAIT_TIME = 5  
_LOG_WAIT_TIME = 5
_MAX_WAIT_TIME = 60 * 5  
_WAIT_TIME_MULTIPLIER = 2  


def _get_current_time() -> datetime.datetime:
    
    return datetime.datetime.now()


def _set_enable_caching_value(
    pipeline_spec: Dict[str, Any], enable_caching: bool
) -> None:
    
    for component in [pipeline_spec["root"]] + list(
        pipeline_spec["components"].values()
    ):
        if "dag" in component:
            for task in component["dag"]["tasks"].values():
                task["cachingOptions"] = {"enableCache": enable_caching}


class PipelineJob(
    base.VertexAiStatefulResource,
    experiment_resources._ExperimentLoggable,
    experiment_loggable_schemas=(
        experiment_resources._ExperimentLoggableSchema(
            title=metadata_constants.SYSTEM_PIPELINE_RUN
        ),
    ),
):
    client_class = utils.PipelineJobClientWithOverride
    _resource_noun = "pipelineJobs"
    _delete_method = "delete_pipeline_job"
    _getter_method = "get_pipeline_job"
    _list_method = "list_pipeline_jobs"
    _parse_resource_name_method = "parse_pipeline_job_path"
    _format_resource_name_method = "pipeline_job_path"

    
    _valid_done_states = _PIPELINE_COMPLETE_STATES

    def __init__(
        self,
        
        display_name: str,
        template_path: str,
        job_id: Optional[str] = None,
        pipeline_root: Optional[str] = None,
        parameter_values: Optional[Dict[str, Any]] = None,
        input_artifacts: Optional[Dict[str, str]] = None,
        enable_caching: Optional[bool] = None,
        encryption_spec_key_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        failure_policy: Optional[str] = None,
    ):
        
        if not display_name:
            display_name = self.__class__._generate_display_name()
        utils.validate_display_name(display_name)

        if labels:
            utils.validate_labels(labels)

        super().__init__(project=project, location=location, credentials=credentials)

        self._parent = initializer.global_config.common_location_path(
            project=project, location=location
        )

        
        pipeline_json = yaml_utils.load_yaml(
            template_path, self.project, self.credentials
        )

        
        if pipeline_json.get("pipelineSpec") is not None:
            pipeline_job = pipeline_json
            pipeline_root = (
                pipeline_root
                or pipeline_job["pipelineSpec"].get("defaultPipelineRoot")
                or pipeline_job["runtimeConfig"].get("gcsOutputDirectory")
                or initializer.global_config.staging_bucket
            )
        else:
            pipeline_job = {
                "pipelineSpec": pipeline_json,
                "runtimeConfig": {},
            }
            pipeline_root = (
                pipeline_root
                or pipeline_job["pipelineSpec"].get("defaultPipelineRoot")
                or initializer.global_config.staging_bucket
            )
        pipeline_root = (
            pipeline_root
            or gcs_utils.generate_gcs_directory_for_pipeline_artifacts(
                project=project,
                location=location,
            )
        )
        builder = pipeline_utils.PipelineRuntimeConfigBuilder.from_job_spec_json(
            pipeline_job
        )
        builder.update_pipeline_root(pipeline_root)
        builder.update_runtime_parameters(parameter_values)
        builder.update_input_artifacts(input_artifacts)

        builder.update_failure_policy(failure_policy)
        runtime_config_dict = builder.build()

        runtime_config = gca_pipeline_job.PipelineJob.RuntimeConfig()._pb
        json_format.ParseDict(runtime_config_dict, runtime_config)

        pipeline_name = pipeline_job["pipelineSpec"]["pipelineInfo"]["name"]
        self.job_id = job_id or "{pipeline_name}-{timestamp}".format(
            pipeline_name=re.sub("[^-0-9a-z]+", "-", pipeline_name.lower())
            .lstrip("-")
            .rstrip("-"),
            timestamp=_get_current_time().strftime("%Y%m%d%H%M%S"),
        )
        if not _VALID_NAME_PATTERN.match(self.job_id):
            raise ValueError(
                f"Generated job ID: {self.job_id} is illegal as a Vertex pipelines job ID. "
                "Expecting an ID following the regex pattern "
                f'"{_VALID_NAME_PATTERN.pattern[1:-1]}"'
            )

        if enable_caching is not None:
            _set_enable_caching_value(pipeline_job["pipelineSpec"], enable_caching)

        pipeline_job_args = {
            "display_name": display_name,
            "pipeline_spec": pipeline_job["pipelineSpec"],
            "labels": labels,
            "runtime_config": runtime_config,
            "encryption_spec": initializer.global_config.get_encryption_spec(
                encryption_spec_key_name=encryption_spec_key_name
            ),
        }

        if _VALID_AR_URL.match(template_path) or _VALID_HTTPS_URL.match(template_path):
            pipeline_job_args["template_uri"] = template_path

        self._gca_resource = gca_pipeline_job.PipelineJob(**pipeline_job_args)

    def run(
        self,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        reserved_ip_ranges: Optional[List[str]] = None,
        sync: Optional[bool] = True,
        create_request_timeout: Optional[float] = None,
        enable_preflight_validations: Optional[bool] = False,
    ) -> None:
        
        network = network or initializer.global_config.network

        self._run(
            service_account=service_account,
            network=network,
            reserved_ip_ranges=reserved_ip_ranges,
            sync=sync,
            create_request_timeout=create_request_timeout,
            enable_preflight_validations=enable_preflight_validations,
        )

    @base.optional_sync()
    def _run(
        self,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        reserved_ip_ranges: Optional[List[str]] = None,
        sync: Optional[bool] = True,
        create_request_timeout: Optional[float] = None,
        enable_preflight_validations: Optional[bool] = False,
    ) -> None:
        
        self.submit(
            service_account=service_account,
            network=network,
            reserved_ip_ranges=reserved_ip_ranges,
            create_request_timeout=create_request_timeout,
            enable_preflight_validations=enable_preflight_validations,
        )

        self._block_until_complete()

        
        for details in self.task_details:
            if details.task_name == "model-evaluation-text-generation-pairwise":
                model_a_eval = details.execution.metadata[
                    "output:model_a_evaluation_path"
                ]
                model_b_eval = details.execution.metadata[
                    "output:model_b_evaluation_path"
                ]
                if model_a_eval:
                    _LOGGER.info("Model A")
                    utils._ipython_utils.display_model_evaluation_button(
                        aiplatform.ModelEvaluation(model_a_eval),
                    )
                if model_b_eval:
                    _LOGGER.info("Model B")
                    utils._ipython_utils.display_model_evaluation_button(
                        aiplatform.ModelEvaluation(model_b_eval),
                    )
                break

    def submit(
        self,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        reserved_ip_ranges: Optional[List[str]] = None,
        create_request_timeout: Optional[float] = None,
        *,
        experiment: Optional[Union[str, experiment_resources.Experiment]] = None,
        enable_preflight_validations: Optional[bool] = False,
    ) -> None:
        
        network = network or initializer.global_config.network
        service_account = service_account or initializer.global_config.service_account

        if service_account:
            self._gca_resource.service_account = service_account

        if network:
            self._gca_resource.network = network

        if reserved_ip_ranges:
            self._gca_resource.reserved_ip_ranges = reserved_ip_ranges

        try:
            output_artifacts_gcs_dir = (
                self._gca_resource.runtime_config.gcs_output_directory
            )
            assert output_artifacts_gcs_dir
            gcs_utils.create_gcs_bucket_for_pipeline_artifacts_if_it_does_not_exist(
                output_artifacts_gcs_dir=output_artifacts_gcs_dir,
                service_account=self._gca_resource.service_account,
                project=self.project,
                location=self.location,
                credentials=self.credentials,
            )
        except:  
            _LOGGER.exception(
                "Error when trying to get or create a GCS bucket for the pipeline output artifacts"
            )

        
        if self._gca_resource.pipeline_spec.get("sdkVersion", "").startswith("tfx"):
            _LOGGER.setLevel(logging.INFO)

        if experiment:
            self._validate_experiment(experiment)

        _LOGGER.log_create_with_lro(self.__class__)

        if enable_preflight_validations:
            self._gca_resource.preflight_validations = True

        def extract_error_messages(error_string):
            

            message_pattern = (
                r"CreatePipelineJobApiErrorDetail\"\n.*message=(.*),\ cause=null"
            )

            matches = re.findall(message_pattern, error_string)

            formatted_errors = [
                f"{i+1}. {message}" for i, message in enumerate(matches)
            ]

            return formatted_errors

        try:
            self._gca_resource = self.api_client.create_pipeline_job(
                parent=self._parent,
                pipeline_job=self._gca_resource,
                pipeline_job_id=self.job_id,
                timeout=create_request_timeout,
            )
        except Exception as e:
            preflight_validations_error_messages = extract_error_messages(str(e))
            if preflight_validations_error_messages:
                raise Exception(
                    "PipelineJob Preflight validations failed with the following errors:\n"
                    + "\n".join(preflight_validations_error_messages)
                ) from e
            raise

        _LOGGER.log_create_complete_with_getter(
            self.__class__, self._gca_resource, "pipeline_job"
        )

        _LOGGER.info("View Pipeline Job:\n%s" % self._dashboard_uri())

        if experiment:
            self._associate_to_experiment(experiment)

    def create_schedule(
        self,
        cron: str,
        display_name: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        allow_queueing: bool = False,
        max_run_count: Optional[int] = None,
        max_concurrent_run_count: int = 1,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        create_request_timeout: Optional[float] = None,
    ) -> "aiplatform.PipelineJobSchedule":
        
        if not display_name:
            display_name = self._generate_display_name(prefix="PipelineJobSchedule")
        utils.validate_display_name(display_name)

        pipeline_job_schedule = aiplatform.PipelineJobSchedule(
            pipeline_job=self,
            display_name=display_name,
            credentials=self.credentials,
            project=self.project,
            location=self.location,
        )

        pipeline_job_schedule.create(
            cron=cron,
            start_time=start_time,
            end_time=end_time,
            allow_queueing=allow_queueing,
            max_run_count=max_run_count,
            max_concurrent_run_count=max_concurrent_run_count,
            service_account=service_account,
            network=network,
            create_request_timeout=create_request_timeout,
        )
        return pipeline_job_schedule

    def wait(self):
        
        if self._latest_future is None:
            self._block_until_complete()
        else:
            super().wait()

    def batch_delete(
        self,
        project: str,
        location: str,
        names: List[str],
    ) -> PipelineServiceV1.BatchDeletePipelineJobsResponse:
        
        user_project = project or initializer.global_config.project
        user_location = location or initializer.global_config.location
        parent = initializer.global_config.common_location_path(
            project=user_project, location=user_location
        )
        pipeline_jobs_names = [
            utils.full_resource_name(
                resource_name=name,
                resource_noun="pipelineJobs",
                parse_resource_name_method=PipelineServiceClientGa.parse_pipeline_job_path,
                format_resource_name_method=PipelineServiceClientGa.pipeline_job_path,
                project=user_project,
                location=user_location,
            )
            for name in names
        ]
        request = aiplatform_v1.BatchDeletePipelineJobsRequest(
            parent=parent, names=pipeline_jobs_names
        )
        operation = self.api_client.batch_delete_pipeline_jobs(request)
        return operation.result()

    def batch_cancel(
        self,
        project: str,
        location: str,
        names: List[str],
    ) -> operation.Operation:
        
        user_project = project or initializer.global_config.project
        user_location = location or initializer.global_config.location
        parent = initializer.global_config.common_location_path(
            project=user_project, location=user_location
        )
        pipeline_jobs_names = [
            utils.full_resource_name(
                resource_name=name,
                resource_noun="pipelineJobs",
                parse_resource_name_method=PipelineServiceClientGa.parse_pipeline_job_path,
                format_resource_name_method=PipelineServiceClientGa.pipeline_job_path,
                project=user_project,
                location=user_location,
            )
            for name in names
        ]
        request = aiplatform_v1.BatchCancelPipelineJobsRequest(
            parent=parent, names=pipeline_jobs_names
        )
        return self.api_client.batch_cancel_pipeline_jobs(request)

    @property
    def pipeline_spec(self):
        return self._gca_resource.pipeline_spec

    @property
    def runtime_config(self) -> gca_pipeline_job.PipelineJob.RuntimeConfig:
        return self._gca_resource.runtime_config

    @property
    def state(self) -> Optional[gca_pipeline_state.PipelineState]:
        
        self._sync_gca_resource()
        return self._gca_resource.state

    @property
    def task_details(self) -> List[gca_pipeline_job.PipelineTaskDetail]:
        self._sync_gca_resource()
        return list(self._gca_resource.job_detail.task_details)

    @property
    def has_failed(self) -> bool:
        
        return self.state == gca_pipeline_state.PipelineState.PIPELINE_STATE_FAILED

    def _dashboard_uri(self) -> str:
        
        fields = self._parse_resource_name(self.resource_name)
        url = f"https://console.cloud.google.com/vertex-ai/locations/{fields['location']}/pipelines/runs/{fields['pipeline_job']}?project={fields['project']}"
        return url

    def _block_until_complete(self):
        
        
        wait = _JOB_WAIT_TIME  
        log_wait = _LOG_WAIT_TIME
        max_wait = _MAX_WAIT_TIME  
        multiplier = _WAIT_TIME_MULTIPLIER  

        previous_time = time.time()
        while self.state not in _PIPELINE_COMPLETE_STATES:
            current_time = time.time()
            if current_time - previous_time >= log_wait:
                _LOGGER.info(
                    "%s %s current state:\n%s"
                    % (
                        self.__class__.__name__,
                        self._gca_resource.name,
                        self._gca_resource.state,
                    )
                )
                log_wait = min(log_wait * multiplier, max_wait)
                previous_time = current_time
            time.sleep(wait)

        
        
        if self._gca_resource.state in _PIPELINE_ERROR_STATES:
            raise RuntimeError("Job failed with:\n%s" % self._gca_resource.error)
        else:
            _LOGGER.log_action_completed_against_resource("run", "completed", self)

    @classmethod
    def get(
        cls,
        resource_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ) -> "PipelineJob":
        
        self = cls._empty_constructor(
            project=project,
            location=location,
            credentials=credentials,
            resource_name=resource_name,
        )

        self._gca_resource = self._get_gca_resource(resource_name=resource_name)

        return self

    def cancel(self) -> None:
        
        self.api_client.cancel_pipeline_job(name=self.resource_name)

    @classmethod
    def list(
        cls,
        filter: Optional[str] = None,
        order_by: Optional[str] = None,
        enable_simple_view: bool = False,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ) -> List["PipelineJob"]:
        

        read_mask_fields = None

        if enable_simple_view:
            read_mask_fields = field_mask.FieldMask(paths=_READ_MASK_FIELDS)
            _LOGGER.warn(
                "By enabling simple view, the PipelineJob resources returned from this method will not contain all fields."
            )

        return cls._list_with_local_order(
            filter=filter,
            order_by=order_by,
            read_mask=read_mask_fields,
            project=project,
            location=location,
            credentials=credentials,
        )

    def wait_for_resource_creation(self) -> None:
        
        self._wait_for_resource_creation()

    def done(self) -> bool:
        
        if not self._gca_resource:
            return False

        return self.state in _PIPELINE_COMPLETE_STATES

    def _get_context(self) -> context.Context:
        
        self.wait_for_resource_creation()
        pipeline_run_context = self._gca_resource.job_detail.pipeline_run_context

        
        while not self.done():
            pipeline_run_context = self._gca_resource.job_detail.pipeline_run_context
            if pipeline_run_context:
                break
            time.sleep(1)

        if not pipeline_run_context:
            if self.has_failed:
                raise RuntimeError(
                    f"Cannot associate PipelineJob to Experiment: {self.gca_resource.error}"
                )
            else:
                raise RuntimeError(
                    "Cannot associate PipelineJob to Experiment because PipelineJob context could not be found."
                )

        return context.Context(
            resource=pipeline_run_context,
            project=self.project,
            location=self.location,
            credentials=self.credentials,
        )

    @classmethod
    def _query_experiment_row(
        cls,
        node: context.Context,
        experiment: Optional[experiment_resources.Experiment] = None,
        include_time_series: bool = True,
    ) -> experiment_resources._ExperimentRow:
        

        system_run_executions = execution.Execution.list(
            project=node.project,
            location=node.location,
            credentials=node.credentials,
            filter=metadata_utils._make_filter_string(
                in_context=[node.resource_name],
                schema_title=metadata_constants.SYSTEM_RUN,
            ),
        )

        metric_artifacts = artifact.Artifact.list(
            project=node.project,
            location=node.location,
            credentials=node.credentials,
            filter=metadata_utils._make_filter_string(
                in_context=[node.resource_name],
                schema_title=[
                    metadata_constants.SYSTEM_METRICS,
                    metadata_constants.GOOGLE_CLASSIFICATION_METRICS,
                    metadata_constants.GOOGLE_REGRESSION_METRICS,
                    metadata_constants.GOOGLE_FORECASTING_METRICS,
                ],
            ),
        )
        row = experiment_resources._ExperimentRow(
            experiment_run_type=node.schema_title, name=node.display_name
        )

        if system_run_executions:
            row.params = {}
            for key, value in system_run_executions[0].metadata.items():
                if key.startswith(metadata_constants.PIPELINE_PARAM_PREFIX):
                    row.params[
                        key[len(metadata_constants.PIPELINE_PARAM_PREFIX) :]
                    ] = value
            row.state = system_run_executions[0].state.name

        for metric_artifact in metric_artifacts:
            if row.metrics:
                row.metrics.update(metric_artifact.metadata)
            else:
                row.metrics = metric_artifact.metadata

        return row

    def clone(
        self,
        display_name: Optional[str] = None,
        job_id: Optional[str] = None,
        pipeline_root: Optional[str] = None,
        parameter_values: Optional[Dict[str, Any]] = None,
        input_artifacts: Optional[Dict[str, str]] = None,
        enable_caching: Optional[bool] = None,
        encryption_spec_key_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
    ) -> "PipelineJob":
        
        
        if not project:
            project = self.project
        if not location:
            location = self.location
        if not credentials:
            credentials = self.credentials

        cloned = self.__class__._empty_constructor(
            project=project,
            location=location,
            credentials=credentials,
        )
        cloned._parent = initializer.global_config.common_location_path(
            project=project, location=location
        )

        
        pipeline_job = json_format.MessageToDict(self._gca_resource._pb)

        
        pipeline_spec = pipeline_job["pipelineSpec"]
        if "deploymentConfig" in pipeline_spec:
            del pipeline_spec["deploymentConfig"]

        
        if enable_caching is not None:
            _set_enable_caching_value(pipeline_spec, enable_caching)

        
        pipeline_name = pipeline_spec["pipelineInfo"]["name"]
        cloned.job_id = job_id or "cloned-{pipeline_name}-{timestamp}".format(
            pipeline_name=re.sub("[^-0-9a-z]+", "-", pipeline_name.lower())
            .lstrip("-")
            .rstrip("-"),
            timestamp=_get_current_time().strftime("%Y%m%d%H%M%S"),
        )
        if not _VALID_NAME_PATTERN.match(cloned.job_id):
            raise ValueError(
                f"Generated job ID: {cloned.job_id} is illegal as a Vertex pipelines job ID. "
                "Expecting an ID following the regex pattern "
                f'"{_VALID_NAME_PATTERN.pattern[1:-1]}"'
            )

        
        if display_name:
            utils.validate_display_name(display_name)
        elif not display_name and "displayName" in pipeline_job:
            display_name = pipeline_job["displayName"]

        if labels:
            utils.validate_labels(labels)
        elif not labels and "labels" in pipeline_job:
            labels = pipeline_job["labels"]

        if encryption_spec_key_name or "encryptionSpec" not in pipeline_job:
            encryption_spec = initializer.global_config.get_encryption_spec(
                encryption_spec_key_name=encryption_spec_key_name
            )
        else:
            encryption_spec = pipeline_job["encryptionSpec"]

        
        builder = pipeline_utils.PipelineRuntimeConfigBuilder.from_job_spec_json(
            pipeline_job
        )
        builder.update_pipeline_root(pipeline_root)
        builder.update_runtime_parameters(parameter_values)
        builder.update_input_artifacts(input_artifacts)
        runtime_config_dict = builder.build()
        runtime_config = gca_pipeline_job.PipelineJob.RuntimeConfig()._pb
        json_format.ParseDict(runtime_config_dict, runtime_config)

        
        cloned._gca_resource = gca_pipeline_job.PipelineJob(
            display_name=display_name,
            pipeline_spec=pipeline_spec,
            labels=labels,
            runtime_config=runtime_config,
            encryption_spec=encryption_spec,
        )

        return cloned

    @staticmethod
    def from_pipeline_func(
        
        pipeline_func: Callable,
        parameter_values: Optional[Dict[str, Any]] = None,
        input_artifacts: Optional[Dict[str, str]] = None,
        output_artifacts_gcs_dir: Optional[str] = None,
        enable_caching: Optional[bool] = None,
        context_name: Optional[str] = "pipeline",
        display_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        job_id: Optional[str] = None,
        
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        encryption_spec_key_name: Optional[str] = None,
    ) -> "PipelineJob":
        

        
        try:
            from kfp.v2 import compiler as compiler_v2
        except ImportError as err:
            raise RuntimeError(
                "Cannot import the kfp.v2.compiler module. Please install or update the kfp package."
            ) from err

        automatic_display_name = " ".join(
            [
                pipeline_func.__name__.replace("_", " "),
                datetime.datetime.now().isoformat(sep=" "),
            ]
        )
        display_name = display_name or automatic_display_name
        job_id = job_id or re.sub(
            r"[^-a-z0-9]", "-", automatic_display_name.lower()
        ).strip("-")
        pipeline_file = tempfile.mkstemp(suffix=".json")
        compiler_v2.Compiler().compile(
            pipeline_func=pipeline_func,
            pipeline_name=context_name,
            package_path=pipeline_file,
        )
        pipeline_job = PipelineJob(
            template_path=pipeline_file,
            parameter_values=parameter_values,
            input_artifacts=input_artifacts,
            pipeline_root=output_artifacts_gcs_dir,
            enable_caching=enable_caching,
            display_name=display_name,
            job_id=job_id,
            labels=labels,
            project=project,
            location=location,
            credentials=credentials,
            encryption_spec_key_name=encryption_spec_key_name,
        )
        return pipeline_job

    def get_associated_experiment(self) -> Optional["aiplatform.Experiment"]:
        

        pipeline_parent_contexts = (
            self._gca_resource.job_detail.pipeline_run_context.parent_contexts
        )

        pipeline_experiment_resources = [
            context.Context(resource_name=c)._gca_resource
            for c in pipeline_parent_contexts
            if c != self._gca_resource.job_detail.pipeline_context.name
        ]

        pipeline_experiment_resource_names = []

        for c in pipeline_experiment_resources:
            if c.schema_title == metadata_constants.SYSTEM_EXPERIMENT:
                pipeline_experiment_resource_names.append(c.name)

        if len(pipeline_experiment_resource_names) > 1:
            _LOGGER.warning(
                f"There is more than one Experiment is associated with this pipeline."
                f"The following experiments were found: {pipeline_experiment_resource_names.join(', ')}\n"
                f"Returning only the following experiment: {pipeline_experiment_resource_names[0]}"
            )

        if len(pipeline_experiment_resource_names) >= 1:
            return experiment_resources.Experiment(
                pipeline_experiment_resource_names[0],
                project=self.project,
                location=self.location,
                credentials=self.credentials,
            )
