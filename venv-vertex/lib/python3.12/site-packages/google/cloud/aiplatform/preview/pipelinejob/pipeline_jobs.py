
















import datetime
import re
from typing import Any, Dict, List, Optional

from google.auth import credentials as auth_credentials
from google.cloud import aiplatform_v1beta1
from google.cloud.aiplatform import base
from google.cloud.aiplatform import compat
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import pipeline_job_schedules
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.constants import pipeline as pipeline_constants
from google.cloud.aiplatform.metadata import constants as metadata_constants
from google.cloud.aiplatform.metadata import experiment_resources
from google.cloud.aiplatform.pipeline_jobs import (
    PipelineJob as PipelineJobGa,
)
from google.cloud.aiplatform_v1.services.pipeline_service import (
    PipelineServiceClient as PipelineServiceClientGa,
)

from google.protobuf import json_format


_LOGGER = base.Logger(__name__)


_VALID_NAME_PATTERN = pipeline_constants._VALID_NAME_PATTERN


_VALID_AR_URL = pipeline_constants._VALID_AR_URL


_VALID_HTTPS_URL = pipeline_constants._VALID_HTTPS_URL


def _get_current_time() -> datetime.datetime:
    
    return datetime.datetime.now()


def _set_enable_caching_value(
    pipeline_spec: Dict[str, Any], enable_caching: bool
) -> None:
    
    for component in [pipeline_spec["root"]] + list(
        pipeline_spec["components"].values()
    ):
        if "dag" in component:
            for task in component["dag"]["tasks"].values():
                task["cachingOptions"] = {"enableCache": enable_caching}


class _PipelineJob(
    PipelineJobGa,
    experiment_loggable_schemas=(
        experiment_resources._ExperimentLoggableSchema(
            title=metadata_constants.SYSTEM_PIPELINE_RUN
        ),
    ),
):
    

    def __init__(
        self,
        display_name: str,
        template_path: str,
        job_id: Optional[str] = None,
        pipeline_root: Optional[str] = None,
        parameter_values: Optional[Dict[str, Any]] = None,
        input_artifacts: Optional[Dict[str, str]] = None,
        enable_caching: Optional[bool] = None,
        encryption_spec_key_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        failure_policy: Optional[str] = None,
        enable_preflight_validations: Optional[bool] = False,
        default_runtime: Optional[Dict[str, Any]] = None,
    ):
        
        super().__init__(
            display_name=display_name,
            template_path=template_path,
            job_id=job_id,
            pipeline_root=pipeline_root,
            parameter_values=parameter_values,
            input_artifacts=input_artifacts,
            enable_caching=enable_caching,
            encryption_spec_key_name=encryption_spec_key_name,
            labels=labels,
            credentials=credentials,
            project=project,
            location=location,
            failure_policy=failure_policy,
        )

        
        pipeline_json = utils.yaml_utils.load_yaml(
            template_path, self.project, self.credentials
        )

        
        if pipeline_json.get("pipelineSpec") is not None:
            pipeline_job = pipeline_json
            pipeline_root = (
                pipeline_root
                or pipeline_job["pipelineSpec"].get("defaultPipelineRoot")
                or pipeline_job["runtimeConfig"].get("gcsOutputDirectory")
                or initializer.global_config.staging_bucket
            )
        else:
            pipeline_job = {
                "pipelineSpec": pipeline_json,
                "runtimeConfig": {},
            }
            pipeline_root = (
                pipeline_root
                or pipeline_job["pipelineSpec"].get("defaultPipelineRoot")
                or initializer.global_config.staging_bucket
            )
        pipeline_root = (
            pipeline_root
            or utils.gcs_utils.generate_gcs_directory_for_pipeline_artifacts(
                project=project,
                location=location,
            )
        )
        builder = utils.pipeline_utils.PipelineRuntimeConfigBuilder.from_job_spec_json(
            pipeline_job
        )
        builder.update_pipeline_root(pipeline_root)
        builder.update_runtime_parameters(parameter_values)
        builder.update_input_artifacts(input_artifacts)

        builder.update_failure_policy(failure_policy)
        builder.update_default_runtime(default_runtime)
        runtime_config_dict = builder.build()
        runtime_config = aiplatform_v1beta1.PipelineJob.RuntimeConfig()._pb
        json_format.ParseDict(runtime_config_dict, runtime_config)

        pipeline_name = pipeline_job["pipelineSpec"]["pipelineInfo"]["name"]
        self.job_id = job_id or "{pipeline_name}-{timestamp}".format(
            pipeline_name=re.sub("[^-0-9a-z]+", "-", pipeline_name.lower())
            .lstrip("-")
            .rstrip("-"),
            timestamp=_get_current_time().strftime("%Y%m%d%H%M%S"),
        )
        if not _VALID_NAME_PATTERN.match(self.job_id):
            raise ValueError(
                f"Generated job ID: {self.job_id} is illegal as a Vertex pipelines job ID. "
                "Expecting an ID following the regex pattern "
                f'"{_VALID_NAME_PATTERN.pattern[1:-1]}"'
            )

        if enable_caching is not None:
            _set_enable_caching_value(pipeline_job["pipelineSpec"], enable_caching)

        pipeline_job_args = {
            "display_name": display_name,
            "pipeline_spec": pipeline_job["pipelineSpec"],
            "labels": labels,
            "runtime_config": runtime_config,
            "encryption_spec": initializer.global_config.get_encryption_spec(
                encryption_spec_key_name=encryption_spec_key_name
            ),
            "preflight_validations": enable_preflight_validations,
        }

        if _VALID_AR_URL.match(template_path) or _VALID_HTTPS_URL.match(template_path):
            pipeline_job_args["template_uri"] = template_path

        self._v1_beta1_pipeline_job = aiplatform_v1beta1.PipelineJob(
            **pipeline_job_args
        )

    def create_schedule(
        self,
        cron_expression: str,
        display_name: str,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        allow_queueing: bool = False,
        max_run_count: Optional[int] = None,
        max_concurrent_run_count: int = 1,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        create_request_timeout: Optional[float] = None,
    ) -> "pipeline_job_schedules.PipelineJobSchedule":  
        
        return super().create_schedule(
            cron=cron_expression,
            display_name=display_name,
            start_time=start_time,
            end_time=end_time,
            allow_queueing=allow_queueing,
            max_run_count=max_run_count,
            max_concurrent_run_count=max_concurrent_run_count,
            service_account=service_account,
            network=network,
            create_request_timeout=create_request_timeout,
        )

    @classmethod
    def batch_delete(
        cls,
        names: List[str],
        project: Optional[str] = None,
        location: Optional[str] = None,
    ) -> aiplatform_v1beta1.BatchDeletePipelineJobsResponse:
        
        user_project = project or initializer.global_config.project
        user_location = location or initializer.global_config.location
        parent = initializer.global_config.common_location_path(
            project=user_project, location=user_location
        )
        pipeline_jobs_names = [
            utils.full_resource_name(
                resource_name=name,
                resource_noun="pipelineJobs",
                parse_resource_name_method=PipelineServiceClientGa.parse_pipeline_job_path,
                format_resource_name_method=PipelineServiceClientGa.pipeline_job_path,
                project=user_project,
                location=user_location,
            )
            for name in names
        ]
        request = aiplatform_v1beta1.BatchDeletePipelineJobsRequest(
            parent=parent, names=pipeline_jobs_names
        )
        client = cls._instantiate_client(
            location=user_location,
            appended_user_agent=["preview-pipeline-jobs-batch-delete"],
        )
        v1beta1_client = client.select_version(compat.V1BETA1)
        operation = v1beta1_client.batch_delete_pipeline_jobs(request)
        return operation.result()

    def submit(
        self,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        reserved_ip_ranges: Optional[List[str]] = None,
        create_request_timeout: Optional[float] = None,
        job_id: Optional[str] = None,
    ) -> None:
        
        network = network or initializer.global_config.network
        service_account = service_account or initializer.global_config.service_account
        gca_resouce = self._v1_beta1_pipeline_job

        if service_account:
            gca_resouce.service_account = service_account

        if network:
            gca_resouce.network = network

        if reserved_ip_ranges:
            gca_resouce.reserved_ip_ranges = reserved_ip_ranges
        user_project = initializer.global_config.project
        user_location = initializer.global_config.location
        parent = initializer.global_config.common_location_path(
            project=user_project, location=user_location
        )

        client = self._instantiate_client(
            location=user_location,
            appended_user_agent=["preview-pipeline-job-submit"],
        )
        v1beta1_client = client.select_version(compat.V1BETA1)

        _LOGGER.log_create_with_lro(self.__class__)

        request = aiplatform_v1beta1.CreatePipelineJobRequest(
            parent=parent,
            pipeline_job=self._v1_beta1_pipeline_job,
            pipeline_job_id=job_id or self.job_id,
        )

        response = v1beta1_client.create_pipeline_job(request=request)

        self._gca_resource = response

        _LOGGER.log_create_complete_with_getter(
            self.__class__, self._gca_resource, "pipeline_job"
        )

        _LOGGER.info("View Pipeline Job:\n%s" % self._dashboard_uri())

    def rerun(
        self,
        original_pipelinejob_name: str,
        pipeline_task_rerun_configs: Optional[
            List[aiplatform_v1beta1.PipelineTaskRerunConfig]
        ] = None,
        parameter_values: Optional[Dict[str, Any]] = None,
        job_id: Optional[str] = None,
        service_account: Optional[str] = None,
        network: Optional[str] = None,
        reserved_ip_ranges: Optional[List[str]] = None,
    ) -> None:
        
        network = network or initializer.global_config.network
        service_account = service_account or initializer.global_config.service_account
        gca_resouce = self._v1_beta1_pipeline_job

        if service_account:
            gca_resouce.service_account = service_account

        if network:
            gca_resouce.network = network

        if reserved_ip_ranges:
            gca_resouce.reserved_ip_ranges = reserved_ip_ranges
        user_project = initializer.global_config.project
        user_location = initializer.global_config.location
        parent = initializer.global_config.common_location_path(
            project=user_project, location=user_location
        )

        client = self._instantiate_client(
            location=user_location,
            appended_user_agent=["preview-pipeline-job-submit"],
        )
        v1beta1_client = client.select_version(compat.V1BETA1)

        _LOGGER.log_create_with_lro(self.__class__)

        pipeline_job = self._v1_beta1_pipeline_job
        try:
            get_request = aiplatform_v1beta1.GetPipelineJobRequest(
                name=original_pipelinejob_name
            )
            original_pipeline_job = v1beta1_client.get_pipeline_job(request=get_request)
            pipeline_job.original_pipeline_job_id = int(
                original_pipeline_job.labels["vertex-ai-pipelines-run-billing-id"]
            )
            original_pipeline_task_details = (
                original_pipeline_job.job_detail.task_details
            )
        except Exception as e:
            raise ValueError(
                f"Failed to get original pipeline job: {original_pipelinejob_name}"
            ) from e

        task_id_to_task_rerun_config = {}
        for task_rerun_config in pipeline_task_rerun_configs:
            task_id_to_task_rerun_config[task_rerun_config.task_id] = task_rerun_config

        pipeline_job.pipeline_task_rerun_configs = []
        for task_detail in original_pipeline_task_details:
            if task_detail.task_id in task_id_to_task_rerun_config:
                task_rerun_config = task_id_to_task_rerun_config[task_detail.task_id]
                if task_detail.task_unique_name:
                    task_rerun_config.task_name = task_detail.task_unique_name
                pipeline_job.pipeline_task_rerun_configs.append(task_rerun_config)
            else:
                pipeline_job.pipeline_task_rerun_configs.append(
                    aiplatform_v1beta1.PipelineTaskRerunConfig(
                        task_id=task_detail.task_id,
                        task_name=task_detail.task_unique_name,
                        skip_task=task_detail.state
                        == aiplatform_v1beta1.PipelineTaskDetail.State.SUCCEEDED,
                    )
                )

        if parameter_values:
            runtime_config = self._v1_beta1_pipeline_job.runtime_config
            runtime_config.parameter_values = parameter_values

        pipeline_name = self._v1_beta1_pipeline_job.display_name

        job_id = job_id or "{pipeline_name}-{timestamp}".format(
            pipeline_name=re.sub("[^-0-9a-z]+", "-", pipeline_name.lower())
            .lstrip("-")
            .rstrip("-"),
            timestamp=_get_current_time().strftime("%Y%m%d%H%M%S"),
        )

        request = aiplatform_v1beta1.CreatePipelineJobRequest(
            parent=parent,
            pipeline_job=self._v1_beta1_pipeline_job,
            pipeline_job_id=job_id,
        )

        response = v1beta1_client.create_pipeline_job(request=request)

        self._gca_resource = response

        _LOGGER.log_create_complete_with_getter(
            self.__class__, self._gca_resource, "pipeline_job"
        )

        _LOGGER.info("View Pipeline Job:\n%s" % self._dashboard_uri())
