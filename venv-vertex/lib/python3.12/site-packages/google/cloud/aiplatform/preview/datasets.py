
















import dataclasses
from typing import Dict, List, Optional, Tuple
import uuid

from google.auth import credentials as auth_credentials
from google.cloud import storage
from google.cloud.aiplatform import base
from google.cloud.aiplatform import compat
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.compat.types import (
    dataset_v1beta1 as gca_dataset,
    dataset_service_v1beta1 as gca_dataset_service,
)
from vertexai import generative_models
from vertexai.generative_models import _generative_models
from vertexai.preview import prompts
import pandas

from google.protobuf import field_mask_pb2
from google.protobuf import struct_pb2
from google.protobuf import json_format


_MULTIMODAL_METADATA_SCHEMA_URI = (
    "gs://google-cloud-aiplatform/schema/dataset/metadata/multimodal_1.0.0.yaml"
)
_DEFAULT_BQ_DATASET_PREFIX = "vertex_datasets"
_DEFAULT_BQ_TABLE_PREFIX = "multimodal_dataset"
_INPUT_CONFIG_FIELD = "inputConfig"
_BIGQUERY_SOURCE_FIELD = "bigquerySource"
_URI_FIELD = "uri"
_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD = "geminiTemplateConfigSource"
_GEMINI_TEMPLATE_CONFIG_FIELD = "geminiTemplateConfig"
_PROMPT_URI_FIELD = "promptUri"
_REQUEST_COLUMN_NAME_FIELD = "requestColumnName"
_BQ_MULTIREGIONS = {"us", "eu"}

_LOGGER = base.Logger(__name__)


def _try_import_bigframes():
    
    try:
        import bigframes
        import bigframes.pandas
        import bigframes.bigquery

        return bigframes
    except ImportError as exc:
        raise ImportError(
            "`bigframes` is not installed but required for this functionality."
        ) from exc


def _get_metadata_for_bq(
    *,
    bq_uri: str,
    template_config: Optional[gca_dataset_service.GeminiTemplateConfig] = None,
    prompt_uri: Optional[str] = None,
    request_column_name: Optional[str] = None,
) -> struct_pb2.Value:
    if (
        sum(
            1
            for param in (template_config, prompt_uri, request_column_name)
            if param is not None
        )
        > 1
    ):
        raise ValueError(
            "Only one of template_config, prompt_uri, request_column_name can be specified."
        )

    input_config = {_INPUT_CONFIG_FIELD: {_BIGQUERY_SOURCE_FIELD: {_URI_FIELD: bq_uri}}}
    if template_config is not None:
        template_config_dict = gca_dataset_service.GeminiTemplateConfig.to_dict(
            template_config
        )
        input_config[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD] = {
            _GEMINI_TEMPLATE_CONFIG_FIELD: template_config_dict
        }
    if prompt_uri is not None:
        input_config[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD] = {
            _PROMPT_URI_FIELD: prompt_uri
        }
    if request_column_name is not None:
        input_config[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD] = {
            _REQUEST_COLUMN_NAME_FIELD: request_column_name
        }
    return json_format.ParseDict(input_config, struct_pb2.Value())


def _bq_dataset_location_allowed(
    vertex_location: str, bq_dataset_location: str
) -> bool:
    if bq_dataset_location == vertex_location:
        return True
    if bq_dataset_location in _BQ_MULTIREGIONS:
        return vertex_location.startswith(bq_dataset_location)
    return False


def _normalize_and_validate_table_id(
    *,
    table_id: str,
    project: Optional[str] = None,
    vertex_location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
):
    from google.cloud import bigquery  

    if not project:
        project = initializer.global_config.project
    if not vertex_location:
        vertex_location = initializer.global_config.location
    if not credentials:
        credentials = initializer.global_config.credentials

    table_ref = bigquery.TableReference.from_string(table_id, default_project=project)
    if table_ref.project != project:
        raise ValueError(
            f"The BigQuery table "
            f"`{table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}`"
            " must be in the same project as the multimodal dataset."
            f" The multimodal dataset is in `{project}`, but the BigQuery table"
            f" is in `{table_ref.project}`."
        )

    dataset_ref = bigquery.DatasetReference(
        project=table_ref.project, dataset_id=table_ref.dataset_id
    )
    client = bigquery.Client(project=project, credentials=credentials)
    bq_dataset = client.get_dataset(dataset_ref=dataset_ref)
    if not _bq_dataset_location_allowed(vertex_location, bq_dataset.location):
        raise ValueError(
            f"The BigQuery dataset"
            f" `{dataset_ref.project}.{dataset_ref.dataset_id}` must be in the"
            " same location as the multimodal dataset. The multimodal dataset"
            f" is in `{vertex_location}`, but the BigQuery dataset is in"
            f" `{bq_dataset.location}`."
        )
    return f"{table_ref.project}.{table_ref.dataset_id}.{table_ref.table_id}"


def _create_default_bigquery_dataset_if_not_exists(
    *,
    project: Optional[str] = None,
    location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
) -> str:
    
    from google.cloud import bigquery  

    if not project:
        project = initializer.global_config.project
    if not location:
        location = initializer.global_config.location
    if not credentials:
        credentials = initializer.global_config.credentials

    bigquery_client = bigquery.Client(project=project, credentials=credentials)
    location_str = location.lower().replace("-", "_")
    dataset_id = bigquery.DatasetReference(
        project, f"{_DEFAULT_BQ_DATASET_PREFIX}_{location_str}"
    )
    dataset = bigquery.Dataset(dataset_ref=dataset_id)
    dataset.location = location
    bigquery_client.create_dataset(dataset, exists_ok=True)
    return f"{dataset_id.project}.{dataset_id.dataset_id}"


def _generate_target_table_id(dataset_id: str):
    return f"{dataset_id}.{_DEFAULT_BQ_TABLE_PREFIX}_{str(uuid.uuid4())}"


def construct_single_turn_template(
    *,
    prompt: str = None,
    response: Optional[str] = None,
    system_instruction: Optional[str] = None,
    model: Optional[str] = None,
    cached_content: Optional[str] = None,
    tools: Optional[List[generative_models.Tool]] = None,
    tool_config: Optional[generative_models.ToolConfig] = None,
    safety_settings: Optional[List[generative_models.SafetySetting]] = None,
    generation_config: Optional[generative_models.GenerationConfig] = None,
    field_mapping: List[Dict[str, str]] = None,
) -> "GeminiTemplateConfig":
    
    contents = []
    contents.append(
        generative_models.Content(
            role="user",
            parts=[
                generative_models.Part.from_text(prompt),
            ],
        )
    )
    if response:
        contents.append(
            generative_models.Content(
                role="model",
                parts=[
                    generative_models.Part.from_text(response),
                ],
            )
        )
    if system_instruction:
        system_instruction = generative_models.Content(
            parts=[
                generative_models.Part.from_text(system_instruction),
            ],
        )

    
    gemini_example = GeminiExample(
        model=model,
        contents=contents,
        system_instruction=system_instruction,
        cached_content=cached_content,
        tools=tools,
        tool_config=tool_config,
        safety_settings=safety_settings,
        generation_config=generation_config,
    )
    return GeminiTemplateConfig(
        gemini_example=gemini_example, field_mapping=field_mapping
    )


class GeminiExample:
    

    Content = generative_models.Content
    Part = generative_models.Part
    Tool = generative_models.Tool
    ToolConfig = generative_models.ToolConfig
    SafetySetting = generative_models.SafetySetting
    GenerationConfig = generative_models.GenerationConfig

    def __init__(
        self,
        *,
        model: Optional[str] = None,
        contents: Optional[List[Content]] = None,
        system_instruction: Optional[Content] = None,
        cached_content: Optional[str] = None,
        tools: Optional[List[Tool]] = None,
        tool_config: Optional[ToolConfig] = None,
        safety_settings: Optional[List[SafetySetting]] = None,
        generation_config: Optional[GenerationConfig] = None,
    ):
        
        self._raw_gemini_example = gca_dataset_service.GeminiExample()
        self.model = model
        self.contents = contents
        self.system_instruction = system_instruction
        self.cached_content = cached_content
        self.tools = tools
        self.tool_config = tool_config
        self.safety_settings = safety_settings
        self.generation_config = generation_config

    @property
    def model(self) -> Optional[str]:
        
        if not self._raw_gemini_example.model:
            return None
        return self._raw_gemini_example.model

    @model.setter
    def model(self, model: str):
        
        self._raw_gemini_example.model = model

    @property
    def contents(self) -> Optional[List[Content]]:
        
        if not self._raw_gemini_example.contents:
            return None
        return [
            generative_models.Content._from_gapic(content)
            for content in self._raw_gemini_example.contents
        ]

    @contents.setter
    def contents(self, contents: Optional[List[Content]]):
        
        if contents is None:
            self._raw_gemini_example.contents = None
        else:
            self._raw_gemini_example.contents = [
                content._raw_content for content in contents
            ]

    @property
    def system_instruction(self) -> Optional[Content]:
        
        if not self._raw_gemini_example.system_instruction:
            return None
        return generative_models.Content._from_gapic(
            self._raw_gemini_example.system_instruction
        )

    @system_instruction.setter
    def system_instruction(self, system_instruction: Optional[Content]):
        
        if system_instruction is None:
            self._raw_gemini_example.system_instruction = None
        else:
            self._raw_gemini_example.system_instruction = (
                system_instruction._raw_content
            )

    @property
    def cached_content(self) -> Optional[str]:
        
        if not self._raw_gemini_example.cached_content:
            return None
        return self._raw_gemini_example.cached_content

    @cached_content.setter
    def cached_content(self, cached_content: Optional[str]):
        
        self._raw_gemini_example.cached_content = cached_content

    @property
    def tools(self) -> Optional[List[Tool]]:
        
        if not self._raw_gemini_example.tools:
            return None
        return [
            generative_models.Tool._from_gapic(tool)
            for tool in self._raw_gemini_example.tools
        ]

    @tools.setter
    def tools(self, tools: Optional[List[Tool]]):
        
        if tools is None:
            self._raw_gemini_example.tools = None
        else:
            self._raw_gemini_example.tools = [tool._raw_tool for tool in tools]

    @property
    def tool_config(self) -> Optional[ToolConfig]:
        
        if not self._raw_gemini_example.tool_config:
            return None
        return generative_models.ToolConfig._from_gapic(
            self._raw_gemini_example.tool_config
        )

    @tool_config.setter
    def tool_config(self, tool_config: Optional[ToolConfig]):
        
        if tool_config is None:
            self._raw_gemini_example.tool_config = None
        else:
            self._raw_gemini_example.tool_config = tool_config._gapic_tool_config

    @property
    def safety_settings(self) -> Optional[List[SafetySetting]]:
        
        if not self._raw_gemini_example.safety_settings:
            return None
        return [
            generative_models.SafetySetting._from_gapic(safety_setting)
            for safety_setting in self._raw_gemini_example.safety_settings
        ]

    @safety_settings.setter
    def safety_settings(self, safety_settings: Optional[List[SafetySetting]]):
        
        if safety_settings is None:
            self._raw_gemini_example.safety_settings = None
        else:
            self._raw_gemini_example.safety_settings = [
                safety_setting._raw_safety_setting for safety_setting in safety_settings
            ]

    @property
    def generation_config(self) -> Optional[GenerationConfig]:
        
        if not self._raw_gemini_example.generation_config:
            return None
        return generative_models.GenerationConfig._from_gapic(
            self._raw_gemini_example.generation_config
        )

    @generation_config.setter
    def generation_config(self, generation_config: Optional[GenerationConfig]):
        
        if generation_config is None:
            self._raw_gemini_example.generation_config = None
        else:
            self._raw_gemini_example.generation_config = (
                generation_config._raw_generation_config
            )

    @classmethod
    def _from_gapic(
        cls, raw_gemini_example: gca_dataset_service.GeminiExample
    ) -> "GeminiExample":
        example = cls()
        example._raw_gemini_example = raw_gemini_example
        return example

    @classmethod
    def from_prompt(cls, prompt: prompts.Prompt) -> "GeminiExample":
        
        contents = prompt.assemble_contents()
        if prompt.system_instruction:
            system_instructions = generative_models.Content._from_gapic(
                _generative_models._to_content(prompt.system_instruction)
            )
        else:
            system_instructions = None
        
        
        
        if isinstance(prompt.safety_settings, generative_models.SafetySetting):
            safety_settings = [prompt.safety_settings]
        else:
            safety_settings = prompt.safety_settings

        return cls(
            model=prompt.model_name,
            contents=contents,
            system_instruction=system_instructions,
            tools=prompt.tools,
            tool_config=prompt.tool_config,
            safety_settings=safety_settings,
            generation_config=prompt.generation_config,
        )

    def __repr__(self) -> str:
        return self._raw_gemini_example.__repr__()


class GeminiTemplateConfig:
    

    def __init__(
        self,
        *,
        gemini_example: Optional[GeminiExample] = None,
        field_mapping: Optional[Dict[str, str]] = None,
    ):
        
        raw_gemini_example = (
            gemini_example._raw_gemini_example if gemini_example is not None else None
        )
        self._raw_gemini_template_config = gca_dataset_service.GeminiTemplateConfig(
            gemini_example=raw_gemini_example, field_mapping=field_mapping
        )

    @classmethod
    def _from_gapic(
        cls, raw_gemini_template_config: gca_dataset_service.GeminiTemplateConfig
    ) -> None:
        template_config = cls()
        template_config._raw_gemini_template_config = raw_gemini_template_config
        return template_config

    @property
    def gemini_example(self) -> Optional[GeminiExample]:
        
        return GeminiExample._from_gapic(
            self._raw_gemini_template_config.gemini_example
        )

    @property
    def field_mapping(self) -> Optional[Dict[str, str]]:
        
        return dict(self._raw_gemini_template_config.field_mapping)

    def __repr__(self) -> str:
        return self._raw_gemini_template_config.__repr__()


@dataclasses.dataclass(frozen=True)
class TuningResourceUsageAssessmentResult:
    

    token_count: int
    billable_character_count: int


@dataclasses.dataclass(frozen=True)
class TuningValidationAssessmentResult:
    

    errors: List[str]


@dataclasses.dataclass(frozen=True)
class BatchPredictionResourceUsageAssessmentResult:
    

    token_count: int
    audio_token_count: int


class MultimodalDataset(base.VertexAiResourceNounWithFutureManager):
    

    client_class = utils.DatasetClientWithOverride
    _resource_noun = "datasets"
    _getter_method = "get_dataset"
    _list_method = "list_datasets"
    _delete_method = "delete_dataset"
    _parse_resource_name_method = "parse_dataset_path"
    _format_resource_name_method = "dataset_path"
    _DEFAULT_REQUEST_COLUMN_NAME = "requests"

    def __init__(
        self,
        *,
        dataset_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ):
        
        super().__init__(
            project=project,
            location=location,
            credentials=credentials,
            resource_name=dataset_name,
        )
        self.api_client = self.api_client.select_version(compat.V1BETA1)
        self._gca_resource = self._get_gca_resource(resource_name=dataset_name)
        self._validate_metadata_schema_uri()

    @property
    def metadata_schema_uri(self) -> str:
        
        self._assert_gca_resource_is_available()
        return self._gca_resource.metadata_schema_uri

    def _validate_metadata_schema_uri(self):
        if self.metadata_schema_uri != _MULTIMODAL_METADATA_SCHEMA_URI:

            raise ValueError(
                f"Dataset {self.resource_name} is not a multimodal dataset"
            )

    @property
    def bigquery_table(self) -> str:
        
        self._assert_gca_resource_is_available()
        return self._gca_resource.metadata[_INPUT_CONFIG_FIELD][_BIGQUERY_SOURCE_FIELD][
            _URI_FIELD
        ]

    @classmethod
    def from_bigquery(
        cls,
        *,
        bigquery_source: str,
        display_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        labels: Optional[Dict[str, str]] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "MultimodalDataset":
        
        if not bigquery_source.startswith("bq://"):
            bigquery_source = f"bq://{bigquery_source}"
        return cls._create_from_bigquery(
            metadata=_get_metadata_for_bq(bq_uri=bigquery_source),
            display_name=display_name,
            project=project,
            location=location,
            credentials=credentials,
            labels=labels,
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    @classmethod
    def from_pandas(
        cls,
        *,
        dataframe: pandas.DataFrame,
        target_table_id: Optional[str] = None,
        display_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        labels: Optional[Dict[str, str]] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "MultimodalDataset":
        
        bigframes = _try_import_bigframes()
        from google.cloud import bigquery  

        if not project:
            project = initializer.global_config.project
        if not location:
            location = initializer.global_config.location
        if not credentials:
            credentials = initializer.global_config.credentials

        if target_table_id:
            target_table_id = _normalize_and_validate_table_id(
                table_id=target_table_id,
                project=project,
                vertex_location=location,
                credentials=credentials,
            )
        else:
            dataset_id = _create_default_bigquery_dataset_if_not_exists(
                project=project, location=location, credentials=credentials
            )
            target_table_id = _generate_target_table_id(dataset_id)

        session_options = bigframes.BigQueryOptions(
            credentials=credentials,
            project=project,
            location=location,
        )
        with bigframes.connect(session_options) as session:
            temp_bigframes_df = session.read_pandas(dataframe)
            temp_table_id = temp_bigframes_df.to_gbq()
        client = bigquery.Client(project=project, credentials=credentials)
        copy_job = client.copy_table(
            sources=temp_table_id,
            destination=target_table_id,
        )
        copy_job.result()

        bigquery_uri = f"bq://{target_table_id}"
        return cls._create_from_bigquery(
            metadata=_get_metadata_for_bq(bq_uri=bigquery_uri),
            display_name=display_name,
            project=project,
            location=location,
            credentials=credentials,
            labels=labels,
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    @classmethod
    def from_bigframes(
        cls,
        *,
        dataframe: "bigframes.pandas.DataFrame",  
        target_table_id: Optional[str] = None,
        display_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        labels: Optional[Dict[str, str]] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "MultimodalDataset":
        
        from google.cloud import bigquery  

        if target_table_id:
            target_table_id = _normalize_and_validate_table_id(
                table_id=target_table_id,
                project=project,
                vertex_location=location,
                credentials=credentials,
            )
        else:
            dataset_id = _create_default_bigquery_dataset_if_not_exists(
                project=project, location=location, credentials=credentials
            )
            target_table_id = _generate_target_table_id(dataset_id)

        if not project:
            project = initializer.global_config.project

        temp_table_id = dataframe.to_gbq()
        client = bigquery.Client(project=project, credentials=credentials)
        copy_job = client.copy_table(
            sources=temp_table_id,
            destination=target_table_id,
        )
        copy_job.result()

        bigquery_uri = f"bq://{target_table_id}"
        return cls._create_from_bigquery(
            metadata=_get_metadata_for_bq(bq_uri=bigquery_uri),
            display_name=display_name,
            project=project,
            location=location,
            credentials=credentials,
            labels=labels,
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    @classmethod
    def from_gemini_request_jsonl(
        cls,
        *,
        gcs_uri: str,
        target_table_id: Optional[str] = None,
        display_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        labels: Optional[Dict[str, str]] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "MultimodalDataset":
        
        bigframes = _try_import_bigframes()
        from google.cloud import bigquery  

        if not project:
            project = initializer.global_config.project
        if not location:
            location = initializer.global_config.location
        if not credentials:
            credentials = initializer.global_config.credentials

        if target_table_id:
            target_table_id = _normalize_and_validate_table_id(
                table_id=target_table_id,
                project=project,
                vertex_location=location,
                credentials=credentials,
            )
        else:
            dataset_id = _create_default_bigquery_dataset_if_not_exists(
                project=project, location=location, credentials=credentials
            )
            target_table_id = _generate_target_table_id(dataset_id)

        gcs_uri_prefix = "gs://"
        if gcs_uri.startswith(gcs_uri_prefix):
            gcs_uri = gcs_uri[len(gcs_uri_prefix) :]
        parts = gcs_uri.split("/", 1)
        if len(parts) != 2:
            raise ValueError(
                "Invalid GCS URI format. Expected: gs://bucket-name/object-path"
            )
        bucket_name = parts[0]
        blob_name = parts[1]

        storage_client = storage.Client(project=project)
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        request_column_name = cls._DEFAULT_REQUEST_COLUMN_NAME

        jsonl_string = blob.download_as_text()
        lines = [line.strip() for line in jsonl_string.splitlines() if line.strip()]
        df = pandas.DataFrame(lines, columns=[request_column_name])

        session_options = bigframes.BigQueryOptions(
            credentials=credentials,
            project=project,
            location=location,
        )
        with bigframes.connect(session_options) as session:
            temp_bigframes_df = session.read_pandas(df)
            temp_bigframes_df[request_column_name] = bigframes.bigquery.parse_json(
                temp_bigframes_df[request_column_name]
            )
            temp_table_id = temp_bigframes_df.to_gbq()
        client = bigquery.Client(project=project, credentials=credentials)
        copy_job = client.copy_table(
            sources=temp_table_id,
            destination=target_table_id,
        )
        copy_job.result()

        bigquery_uri = f"bq://{target_table_id}"
        return cls._create_from_bigquery(
            metadata=_get_metadata_for_bq(
                bq_uri=bigquery_uri, request_column_name=request_column_name
            ),
            display_name=display_name,
            project=project,
            location=location,
            credentials=credentials,
            labels=labels,
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    def to_bigframes(self) -> "bigframes.pandas.DataFrame":  
        
        bigframes = _try_import_bigframes()
        return bigframes.pandas.read_gbq_table(self.bigquery_table.lstrip("bq://"))

    @classmethod
    @base.optional_sync()
    def _create_from_bigquery(
        cls,
        *,
        metadata: struct_pb2.Value,
        display_name: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        labels: Optional[Dict[str, str]] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "MultimodalDataset":
        if not display_name:
            display_name = cls._generate_display_name()
        utils.validate_display_name(display_name)
        if labels:
            utils.validate_labels(labels)
        if not project:
            project = initializer.global_config.project
        if not location:
            location = initializer.global_config.location
        if not credentials:
            credentials = initializer.global_config.credentials

        dataset = gca_dataset.Dataset(
            display_name=display_name,
            metadata_schema_uri=_MULTIMODAL_METADATA_SCHEMA_URI,
            metadata=metadata,
            labels=labels,
        )
        parent = initializer.global_config.common_location_path(
            project=project, location=location
        )
        api_client = cls._instantiate_client(
            location=location, credentials=credentials
        ).select_version(compat.V1BETA1)
        create_lro = api_client.create_dataset(
            dataset=dataset, parent=parent, timeout=create_request_timeout
        )
        _LOGGER.log_create_with_lro(cls, create_lro)
        created_dataset = create_lro.result(timeout=None)
        _LOGGER.log_create_complete(cls, created_dataset, "ds")
        return cls(dataset_name=created_dataset.name)

    def update(
        self,
        *,
        display_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        description: Optional[str] = None,
        update_request_timeout: Optional[float] = None,
    ):
        
        update_mask = field_mask_pb2.FieldMask()
        if display_name:
            update_mask.paths.append("display_name")

        if labels:
            update_mask.paths.append("labels")

        if description:
            update_mask.paths.append("description")

        update_dataset = gca_dataset.Dataset(
            name=self.resource_name,
            display_name=display_name,
            description=description,
            labels=labels,
        )

        self._gca_resource = self.api_client.update_dataset(
            dataset=update_dataset,
            update_mask=update_mask,
            timeout=update_request_timeout,
        )

        return self

    def attach_template_config(
        self,
        *,
        template_config: Optional[GeminiTemplateConfig] = None,
        prompt: Optional[prompts.Prompt] = None,
        update_request_timeout: Optional[float] = None,
    ):
        
        if not (template_config or prompt):
            raise ValueError("Either template_config or prompt must be provided.")
        if template_config and prompt:
            raise ValueError("Only one of template_config or prompt can be provided.")

        raw_template_config = None
        if template_config:
            raw_template_config = template_config._raw_gemini_template_config
        prompt_uri = None
        if prompt:
            if prompt.prompt_id:
                saved_prompt = prompt
            else:
                saved_prompt = prompts.create_version(prompt)
            location = initializer.global_config.location
            project = initializer.global_config.project
            
            prompt_uri = f"projects/{project}/locations/{location}/datasets/{saved_prompt.prompt_id}"

        update_mask = field_mask_pb2.FieldMask(paths=["metadata"])
        update_dataset = gca_dataset.Dataset(
            name=self.resource_name,
            metadata=_get_metadata_for_bq(
                bq_uri=self.bigquery_table,
                template_config=raw_template_config,
                prompt_uri=prompt_uri,
            ),
        )
        self._gca_resource = self.api_client.update_dataset(
            dataset=update_dataset,
            update_mask=update_mask,
            timeout=update_request_timeout,
        )
        return self

    @property
    def template_config(self) -> Optional[GeminiTemplateConfig]:
        
        self._assert_gca_resource_is_available()
        
        if _GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD not in self._gca_resource.metadata:
            return None
        
        if (
            _GEMINI_TEMPLATE_CONFIG_FIELD
            in self._gca_resource.metadata[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD]
        ):
            struct_proto_container = self._gca_resource.metadata[
                _GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD
            ].pb
            struct_proto = struct_proto_container.get(_GEMINI_TEMPLATE_CONFIG_FIELD)
            
            gapic_template_config = gca_dataset_service.GeminiTemplateConfig.from_json(
                json_format.MessageToJson(struct_proto)
            )
            return GeminiTemplateConfig._from_gapic(gapic_template_config)
        
        if (
            _PROMPT_URI_FIELD
            in self._gca_resource.metadata[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD]
        ):
            prompt_uri = (
                self._gca_resource.metadata[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD]
                .pb.get(_PROMPT_URI_FIELD)
                .string_value
            )
            resource_name_prefix = f"projects/{initializer.global_config.project}/locations/{initializer.global_config.location}/datasets/"
            if not prompt_uri.startswith(resource_name_prefix):
                prompt_location = prompt_uri.split("/")[3]
                prompt_project = prompt_uri.split("/")[1]
                raise ValueError(
                    "Attached prompt is not in the currently configured global "
                    "project and/or location. (Configured project/location: "
                    f"{initializer.global_config.location}, "
                    f"{initializer.global_config.project}; Attached prompt: "
                    f"{prompt_location}, {prompt_project})"
                )
            prompt_id = prompt_uri
            if prompt_id.startswith(resource_name_prefix):
                prompt_id = prompt_id[len(resource_name_prefix) :]
            prompt = prompts.get(prompt_id)
            return GeminiTemplateConfig(
                gemini_example=GeminiExample.from_prompt(prompt), field_mapping={}
            )

        return None

    @property
    def request_column_name(self) -> Optional[str]:
        

        self._assert_gca_resource_is_available()
        
        if _GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD not in self._gca_resource.metadata:
            return None
        if (
            _REQUEST_COLUMN_NAME_FIELD
            not in self._gca_resource.metadata[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD]
        ):
            return None
        return self._gca_resource.metadata[_GEMINI_TEMPLATE_CONFIG_SOURCE_FIELD][
            _REQUEST_COLUMN_NAME_FIELD
        ]

    def assemble(
        self,
        *,
        template_config: Optional[GeminiTemplateConfig] = None,
        load_dataframe: bool = True,
        assemble_request_timeout: Optional[float] = None,
    ) -> Tuple[str, "bigframes.pandas.DataFrame"]:  
        
        bigframes = _try_import_bigframes()
        request = gca_dataset_service.AssembleDataRequest(
            name=self.resource_name,
            gemini_request_read_config=self._build_gemini_request_read_config(
                template_config
            ),
        )

        assemble_lro = self.api_client.assemble_data(
            request=request, timeout=assemble_request_timeout
        )
        _LOGGER.log_action_started_against_resource_with_lro(
            "Assemble", "data", self.__class__, assemble_lro
        )
        result = assemble_lro.result(timeout=None)
        _LOGGER.log_action_completed_against_resource("data", "assembled", self)
        table_id = result.bigquery_destination.lstrip("bq://")
        if load_dataframe:
            session_options = bigframes.BigQueryOptions(
                credentials=initializer.global_config.credentials,
                project=initializer.global_config.project,
                location=initializer.global_config.location,
            )
            with bigframes.connect(session_options) as session:
                df = session.read_gbq(table_id)
        else:
            df = None

        return (table_id, df)

    def assess_tuning_resources(
        self,
        *,
        model_name: str,
        template_config: Optional[GeminiTemplateConfig] = None,
        assess_request_timeout: Optional[float] = None,
    ) -> TuningResourceUsageAssessmentResult:
        
        request = self._build_assess_data_request(template_config)
        request.tuning_resource_usage_assessment_config = (
            gca_dataset_service.AssessDataRequest.TuningResourceUsageAssessmentConfig(
                model_name=model_name
            )
        )

        assessment_result = (
            self.api_client.assess_data(request=request, timeout=assess_request_timeout)
            .result(timeout=None)
            .tuning_resource_usage_assessment_result
        )
        return TuningResourceUsageAssessmentResult(
            token_count=assessment_result.token_count,
            billable_character_count=assessment_result.billable_character_count,
        )

    def assess_tuning_validity(
        self,
        *,
        model_name: str,
        dataset_usage: str,
        template_config: Optional[GeminiTemplateConfig] = None,
        assess_request_timeout: Optional[float] = None,
    ) -> TuningValidationAssessmentResult:
        
        DatasetUsage = (
            gca_dataset_service.AssessDataRequest.TuningValidationAssessmentConfig.DatasetUsage
        )
        try:
            dataset_usage_enum = DatasetUsage[dataset_usage]
        except KeyError as e:
            valid_dataset_usage_names = [
                e.name for e in DatasetUsage if e.name != "DATASET_USAGE_UNSPECIFIED"
            ]
            raise ValueError(
                f"Argument 'dataset_usage' must be one of the following: "
                f"{', '.join(valid_dataset_usage_names)}."
            ) from e
        if dataset_usage_enum == DatasetUsage.DATASET_USAGE_UNSPECIFIED:
            raise ValueError("Dataset usage must be specified.")

        request = self._build_assess_data_request(template_config)
        request.tuning_validation_assessment_config = (
            gca_dataset_service.AssessDataRequest.TuningValidationAssessmentConfig(
                model_name=model_name,
                dataset_usage=dataset_usage_enum,
            )
        )
        assess_lro = self.api_client.assess_data(
            request=request, timeout=assess_request_timeout
        )
        assessment_result = assess_lro.result(timeout=None)
        return TuningValidationAssessmentResult(
            errors=assessment_result.tuning_validation_assessment_result.errors
        )

    def assess_batch_prediction_resources(
        self,
        *,
        model_name: str,
        template_config: Optional[GeminiTemplateConfig] = None,
        assess_request_timeout: Optional[float] = None,
    ) -> BatchPredictionResourceUsageAssessmentResult:
        
        request = self._build_assess_data_request(template_config)
        request.batch_prediction_resource_usage_assessment_config = gca_dataset_service.AssessDataRequest.BatchPredictionResourceUsageAssessmentConfig(
            model_name=model_name
        )

        assessment_result = (
            self.api_client.assess_data(request=request, timeout=assess_request_timeout)
            .result(timeout=None)
            .batch_prediction_resource_usage_assessment_result
        )
        return BatchPredictionResourceUsageAssessmentResult(
            token_count=assessment_result.token_count,
            audio_token_count=assessment_result.audio_token_count,
        )

    def assess_batch_prediction_validity(
        self,
        *,
        model_name: str,
        template_config: Optional[GeminiTemplateConfig] = None,
        assess_request_timeout: Optional[float] = None,
    ) -> None:
        
        request = self._build_assess_data_request(template_config)
        request.batch_prediction_validation_assessment_config = gca_dataset_service.AssessDataRequest.BatchPredictionValidationAssessmentConfig(
            model_name=model_name,
        )
        assess_lro = self.api_client.assess_data(
            request=request, timeout=assess_request_timeout
        )
        assess_lro.result(timeout=None)

    def _build_assess_data_request(
        self,
        template_config: Optional[GeminiTemplateConfig] = None,
    ):
        return gca_dataset_service.AssessDataRequest(
            name=self.resource_name,
            gemini_request_read_config=self._build_gemini_request_read_config(
                template_config
            ),
        )

    def _build_gemini_request_read_config(
        self, provided_template_config: Optional[GeminiTemplateConfig] = None
    ) -> gca_dataset_service.GeminiRequestReadConfig:
        
        if provided_template_config is not None:
            return gca_dataset_service.GeminiRequestReadConfig(
                template_config=provided_template_config._raw_gemini_template_config
            )
        elif self.template_config is not None:
            return gca_dataset_service.GeminiRequestReadConfig(
                template_config=self.template_config._raw_gemini_template_config
            )
        elif self.request_column_name is not None:
            return gca_dataset_service.GeminiRequestReadConfig(
                assembled_request_column_name=self.request_column_name
            )
        else:
            raise ValueError(
                "No template config was provided and no template config is attached to the dataset."
            )
