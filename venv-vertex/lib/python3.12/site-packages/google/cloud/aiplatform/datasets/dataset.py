
















from typing import Any, Dict, List, Optional, Sequence, Tuple, Union

from google.api_core import operation
from google.auth import credentials as auth_credentials

from google.cloud.aiplatform import base
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import utils

from google.cloud.aiplatform.compat.services import dataset_service_client
from google.cloud.aiplatform.compat.types import (
    dataset as gca_dataset,
    dataset_service as gca_dataset_service,
    encryption_spec as gca_encryption_spec,
    io as gca_io,
)
from google.cloud.aiplatform.datasets import _datasources
from google.protobuf import field_mask_pb2
from google.protobuf import json_format

_LOGGER = base.Logger(__name__)


class _Dataset(base.VertexAiResourceNounWithFutureManager):
    

    client_class = utils.DatasetClientWithOverride
    _resource_noun = "datasets"
    _getter_method = "get_dataset"
    _list_method = "list_datasets"
    _delete_method = "delete_dataset"
    _parse_resource_name_method = "parse_dataset_path"
    _format_resource_name_method = "dataset_path"

    _supported_metadata_schema_uris: Tuple[str] = ()

    def __init__(
        self,
        dataset_name: str,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ):
        

        super().__init__(
            project=project,
            location=location,
            credentials=credentials,
            resource_name=dataset_name,
        )
        self._gca_resource = self._get_gca_resource(resource_name=dataset_name)
        self._validate_metadata_schema_uri()

    @property
    def metadata_schema_uri(self) -> str:
        
        self._assert_gca_resource_is_available()
        return self._gca_resource.metadata_schema_uri

    def _validate_metadata_schema_uri(self) -> None:
        
        if self._supported_metadata_schema_uris and (
            self.metadata_schema_uri not in self._supported_metadata_schema_uris
        ):
            raise ValueError(
                f"{self.__class__.__name__} class can not be used to retrieve "
                f"dataset resource {self.resource_name}, check the dataset type"
            )

    @classmethod
    def create(
        cls,
        
        display_name: str,
        metadata_schema_uri: str,
        gcs_source: Optional[Union[str, Sequence[str]]] = None,
        bq_source: Optional[str] = None,
        import_schema_uri: Optional[str] = None,
        data_item_labels: Optional[Dict] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        labels: Optional[Dict[str, str]] = None,
        encryption_spec_key_name: Optional[str] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
    ) -> "_Dataset":
        
        if not display_name:
            display_name = cls._generate_display_name()
        utils.validate_display_name(display_name)
        if labels:
            utils.validate_labels(labels)

        api_client = cls._instantiate_client(location=location, credentials=credentials)

        datasource = _datasources.create_datasource(
            metadata_schema_uri=metadata_schema_uri,
            import_schema_uri=import_schema_uri,
            gcs_source=gcs_source,
            bq_source=bq_source,
            data_item_labels=data_item_labels,
        )

        return cls._create_and_import(
            api_client=api_client,
            parent=initializer.global_config.common_location_path(
                project=project, location=location
            ),
            display_name=display_name,
            metadata_schema_uri=metadata_schema_uri,
            datasource=datasource,
            project=project or initializer.global_config.project,
            location=location or initializer.global_config.location,
            credentials=credentials or initializer.global_config.credentials,
            request_metadata=request_metadata,
            labels=labels,
            encryption_spec=initializer.global_config.get_encryption_spec(
                encryption_spec_key_name=encryption_spec_key_name
            ),
            sync=sync,
            create_request_timeout=create_request_timeout,
        )

    @classmethod
    @base.optional_sync()
    def _create_and_import(
        cls,
        api_client: dataset_service_client.DatasetServiceClient,
        parent: str,
        display_name: str,
        metadata_schema_uri: str,
        datasource: _datasources.Datasource,
        project: str,
        location: str,
        credentials: Optional[auth_credentials.Credentials],
        request_metadata: Optional[Sequence[Tuple[str, str]]] = (),
        labels: Optional[Dict[str, str]] = None,
        encryption_spec: Optional[gca_encryption_spec.EncryptionSpec] = None,
        sync: bool = True,
        create_request_timeout: Optional[float] = None,
        import_request_timeout: Optional[float] = None,
    ) -> "_Dataset":
        

        create_dataset_lro = cls._create(
            api_client=api_client,
            parent=parent,
            display_name=display_name,
            metadata_schema_uri=metadata_schema_uri,
            datasource=datasource,
            request_metadata=request_metadata,
            labels=labels,
            encryption_spec=encryption_spec,
            create_request_timeout=create_request_timeout,
        )

        _LOGGER.log_create_with_lro(cls, create_dataset_lro)

        created_dataset = create_dataset_lro.result(timeout=None)

        _LOGGER.log_create_complete(cls, created_dataset, "ds")

        dataset_obj = cls(
            dataset_name=created_dataset.name,
            project=project,
            location=location,
            credentials=credentials,
        )

        
        if isinstance(datasource, _datasources.DatasourceImportable):
            dataset_obj._import_and_wait(
                datasource, import_request_timeout=import_request_timeout
            )

        return dataset_obj

    def _import_and_wait(
        self,
        datasource,
        import_request_timeout: Optional[float] = None,
    ):
        _LOGGER.log_action_start_against_resource(
            "Importing",
            "data",
            self,
        )

        import_lro = self._import(
            datasource=datasource, import_request_timeout=import_request_timeout
        )

        _LOGGER.log_action_started_against_resource_with_lro(
            "Import", "data", self.__class__, import_lro
        )

        import_lro.result(timeout=None)

        _LOGGER.log_action_completed_against_resource("data", "imported", self)

    @classmethod
    def _create(
        cls,
        api_client: dataset_service_client.DatasetServiceClient,
        parent: str,
        display_name: str,
        metadata_schema_uri: str,
        datasource: _datasources.Datasource,
        request_metadata: Sequence[Tuple[str, str]] = (),
        labels: Optional[Dict[str, str]] = None,
        encryption_spec: Optional[gca_encryption_spec.EncryptionSpec] = None,
        create_request_timeout: Optional[float] = None,
    ) -> operation.Operation:
        

        gapic_dataset = gca_dataset.Dataset(
            display_name=display_name,
            metadata_schema_uri=metadata_schema_uri,
            metadata=datasource.dataset_metadata,
            labels=labels,
            encryption_spec=encryption_spec,
        )

        return api_client.create_dataset(
            parent=parent,
            dataset=gapic_dataset,
            metadata=request_metadata,
            timeout=create_request_timeout,
        )

    def _import(
        self,
        datasource: _datasources.DatasourceImportable,
        import_request_timeout: Optional[float] = None,
    ) -> operation.Operation:
        
        return self.api_client.import_data(
            name=self.resource_name,
            import_configs=[datasource.import_data_config],
            timeout=import_request_timeout,
        )

    @base.optional_sync(return_input_arg="self")
    def import_data(
        self,
        gcs_source: Union[str, Sequence[str]],
        import_schema_uri: str,
        data_item_labels: Optional[Dict] = None,
        sync: bool = True,
        import_request_timeout: Optional[float] = None,
    ) -> "_Dataset":
        
        datasource = _datasources.create_datasource(
            metadata_schema_uri=self.metadata_schema_uri,
            import_schema_uri=import_schema_uri,
            gcs_source=gcs_source,
            data_item_labels=data_item_labels,
        )

        self._import_and_wait(
            datasource=datasource, import_request_timeout=import_request_timeout
        )
        return self

    def _validate_and_convert_export_split(
        self,
        split: Union[Dict[str, str], Dict[str, float]],
    ) -> Union[gca_dataset.ExportFilterSplit, gca_dataset.ExportFractionSplit]:
        
        if len(split) != 3:
            raise ValueError(
                "The provided split for data export does not provide enough"
                "information. It must have three fields, mapping to training,"
                "validation and test splits respectively."
            )

        if not ("training_filter" in split or "training_fraction" in split):
            raise ValueError(
                "The provided filter for data export does not provide enough"
                "information. It must have three fields, mapping to training,"
                "validation and test respectively."
            )

        if "training_filter" in split:
            if (
                "validation_filter" in split
                and "test_filter" in split
                and isinstance(split["training_filter"], str)
                and isinstance(split["validation_filter"], str)
                and isinstance(split["test_filter"], str)
            ):
                return gca_dataset.ExportFilterSplit(
                    training_filter=split["training_filter"],
                    validation_filter=split["validation_filter"],
                    test_filter=split["test_filter"],
                )
            else:
                raise ValueError(
                    "The provided ExportFilterSplit does not contain all"
                    "three required fields: training_filter, "
                    "validation_filter and test_filter."
                )
        else:
            if (
                "validation_fraction" in split
                and "test_fraction" in split
                and isinstance(split["training_fraction"], float)
                and isinstance(split["validation_fraction"], float)
                and isinstance(split["test_fraction"], float)
            ):
                return gca_dataset.ExportFractionSplit(
                    training_fraction=split["training_fraction"],
                    validation_fraction=split["validation_fraction"],
                    test_fraction=split["test_fraction"],
                )
            else:
                raise ValueError(
                    "The provided ExportFractionSplit does not contain all"
                    "three required fields: training_fraction, "
                    "validation_fraction and test_fraction."
                )

    def _get_completed_export_data_operation(
        self,
        output_dir: str,
        export_use: Optional[gca_dataset.ExportDataConfig.ExportUse] = None,
        annotation_filter: Optional[str] = None,
        saved_query_id: Optional[str] = None,
        annotation_schema_uri: Optional[str] = None,
        split: Optional[
            Union[gca_dataset.ExportFilterSplit, gca_dataset.ExportFractionSplit]
        ] = None,
    ) -> gca_dataset_service.ExportDataResponse:
        self.wait()

        
        export_data_config = gca_dataset.ExportDataConfig(
            gcs_destination=gca_io.GcsDestination(output_uri_prefix=output_dir)
        )
        if export_use is not None:
            export_data_config.export_use = export_use
        if annotation_filter is not None:
            export_data_config.annotation_filter = annotation_filter
        if saved_query_id is not None:
            export_data_config.saved_query_id = saved_query_id
        if annotation_schema_uri is not None:
            export_data_config.annotation_schema_uri = annotation_schema_uri
        if split is not None:
            if isinstance(split, gca_dataset.ExportFilterSplit):
                export_data_config.filter_split = split
            elif isinstance(split, gca_dataset.ExportFractionSplit):
                export_data_config.fraction_split = split

        _LOGGER.log_action_start_against_resource("Exporting", "data", self)

        export_lro = self.api_client.export_data(
            name=self.resource_name, export_config=export_data_config
        )

        _LOGGER.log_action_started_against_resource_with_lro(
            "Export", "data", self.__class__, export_lro
        )

        export_data_response = export_lro.result()

        _LOGGER.log_action_completed_against_resource("data", "export", self)

        return export_data_response

    
    def export_data(self, output_dir: str) -> Sequence[str]:
        
        return self._get_completed_export_data_operation(output_dir).exported_files

    def export_data_for_custom_training(
        self,
        output_dir: str,
        annotation_filter: Optional[str] = None,
        saved_query_id: Optional[str] = None,
        annotation_schema_uri: Optional[str] = None,
        split: Optional[Union[Dict[str, str], Dict[str, float]]] = None,
    ) -> Dict[str, Any]:
        
        split = self._validate_and_convert_export_split(split)

        return json_format.MessageToDict(
            self._get_completed_export_data_operation(
                output_dir,
                gca_dataset.ExportDataConfig.ExportUse.CUSTOM_CODE_TRAINING,
                annotation_filter,
                saved_query_id,
                annotation_schema_uri,
                split,
            )._pb
        )

    def update(
        self,
        *,
        display_name: Optional[str] = None,
        labels: Optional[Dict[str, str]] = None,
        description: Optional[str] = None,
        update_request_timeout: Optional[float] = None,
    ) -> "_Dataset":
        

        update_mask = field_mask_pb2.FieldMask()
        if display_name:
            update_mask.paths.append("display_name")

        if labels:
            update_mask.paths.append("labels")

        if description:
            update_mask.paths.append("description")

        update_dataset = gca_dataset.Dataset(
            name=self.resource_name,
            display_name=display_name,
            description=description,
            labels=labels,
        )

        self._gca_resource = self.api_client.update_dataset(
            dataset=update_dataset,
            update_mask=update_mask,
            timeout=update_request_timeout,
        )

        return self

    @classmethod
    def list(
        cls,
        filter: Optional[str] = None,
        order_by: Optional[str] = None,
        project: Optional[str] = None,
        location: Optional[str] = None,
        credentials: Optional[auth_credentials.Credentials] = None,
    ) -> List[base.VertexAiResourceNoun]:
        

        dataset_subclass_filter = (
            lambda gapic_obj: gapic_obj.metadata_schema_uri
            in cls._supported_metadata_schema_uris
        )

        return cls._list_with_local_order(
            cls_filter=dataset_subclass_filter,
            filter=filter,
            order_by=order_by,
            project=project,
            location=location,
            credentials=credentials,
        )
