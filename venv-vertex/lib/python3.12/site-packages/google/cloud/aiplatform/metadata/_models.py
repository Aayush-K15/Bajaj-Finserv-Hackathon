
















import importlib
import os
import pickle
import tempfile
from typing import Any, Dict, Optional, Sequence, Union

from google.auth import credentials as auth_credentials
from google.cloud import storage
from google.cloud import aiplatform
from google.cloud.aiplatform import base
from google.cloud.aiplatform import explain
from google.cloud.aiplatform import helpers
from google.cloud.aiplatform import initializer
from google.cloud.aiplatform import models
from google.cloud.aiplatform import utils
from google.cloud.aiplatform.metadata.schema import utils as schema_utils
from google.cloud.aiplatform.metadata.schema.google import (
    artifact_schema as google_artifact_schema,
)
from google.cloud.aiplatform.utils import gcs_utils


_LOGGER = base.Logger(__name__)

_PICKLE_PROTOCOL = 4
_MAX_INPUT_EXAMPLE_ROWS = 5


def _save_sklearn_model(
    model: "sklearn.base.BaseEstimator",  
    path: str,
) -> str:
    
    with open(path, "wb") as f:
        pickle.dump(model, f, protocol=_PICKLE_PROTOCOL)
    return f"{model.__class__.__module__}.{model.__class__.__name__}"


def _save_xgboost_model(
    model: Union["xgb.Booster", "xgb.XGBModel"],  
    path: str,
) -> str:
    
    model.save_model(path)
    return f"{model.__class__.__module__}.{model.__class__.__name__}"


def _save_tensorflow_model(
    model: "tf.Module",  
    path: str,
    tf_save_model_kwargs: Optional[Dict[str, Any]] = None,
) -> str:
    
    try:
        import tensorflow as tf
    except ImportError:
        raise ImportError(
            "tensorflow is not installed and required for saving models."
        ) from None

    tf_save_model_kwargs = tf_save_model_kwargs or {}
    if isinstance(model, tf.keras.Model):
        model.save(path, **tf_save_model_kwargs)
        return "tensorflow.keras.Model"
    elif isinstance(model, tf.Module):
        tf.saved_model.save(model, path, **tf_save_model_kwargs)
        return "tensorflow.Module"


def _load_sklearn_model(
    model_file: str,
    model_artifact: google_artifact_schema.ExperimentModel,
) -> "sklearn.base.BaseEstimator":  
    
    try:
        import sklearn
    except ImportError:
        raise ImportError(
            "sklearn is not installed and is required for loading models."
        ) from None

    if sklearn.__version__ < model_artifact.framework_version:
        _LOGGER.warning(
            f"The original model was saved via sklearn {model_artifact.framework_version}. "
            f"You are using sklearn {sklearn.__version__}."
            "Attempting to load model..."
        )
    with open(model_file, "rb") as f:
        sk_model = pickle.load(f)

    return sk_model


def _load_xgboost_model(
    model_file: str,
    model_artifact: google_artifact_schema.ExperimentModel,
) -> Union["xgb.Booster", "xgb.XGBModel"]:  
    
    try:
        import xgboost as xgb
    except ImportError:
        raise ImportError(
            "xgboost is not installed and is required for loading models."
        ) from None

    if xgb.__version__ < model_artifact.framework_version:
        _LOGGER.warning(
            f"The original model was saved via xgboost {model_artifact.framework_version}. "
            f"You are using xgboost {xgb.__version__}."
            "Attempting to load model..."
        )

    module, class_name = model_artifact.model_class.rsplit(".", maxsplit=1)
    xgb_model = getattr(importlib.import_module(module), class_name)()
    xgb_model.load_model(model_file)

    return xgb_model


def _load_tensorflow_model(
    model_file: str,
    model_artifact: google_artifact_schema.ExperimentModel,
) -> "tf.Module":  
    
    try:
        import tensorflow as tf
    except ImportError:
        raise ImportError(
            "tensorflow is not installed and is required for loading models."
        ) from None

    if tf.__version__ < model_artifact.framework_version:
        _LOGGER.warning(
            f"The original model was saved via tensorflow {model_artifact.framework_version}. "
            f"You are using tensorflow {tf.__version__}."
            "Attempting to load model..."
        )

    if model_artifact.model_class == "tensorflow.keras.Model":
        tf_model = tf.keras.models.load_model(model_file)
    elif model_artifact.model_class == "tensorflow.Module":
        tf_model = tf.saved_model.load(model_file)
    else:
        raise ValueError(f"Unsupported model class: {model_artifact.model_class}")

    return tf_model


def _save_input_example(
    input_example: Union[list, dict, "pd.DataFrame", "np.ndarray"],  
    path: str,
):
    
    try:
        import numpy as np
    except ImportError:
        raise ImportError(
            "numpy is not installed and is required for saving input examples. "
            "Please install google-cloud-aiplatform[metadata]."
        ) from None

    try:
        import yaml
    except ImportError:
        raise ImportError(
            "PyYAML is not installed and is required for saving input examples."
        ) from None

    example = {}
    if isinstance(input_example, list):
        if all(isinstance(x, list) for x in input_example):
            example = {
                "type": "list",
                "data": input_example[:_MAX_INPUT_EXAMPLE_ROWS],
            }
        elif all(np.isscalar(x) for x in input_example):
            example = {
                "type": "list",
                "data": input_example,
            }
        else:
            raise ValueError("The value inside a list must be a scalar or list.")

    if isinstance(input_example, dict):
        if all(isinstance(x, list) for x in input_example.values()):
            example = {
                "type": "dict",
                "data": {
                    k: v[:_MAX_INPUT_EXAMPLE_ROWS] for k, v in input_example.items()
                },
            }
        elif all(isinstance(x, np.ndarray) for x in input_example.values()):
            example = {
                "type": "dict",
                "data": {
                    k: v[:_MAX_INPUT_EXAMPLE_ROWS].tolist()
                    for k, v in input_example.items()
                },
            }
        elif all(np.isscalar(x) for x in input_example.values()):
            example = {"type": "dict", "data": input_example}
        else:
            raise ValueError(
                "The value inside a dictionary must be a scalar, list, or np.ndarray"
            )

    if isinstance(input_example, np.ndarray):
        example = {
            "type": "numpy.ndarray",
            "data": input_example[:_MAX_INPUT_EXAMPLE_ROWS].tolist(),
        }

    try:
        import pandas as pd

        if isinstance(input_example, pd.DataFrame):
            example = {
                "type": "pandas.DataFrame",
                "data": input_example.head(_MAX_INPUT_EXAMPLE_ROWS).to_dict("list"),
            }
    except ImportError:
        pass

    if not example:
        raise ValueError(
            (
                "Input example type not supported. "
                "Valid example must be a list, dict, np.ndarray, or pd.DataFrame."
            )
        )

    example_file = os.path.join(path, "instance.yaml")
    with open(example_file, "w") as file:
        yaml.dump(
            {"input_example": example}, file, default_flow_style=None, sort_keys=False
        )


_FRAMEWORK_SPECS = {
    "sklearn": {
        "save_method": _save_sklearn_model,
        "load_method": _load_sklearn_model,
        "model_file": "model.pkl",
    },
    "xgboost": {
        "save_method": _save_xgboost_model,
        "load_method": _load_xgboost_model,
        "model_file": "model.bst",
    },
    "tensorflow": {
        "save_method": _save_tensorflow_model,
        "load_method": _load_tensorflow_model,
        "model_file": "saved_model",
    },
}


def save_model(
    model: Union[
        "sklearn.base.BaseEstimator", "xgb.Booster", "tf.Module"  
    ],
    artifact_id: Optional[str] = None,
    *,
    uri: Optional[str] = None,
    input_example: Union[list, dict, "pd.DataFrame", "np.ndarray"] = None,  
    tf_save_model_kwargs: Optional[Dict[str, Any]] = None,
    display_name: Optional[str] = None,
    metadata_store_id: Optional[str] = "default",
    project: Optional[str] = None,
    location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
) -> google_artifact_schema.ExperimentModel:
    
    framework_name = framework_version = ""
    try:
        import sklearn
    except ImportError:
        pass
    else:
        
        
        if isinstance(
            model, sklearn.base.BaseEstimator
        ) and model.__class__.__module__.startswith("sklearn"):
            framework_name = "sklearn"
            framework_version = sklearn.__version__
    try:
        import sklearn.v1_0_2
    except ImportError:
        pass
    else:
        if isinstance(
            model, sklearn.v1_0_2.base.BaseEstimator
        ) and model.__class__.__module__.startswith("sklearn"):
            framework_name = "sklearn"
            framework_version = sklearn.v1_0_2.__version__

    try:
        import xgboost as xgb
    except ImportError:
        pass
    else:
        if isinstance(model, (xgb.Booster, xgb.XGBModel)):
            framework_name = "xgboost"
            framework_version = xgb.__version__

    try:
        import tensorflow as tf
    except ImportError:
        pass
    else:
        if isinstance(model, tf.Module):
            framework_name = "tensorflow"
            framework_version = tf.__version__

    if framework_name not in _FRAMEWORK_SPECS:
        raise ValueError(
            f"Model type {model.__class__.__module__}.{model.__class__.__name__} not supported."
        )

    save_method = _FRAMEWORK_SPECS[framework_name]["save_method"]
    model_file = _FRAMEWORK_SPECS[framework_name]["model_file"]

    if not uri:
        staging_bucket = initializer.global_config.staging_bucket
        
        if not staging_bucket:
            project = project or initializer.global_config.project
            location = location or initializer.global_config.location
            credentials = credentials or initializer.global_config.credentials

            staging_bucket_name = project + "-vertex-staging-" + location
            client = storage.Client(project=project, credentials=credentials)
            staging_bucket = storage.Bucket(client=client, name=staging_bucket_name)
            if not staging_bucket.exists():
                _LOGGER.info(f'Creating staging bucket "{staging_bucket_name}"')
                staging_bucket = client.create_bucket(
                    bucket_or_name=staging_bucket,
                    project=project,
                    location=location,
                )
            staging_bucket = f"gs://{staging_bucket_name}"

        unique_name = utils.timestamped_unique_name()
        uri = f"{staging_bucket}/{unique_name}-{framework_name}-model"

    with tempfile.TemporaryDirectory() as temp_dir:
        
        if framework_name == "tensorflow":
            path = os.path.join(uri, model_file)
            model_class = save_method(model, path, tf_save_model_kwargs)
        
        else:
            path = os.path.join(temp_dir, model_file)
            model_class = save_method(model, path)

        if input_example is not None:
            _save_input_example(input_example, temp_dir)
            predict_schemata = schema_utils.PredictSchemata(
                instance_schema_uri=os.path.join(uri, "instance.yaml")
            )
        else:
            predict_schemata = None
        gcs_utils.upload_to_gcs(temp_dir, uri)

    model_artifact = google_artifact_schema.ExperimentModel(
        framework_name=framework_name,
        framework_version=framework_version,
        model_file=model_file,
        model_class=model_class,
        predict_schemata=predict_schemata,
        artifact_id=artifact_id,
        uri=uri,
        display_name=display_name,
    )
    model_artifact.create(
        metadata_store_id=metadata_store_id,
        project=project,
        location=location,
        credentials=credentials,
    )

    return model_artifact


def load_model(
    model: Union[str, google_artifact_schema.ExperimentModel]
) -> Union["sklearn.base.BaseEstimator", "xgb.Booster", "tf.Module"]:  
    
    if isinstance(model, str):
        model = aiplatform.get_experiment_model(model)
    framework_name = model.framework_name

    if framework_name not in _FRAMEWORK_SPECS:
        raise ValueError(f"Model type {framework_name} not supported.")

    load_method = _FRAMEWORK_SPECS[framework_name]["load_method"]
    model_file = _FRAMEWORK_SPECS[framework_name]["model_file"]

    source_file_uri = os.path.join(model.uri, model_file)
    
    if framework_name == "tensorflow":
        loaded_model = load_method(source_file_uri, model)
    
    else:
        with tempfile.TemporaryDirectory() as temp_dir:
            destination_file_path = os.path.join(temp_dir, model_file)
            gcs_utils.download_file_from_gcs(source_file_uri, destination_file_path)
            loaded_model = load_method(destination_file_path, model)

    return loaded_model



def register_model(
    model: Union[str, google_artifact_schema.ExperimentModel],
    *,
    model_id: Optional[str] = None,
    parent_model: Optional[str] = None,
    use_gpu: bool = False,
    is_default_version: bool = True,
    version_aliases: Optional[Sequence[str]] = None,
    version_description: Optional[str] = None,
    display_name: Optional[str] = None,
    description: Optional[str] = None,
    labels: Optional[Dict[str, str]] = None,
    serving_container_image_uri: Optional[str] = None,
    serving_container_predict_route: Optional[str] = None,
    serving_container_health_route: Optional[str] = None,
    serving_container_command: Optional[Sequence[str]] = None,
    serving_container_args: Optional[Sequence[str]] = None,
    serving_container_environment_variables: Optional[Dict[str, str]] = None,
    serving_container_ports: Optional[Sequence[int]] = None,
    instance_schema_uri: Optional[str] = None,
    parameters_schema_uri: Optional[str] = None,
    prediction_schema_uri: Optional[str] = None,
    explanation_metadata: Optional[explain.ExplanationMetadata] = None,
    explanation_parameters: Optional[explain.ExplanationParameters] = None,
    project: Optional[str] = None,
    location: Optional[str] = None,
    credentials: Optional[auth_credentials.Credentials] = None,
    encryption_spec_key_name: Optional[str] = None,
    staging_bucket: Optional[str] = None,
    sync: Optional[bool] = True,
    upload_request_timeout: Optional[float] = None,
) -> models.Model:
    
    if isinstance(model, str):
        model = aiplatform.get_experiment_model(model)

    project = project or model.project
    location = location or model.location
    credentials = credentials or model.credentials

    artifact_uri = model.uri
    framework_name = model.framework_name
    framework_version = model.framework_version
    artifact_uri = (
        f"{model.uri}/saved_model" if framework_name == "tensorflow" else model.uri
    )

    if not serving_container_image_uri:
        if framework_name == "tensorflow" and use_gpu:
            accelerator = "gpu"
        else:
            accelerator = "cpu"
        serving_container_image_uri = helpers._get_closest_match_prebuilt_container_uri(
            framework=framework_name,
            framework_version=framework_version,
            region=location,
            accelerator=accelerator,
        )

    if not display_name:
        display_name = models.Model._generate_display_name(f"{framework_name} model")

    return models.Model.upload(
        serving_container_image_uri=serving_container_image_uri,
        artifact_uri=artifact_uri,
        model_id=model_id,
        parent_model=parent_model,
        is_default_version=is_default_version,
        version_aliases=version_aliases,
        version_description=version_description,
        display_name=display_name,
        description=description,
        labels=labels,
        serving_container_predict_route=serving_container_predict_route,
        serving_container_health_route=serving_container_health_route,
        serving_container_command=serving_container_command,
        serving_container_args=serving_container_args,
        serving_container_environment_variables=serving_container_environment_variables,
        serving_container_ports=serving_container_ports,
        instance_schema_uri=instance_schema_uri,
        parameters_schema_uri=parameters_schema_uri,
        prediction_schema_uri=prediction_schema_uri,
        explanation_metadata=explanation_metadata,
        explanation_parameters=explanation_parameters,
        project=project,
        location=location,
        credentials=credentials,
        encryption_spec_key_name=encryption_spec_key_name,
        staging_bucket=staging_bucket,
        sync=sync,
        upload_request_timeout=upload_request_timeout,
    )


def get_experiment_model_info(
    model: Union[str, google_artifact_schema.ExperimentModel]
) -> Dict[str, Any]:
    
    if isinstance(model, str):
        model = aiplatform.get_experiment_model(model)

    model_info = {
        "model_class": model.model_class,
        "framework_name": model.framework_name,
        "framework_version": model.framework_version,
    }

    
    input_example = None
    source_file = f"{model.uri}/instance.yaml"
    with tempfile.TemporaryDirectory() as temp_dir:
        destination_file = os.path.join(temp_dir, "instance.yaml")
        try:
            gcs_utils.download_file_from_gcs(source_file, destination_file)
        except Exception:
            pass
        else:
            try:
                import yaml
            except ImportError:
                raise ImportError(
                    "PyYAML is not installed and is required for loading input examples."
                ) from None

            with open(destination_file, "r") as f:
                input_example = yaml.safe_load(f)["input_example"]

    if input_example:
        model_info["input_example"] = input_example

    return model_info
