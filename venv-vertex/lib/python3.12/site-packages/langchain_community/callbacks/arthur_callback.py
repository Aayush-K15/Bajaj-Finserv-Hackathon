

from __future__ import annotations

import os
import uuid
from collections import defaultdict
from datetime import datetime
from time import time
from typing import TYPE_CHECKING, Any, DefaultDict, Dict, List, Optional

import numpy as np
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult

if TYPE_CHECKING:
    import arthurai
    from arthurai.core.models import ArthurModel

PROMPT_TOKENS = "prompt_tokens"
COMPLETION_TOKENS = "completion_tokens"
TOKEN_USAGE = "token_usage"
FINISH_REASON = "finish_reason"
DURATION = "duration"


def _lazy_load_arthur() -> arthurai:
    
    try:
        import arthurai
    except ImportError as e:
        raise ImportError(
            "To use the ArthurCallbackHandler you need the"
            " `arthurai` package. Please install it with"
            " `pip install arthurai`.",
            e,
        )

    return arthurai


class ArthurCallbackHandler(BaseCallbackHandler):
    

    def __init__(
        self,
        arthur_model: ArthurModel,
    ) -> None:
        
        super().__init__()
        arthurai = _lazy_load_arthur()
        Stage = arthurai.common.constants.Stage
        ValueType = arthurai.common.constants.ValueType
        self.arthur_model = arthur_model
        
        
        self.attr_names = set([a.name for a in self.arthur_model.get_attributes()])
        self.input_attr = [
            x
            for x in self.arthur_model.get_attributes()
            if x.stage == Stage.ModelPipelineInput
            and x.value_type == ValueType.Unstructured_Text
        ][0].name
        self.output_attr = [
            x
            for x in self.arthur_model.get_attributes()
            if x.stage == Stage.PredictedValue
            and x.value_type == ValueType.Unstructured_Text
        ][0].name
        self.token_likelihood_attr = None
        if (
            len(
                [
                    x
                    for x in self.arthur_model.get_attributes()
                    if x.value_type == ValueType.TokenLikelihoods
                ]
            )
            > 0
        ):
            self.token_likelihood_attr = [
                x
                for x in self.arthur_model.get_attributes()
                if x.value_type == ValueType.TokenLikelihoods
            ][0].name

        self.run_map: DefaultDict[str, Any] = defaultdict(dict)

    @classmethod
    def from_credentials(
        cls,
        model_id: str,
        arthur_url: Optional[str] = "https://app.arthur.ai",
        arthur_login: Optional[str] = None,
        arthur_password: Optional[str] = None,
    ) -> ArthurCallbackHandler:
        
        arthurai = _lazy_load_arthur()
        ArthurAI = arthurai.ArthurAI
        ResponseClientError = arthurai.common.exceptions.ResponseClientError

        
        if arthur_login is None:
            try:
                arthur_api_key = os.environ["ARTHUR_API_KEY"]
            except KeyError:
                raise ValueError(
                    "No Arthur authentication provided. Either give"
                    " a login to the ArthurCallbackHandler"
                    " or set an ARTHUR_API_KEY as an environment variable."
                )
            arthur = ArthurAI(url=arthur_url, access_key=arthur_api_key)
        else:
            if arthur_password is None:
                arthur = ArthurAI(url=arthur_url, login=arthur_login)
            else:
                arthur = ArthurAI(
                    url=arthur_url, login=arthur_login, password=arthur_password
                )
        
        try:
            arthur_model = arthur.get_model(model_id)
        except ResponseClientError:
            raise ValueError(
                f"Was unable to retrieve model with id {model_id} from Arthur."
                " Make sure the ID corresponds to a model that is currently"
                " registered with your Arthur account."
            )
        return cls(arthur_model)

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        
        run_id = kwargs["run_id"]
        self.run_map[run_id]["input_texts"] = prompts
        self.run_map[run_id]["start_time"] = time()

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        
        try:
            import pytz
        except ImportError as e:
            raise ImportError(
                "Could not import pytz. Please install it with 'pip install pytz'."
            ) from e

        run_id = kwargs["run_id"]

        
        
        try:
            run_map_data = self.run_map[run_id]
        except KeyError as e:
            raise KeyError(
                "This function has been called with a run_id"
                " that was never registered in on_llm_start()."
                " Restart and try running the LLM again"
            ) from e

        
        time_from_start_to_end = time() - run_map_data["start_time"]

        
        inferences = []
        for i, generations in enumerate(response.generations):
            for generation in generations:
                inference = {
                    "partner_inference_id": str(uuid.uuid4()),
                    "inference_timestamp": datetime.now(tz=pytz.UTC),
                    self.input_attr: run_map_data["input_texts"][i],
                    self.output_attr: generation.text,
                }

                if generation.generation_info is not None:
                    
                    
                    
                    if (
                        FINISH_REASON in generation.generation_info
                        and FINISH_REASON in self.attr_names
                    ):
                        inference[FINISH_REASON] = generation.generation_info[
                            FINISH_REASON
                        ]

                    
                    
                    logprobs_data = generation.generation_info["logprobs"]
                    if (
                        logprobs_data is not None
                        and self.token_likelihood_attr is not None
                    ):
                        logprobs = logprobs_data["top_logprobs"]
                        likelihoods = [
                            {k: np.exp(v) for k, v in logprobs[i].items()}
                            for i in range(len(logprobs))
                        ]
                        inference[self.token_likelihood_attr] = likelihoods

                
                
                if (
                    isinstance(response.llm_output, dict)
                    and TOKEN_USAGE in response.llm_output
                ):
                    token_usage = response.llm_output[TOKEN_USAGE]
                    if (
                        PROMPT_TOKENS in token_usage
                        and PROMPT_TOKENS in self.attr_names
                    ):
                        inference[PROMPT_TOKENS] = token_usage[PROMPT_TOKENS]
                    if (
                        COMPLETION_TOKENS in token_usage
                        and COMPLETION_TOKENS in self.attr_names
                    ):
                        inference[COMPLETION_TOKENS] = token_usage[COMPLETION_TOKENS]

                
                
                if DURATION in self.attr_names:
                    inference[DURATION] = time_from_start_to_end

                inferences.append(inference)

        
        self.arthur_model.send_inferences(inferences)

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        

    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        

    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any,
    ) -> None:
        

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        

    def on_tool_end(
        self,
        output: Any,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        

    def on_text(self, text: str, **kwargs: Any) -> None:
        

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        
