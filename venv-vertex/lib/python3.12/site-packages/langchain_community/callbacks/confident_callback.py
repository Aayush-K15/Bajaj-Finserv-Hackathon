
import os
import warnings
from typing import Any, Dict, List, Optional, Union

from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.outputs import LLMResult


class DeepEvalCallbackHandler(BaseCallbackHandler):
    

    REPO_URL: str = "https://github.com/confident-ai/deepeval"
    ISSUES_URL: str = f"{REPO_URL}/issues"
    BLOG_URL: str = "https://docs.confident-ai.com"  

    def __init__(
        self,
        metrics: List[Any],
        implementation_name: Optional[str] = None,
    ) -> None:
        

        super().__init__()

        
        try:
            import deepeval  
        except ImportError:
            raise ImportError(
                
            )

        if os.path.exists(".deepeval"):
            warnings.warn(
                
            )

        
        self.implementation_name = implementation_name
        self.metrics = metrics

        warnings.warn(
            (
                "The `DeepEvalCallbackHandler` is currently in beta and is subject to"
                " change based on updates to `langchain`. Please report any issues to"
                f" {self.ISSUES_URL} as an `integration` issue."
            ),
        )

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        
        self.prompts = prompts

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        
        pass

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        
        from deepeval.metrics.answer_relevancy import AnswerRelevancy
        from deepeval.metrics.bias_classifier import UnBiasedMetric
        from deepeval.metrics.metric import Metric
        from deepeval.metrics.toxic_classifier import NonToxicMetric

        for metric in self.metrics:
            for i, generation in enumerate(response.generations):
                
                output = generation[0].text
                query = self.prompts[i]
                if isinstance(metric, AnswerRelevancy):
                    result = metric.measure(
                        output=output,
                        query=query,
                    )
                    print(f"Answer Relevancy: {result}")  
                elif isinstance(metric, UnBiasedMetric):
                    score = metric.measure(output)
                    print(f"Bias Score: {score}")  
                elif isinstance(metric, NonToxicMetric):
                    score = metric.measure(output)
                    print(f"Toxic Score: {score}")  
                else:
                    raise ValueError(
                        f
                    )

    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
        
        pass

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        
        pass

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        
        pass

    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:
        
        pass

    def on_tool_start(
        self,
        serialized: Dict[str, Any],
        input_str: str,
        **kwargs: Any,
    ) -> None:
        
        pass

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        
        pass

    def on_tool_end(
        self,
        output: Any,
        observation_prefix: Optional[str] = None,
        llm_prefix: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        
        pass

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
        
        pass

    def on_text(self, text: str, **kwargs: Any) -> None:
        
        pass

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        
        pass
