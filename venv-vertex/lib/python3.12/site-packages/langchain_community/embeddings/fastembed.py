import importlib
import importlib.metadata
from typing import Any, Dict, List, Literal, Optional, Sequence, cast

import numpy as np
from langchain_core.embeddings import Embeddings
from langchain_core.utils import pre_init
from pydantic import BaseModel, ConfigDict

MIN_VERSION = "0.2.0"


class FastEmbedEmbeddings(BaseModel, Embeddings):
    

    model_name: str = "BAAI/bge-small-en-v1.5"
    

    max_length: int = 512
    

    cache_dir: Optional[str] = None
    

    threads: Optional[int] = None
    

    doc_embed_type: Literal["default", "passage"] = "default"
    

    batch_size: int = 256
    

    parallel: Optional[int] = None
    

    providers: Optional[Sequence[Any]] = None
    

    model: Any = None  

    model_config = ConfigDict(extra="allow", protected_namespaces=())

    @pre_init
    def validate_environment(cls, values: Dict) -> Dict:
        
        model_name = values.get("model_name")
        max_length = values.get("max_length")
        cache_dir = values.get("cache_dir")
        threads = values.get("threads")
        providers = values.get("providers")
        pkg_to_install = (
            "fastembed-gpu"
            if providers and "CUDAExecutionProvider" in providers
            else "fastembed"
        )

        try:
            fastembed = importlib.import_module("fastembed")

        except ModuleNotFoundError:
            raise ImportError(
                "Could not import 'fastembed' Python package. "
                f"Please install it with `pip install {pkg_to_install}`."
            )

        if importlib.metadata.version(pkg_to_install) < MIN_VERSION:
            raise ImportError(
                f"FastEmbedEmbeddings requires "
                f'`pip install -U "{pkg_to_install}>={MIN_VERSION}"`.'
            )

        values["model"] = fastembed.TextEmbedding(
            model_name=model_name,
            max_length=max_length,
            cache_dir=cache_dir,
            threads=threads,
            providers=providers,
        )
        return values

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        
        embeddings: List[np.ndarray]
        if self.doc_embed_type == "passage":
            embeddings = self.model.passage_embed(
                texts, batch_size=self.batch_size, parallel=self.parallel
            )
        else:
            embeddings = self.model.embed(
                texts, batch_size=self.batch_size, parallel=self.parallel
            )
        return [cast(List[float], e.tolist()) for e in embeddings]

    def embed_query(self, text: str) -> List[float]:
        
        query_embeddings: np.ndarray = next(
            self.model.query_embed(
                text, batch_size=self.batch_size, parallel=self.parallel
            )
        )
        return cast(List[float], query_embeddings.tolist())
