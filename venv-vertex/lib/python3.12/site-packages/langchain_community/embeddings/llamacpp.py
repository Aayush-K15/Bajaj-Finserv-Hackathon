from typing import Any, List, Optional

from langchain_core.embeddings import Embeddings
from pydantic import BaseModel, ConfigDict, Field, model_validator
from typing_extensions import Self


class LlamaCppEmbeddings(BaseModel, Embeddings):
    

    client: Any = None  
    model_path: str = Field(default="")

    n_ctx: int = Field(512, alias="n_ctx")
    

    n_parts: int = Field(-1, alias="n_parts")
    

    seed: int = Field(-1, alias="seed")
    

    f16_kv: bool = Field(False, alias="f16_kv")
    

    logits_all: bool = Field(False, alias="logits_all")
    

    vocab_only: bool = Field(False, alias="vocab_only")
    

    use_mlock: bool = Field(False, alias="use_mlock")
    

    n_threads: Optional[int] = Field(None, alias="n_threads")
    

    n_batch: Optional[int] = Field(512, alias="n_batch")
    

    n_gpu_layers: Optional[int] = Field(None, alias="n_gpu_layers")
    

    verbose: bool = Field(True, alias="verbose")
    

    device: Optional[str] = Field(None, alias="device")
    

    model_config = ConfigDict(
        extra="forbid",
        protected_namespaces=(),
    )

    @model_validator(mode="after")
    def validate_environment(self) -> Self:
        
        model_path = self.model_path
        model_param_names = [
            "n_ctx",
            "n_parts",
            "seed",
            "f16_kv",
            "logits_all",
            "vocab_only",
            "use_mlock",
            "n_threads",
            "n_batch",
            "verbose",
            "device",
        ]
        model_params = {k: getattr(self, k) for k in model_param_names}
        
        if self.n_gpu_layers is not None:
            model_params["n_gpu_layers"] = self.n_gpu_layers

        if not self.client:
            try:
                from llama_cpp import Llama

                self.client = Llama(model_path, embedding=True, **model_params)
            except ImportError:
                raise ImportError(
                    "Could not import llama-cpp-python library. "
                    "Please install the llama-cpp-python library to "
                    "use this embedding model: pip install llama-cpp-python"
                )
            except Exception as e:
                raise ValueError(
                    f"Could not load Llama model from path: {model_path}. "
                    f"Received error {e}"
                )

        return self

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        
        embeddings = self.client.create_embedding(texts)
        final_embeddings = []
        for e in embeddings["data"]:
            try:
                if isinstance(e["embedding"][0], list):
                    for data in e["embedding"]:
                        final_embeddings.append(list(map(float, data)))
                else:
                    final_embeddings.append(list(map(float, e["embedding"])))
            except (IndexError, TypeError):
                final_embeddings.append(list(map(float, e["embedding"])))
        return final_embeddings

    def embed_query(self, text: str) -> List[float]:
        
        embedding = self.client.embed(text)
        if embedding and isinstance(embedding, list) and isinstance(embedding[0], list):
            return list(map(float, embedding[0]))
        else:
            return list(map(float, embedding))
