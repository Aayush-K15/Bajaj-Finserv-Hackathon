import logging
from typing import Any, Dict, List, Mapping, Optional

import requests
from langchain_core._api.deprecation import deprecated
from langchain_core.embeddings import Embeddings
from pydantic import BaseModel, ConfigDict

logger = logging.getLogger(__name__)


@deprecated(
    since="0.3.1",
    removal="1.0.0",
    alternative_import="langchain_ollama.OllamaEmbeddings",
)
class OllamaEmbeddings(BaseModel, Embeddings):
    

    base_url: str = "http://localhost:11434"
    
    model: str = "llama2"
    

    embed_instruction: str = "passage: "
    
    query_instruction: str = "query: "
    

    mirostat: Optional[int] = None
    

    mirostat_eta: Optional[float] = None
    

    mirostat_tau: Optional[float] = None
    

    num_ctx: Optional[int] = None
    

    num_gpu: Optional[int] = None
    

    num_thread: Optional[int] = None
    

    repeat_last_n: Optional[int] = None
    

    repeat_penalty: Optional[float] = None
    

    temperature: Optional[float] = None
    

    stop: Optional[List[str]] = None
    

    tfs_z: Optional[float] = None
    

    top_k: Optional[int] = None
    

    top_p: Optional[float] = None
    

    show_progress: bool = False
    

    headers: Optional[dict] = None
    

    @property
    def _default_params(self) -> Dict[str, Any]:
        
        return {
            "model": self.model,
            "options": {
                "mirostat": self.mirostat,
                "mirostat_eta": self.mirostat_eta,
                "mirostat_tau": self.mirostat_tau,
                "num_ctx": self.num_ctx,
                "num_gpu": self.num_gpu,
                "num_thread": self.num_thread,
                "repeat_last_n": self.repeat_last_n,
                "repeat_penalty": self.repeat_penalty,
                "temperature": self.temperature,
                "stop": self.stop,
                "tfs_z": self.tfs_z,
                "top_k": self.top_k,
                "top_p": self.top_p,
            },
        }

    model_kwargs: Optional[dict] = None
    

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        
        return {**{"model": self.model}, **self._default_params}

    model_config = ConfigDict(extra="forbid", protected_namespaces=())

    def _process_emb_response(self, input: str) -> List[float]:
        
        headers = {
            "Content-Type": "application/json",
            **(self.headers or {}),
        }

        try:
            res = requests.post(
                f"{self.base_url}/api/embeddings",
                headers=headers,
                json={"model": self.model, "prompt": input, **self._default_params},
            )
        except requests.exceptions.RequestException as e:
            raise ValueError(f"Error raised by inference endpoint: {e}")

        if res.status_code != 200:
            raise ValueError(
                "Error raised by inference API HTTP code: %s, %s"
                % (res.status_code, res.text)
            )
        try:
            t = res.json()
            return t["embedding"]
        except requests.exceptions.JSONDecodeError as e:
            raise ValueError(
                f"Error raised by inference API: {e}.\nResponse: {res.text}"
            )

    def _embed(self, input: List[str]) -> List[List[float]]:
        if self.show_progress:
            try:
                from tqdm import tqdm

                iter_ = tqdm(input, desc="OllamaEmbeddings")
            except ImportError:
                logger.warning(
                    "Unable to show progress bar because tqdm could not be imported. "
                    "Please install with `pip install tqdm`."
                )
                iter_ = input
        else:
            iter_ = input
        return [self._process_emb_response(prompt) for prompt in iter_]

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        
        instruction_pairs = [f"{self.embed_instruction}{text}" for text in texts]
        embeddings = self._embed(instruction_pairs)
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        
        instruction_pair = f"{self.query_instruction}{text}"
        embedding = self._embed([instruction_pair])[0]
        return embedding
