import importlib.util
import os
from typing import Any, Dict, List, Optional

from langchain_core.embeddings import Embeddings
from pydantic import BaseModel, ConfigDict


class QuantizedBgeEmbeddings(BaseModel, Embeddings):
      

    def __init__(
        self,
        model_name: str,
        *,
        max_seq_len: int = 512,
        pooling_strategy: str = "mean",  
        query_instruction: Optional[str] = None,
        document_instruction: Optional[str] = None,
        padding: bool = True,
        model_kwargs: Optional[Dict] = None,
        encode_kwargs: Optional[Dict] = None,
        onnx_file_name: Optional[str] = "int8-model.onnx",
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)

        
        if importlib.util.find_spec("intel_extension_for_transformers") is None:
            raise ImportError(
                "Could not import intel_extension_for_transformers python package. "
                "Please install it with "
                "`pip install -U intel-extension-for-transformers`."
            )

        
        if importlib.util.find_spec("torch") is None:
            raise ImportError(
                "Could not import torch python package. "
                "Please install it with `pip install -U torch`."
            )

        
        if importlib.util.find_spec("onnx") is None:
            raise ImportError(
                "Could not import onnx python package. "
                "Please install it with `pip install -U onnx`."
            )

        self.model_name_or_path = model_name
        self.max_seq_len = max_seq_len
        self.pooling = pooling_strategy
        self.padding = padding
        self.encode_kwargs = encode_kwargs or {}
        self.model_kwargs = model_kwargs or {}

        self.normalize = self.encode_kwargs.get("normalize_embeddings", False)
        self.batch_size = self.encode_kwargs.get("batch_size", 32)

        self.query_instruction = query_instruction
        self.document_instruction = document_instruction
        self.onnx_file_name = onnx_file_name

        self.load_model()

    def load_model(self) -> None:
        from huggingface_hub import hf_hub_download
        from intel_extension_for_transformers.transformers import AutoModel
        from transformers import AutoConfig, AutoTokenizer

        self.hidden_size = AutoConfig.from_pretrained(
            self.model_name_or_path
        ).hidden_size
        self.transformer_tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
        )
        onnx_model_path = os.path.join(self.model_name_or_path, self.onnx_file_name)  
        if not os.path.exists(onnx_model_path):
            onnx_model_path = hf_hub_download(
                self.model_name_or_path, filename=self.onnx_file_name
            )
        self.transformer_model = AutoModel.from_pretrained(
            onnx_model_path, use_embedding_runtime=True
        )

    model_config = ConfigDict(
        extra="allow",
        protected_namespaces=(),
    )

    def _embed(self, inputs: Any) -> Any:
        import torch

        engine_input = [value for value in inputs.values()]
        outputs = self.transformer_model.generate(engine_input)
        if "last_hidden_state:0" in outputs:
            last_hidden_state = outputs["last_hidden_state:0"]
        else:
            last_hidden_state = [out for out in outputs.values()][0]
        last_hidden_state = torch.tensor(last_hidden_state).reshape(
            inputs["input_ids"].shape[0], inputs["input_ids"].shape[1], self.hidden_size
        )
        if self.pooling == "mean":
            emb = self._mean_pooling(last_hidden_state, inputs["attention_mask"])
        elif self.pooling == "cls":
            emb = self._cls_pooling(last_hidden_state)
        else:
            raise ValueError("pooling method no supported")

        if self.normalize:
            emb = torch.nn.functional.normalize(emb, p=2, dim=1)
        return emb

    @staticmethod
    def _cls_pooling(last_hidden_state: Any) -> Any:
        return last_hidden_state[:, 0]

    @staticmethod
    def _mean_pooling(last_hidden_state: Any, attention_mask: Any) -> Any:
        try:
            import torch
        except ImportError as e:
            raise ImportError(
                "Unable to import torch, please install with `pip install -U torch`."
            ) from e
        input_mask_expanded = (
            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        )
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

    def _embed_text(self, texts: List[str]) -> List[List[float]]:
        inputs = self.transformer_tokenizer(
            texts,
            max_length=self.max_seq_len,
            truncation=True,
            padding=self.padding,
            return_tensors="pt",
        )
        return self._embed(inputs).tolist()

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        
        try:
            import pandas as pd
        except ImportError as e:
            raise ImportError(
                "Unable to import pandas, please install with `pip install -U pandas`."
            ) from e
        docs = [
            self.document_instruction + d if self.document_instruction else d
            for d in texts
        ]

        
        text_list_df = pd.DataFrame(docs, columns=["texts"]).reset_index()

        
        text_list_df["batch_index"] = text_list_df["index"] // self.batch_size

        
        batches = list(text_list_df.groupby(["batch_index"])["texts"].apply(list))

        vectors = []
        for batch in batches:
            vectors += self._embed_text(batch)
        return vectors

    def embed_query(self, text: str) -> List[float]:
        if self.query_instruction:
            text = self.query_instruction + text
        return self._embed_text([text])[0]
