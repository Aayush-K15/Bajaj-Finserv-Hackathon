

from __future__ import annotations

import asyncio
import json
import logging
import secrets
from dataclasses import asdict, is_dataclass
from typing import (
    TYPE_CHECKING,
    Any,
    AsyncIterable,
    Iterable,
    List,
    Optional,
    Sequence,
    Tuple,
    Type,
    TypeVar,
    cast,
)

from langchain_core._api import beta
from langchain_core.documents import Document
from typing_extensions import override

from langchain_community.graph_vectorstores.base import GraphVectorStore, Node
from langchain_community.graph_vectorstores.links import METADATA_LINKS_KEY, Link
from langchain_community.graph_vectorstores.mmr_helper import MmrHelper
from langchain_community.utilities.cassandra import SetupMode
from langchain_community.vectorstores.cassandra import Cassandra as CassandraVectorStore

CGVST = TypeVar("CGVST", bound="CassandraGraphVectorStore")

if TYPE_CHECKING:
    from cassandra.cluster import Session
    from langchain_core.embeddings import Embeddings


logger = logging.getLogger(__name__)


class AdjacentNode:
    id: str
    links: list[Link]
    embedding: list[float]

    def __init__(self, node: Node, embedding: list[float]) -> None:
        
        self.id = node.id or ""
        self.links = node.links
        self.embedding = embedding


def _serialize_links(links: list[Link]) -> str:
    class SetAndLinkEncoder(json.JSONEncoder):
        def default(self, obj: Any) -> Any:  
            if not isinstance(obj, type) and is_dataclass(obj):
                return asdict(obj)

            if isinstance(obj, Iterable):
                return list(obj)

            
            return super().default(obj)

    return json.dumps(links, cls=SetAndLinkEncoder)


def _deserialize_links(json_blob: str | None) -> set[Link]:
    return {
        Link(kind=link["kind"], direction=link["direction"], tag=link["tag"])
        for link in cast(list[dict[str, Any]], json.loads(json_blob or "[]"))
    }


def _metadata_link_key(link: Link) -> str:
    return f"link:{link.kind}:{link.tag}"


def _metadata_link_value() -> str:
    return "link"


def _doc_to_node(doc: Document) -> Node:
    metadata = doc.metadata.copy()
    links = _deserialize_links(metadata.get(METADATA_LINKS_KEY))
    metadata[METADATA_LINKS_KEY] = links

    return Node(
        id=doc.id,
        text=doc.page_content,
        metadata=metadata,
        links=list(links),
    )


def _incoming_links(node: Node | AdjacentNode) -> set[Link]:
    return {link for link in node.links if link.direction in ["in", "bidir"]}


def _outgoing_links(node: Node | AdjacentNode) -> set[Link]:
    return {link for link in node.links if link.direction in ["out", "bidir"]}


@beta()
class CassandraGraphVectorStore(GraphVectorStore):
    def __init__(
        self,
        embedding: Embeddings,
        session: Session | None = None,
        keyspace: str | None = None,
        table_name: str = "",
        ttl_seconds: int | None = None,
        *,
        body_index_options: list[tuple[str, Any]] | None = None,
        setup_mode: SetupMode = SetupMode.SYNC,
        metadata_deny_list: Optional[list[str]] = None,
    ) -> None:
        
        self.embedding = embedding

        if metadata_deny_list is None:
            metadata_deny_list = []
        metadata_deny_list.append(METADATA_LINKS_KEY)

        self.vector_store = CassandraVectorStore(
            embedding=embedding,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ttl_seconds=ttl_seconds,
            body_index_options=body_index_options,
            setup_mode=setup_mode,
            metadata_indexing=("deny_list", metadata_deny_list),
        )

        store_session: Session = self.vector_store.session

        self._insert_node = store_session.prepare(
            f  
        )

    @property
    @override
    def embeddings(self) -> Embeddings | None:
        return self.embedding

    def _get_metadata_filter(
        self,
        metadata: dict[str, Any] | None = None,
        outgoing_link: Link | None = None,
    ) -> dict[str, Any]:
        if outgoing_link is None:
            return metadata or {}

        metadata_filter = {} if metadata is None else metadata.copy()
        metadata_filter[_metadata_link_key(link=outgoing_link)] = _metadata_link_value()
        return metadata_filter

    def _restore_links(self, doc: Document) -> Document:
        
        links = _deserialize_links(doc.metadata.get(METADATA_LINKS_KEY))
        doc.metadata[METADATA_LINKS_KEY] = links
        
        
        for incoming_link_key in [
            _metadata_link_key(link=link)
            for link in links
            if link.direction in ["in", "bidir"]
        ]:
            if incoming_link_key in doc.metadata:
                del doc.metadata[incoming_link_key]

        return doc

    def _get_node_metadata_for_insertion(self, node: Node) -> dict[str, Any]:
        metadata = node.metadata.copy()
        metadata[METADATA_LINKS_KEY] = _serialize_links(node.links)
        
        
        for incoming_link in _incoming_links(node=node):
            metadata[_metadata_link_key(link=incoming_link)] = _metadata_link_value()
        return metadata

    def _get_docs_for_insertion(
        self, nodes: Iterable[Node]
    ) -> tuple[list[Document], list[str]]:
        docs = []
        ids = []
        for node in nodes:
            node_id = secrets.token_hex(8) if not node.id else node.id

            doc = Document(
                page_content=node.text,
                metadata=self._get_node_metadata_for_insertion(node=node),
                id=node_id,
            )
            docs.append(doc)
            ids.append(node_id)
        return (docs, ids)

    @override
    def add_nodes(
        self,
        nodes: Iterable[Node],
        **kwargs: Any,
    ) -> Iterable[str]:
        
        (docs, ids) = self._get_docs_for_insertion(nodes=nodes)
        return self.vector_store.add_documents(docs, ids=ids)

    @override
    async def aadd_nodes(
        self,
        nodes: Iterable[Node],
        **kwargs: Any,
    ) -> AsyncIterable[str]:
        
        (docs, ids) = self._get_docs_for_insertion(nodes=nodes)
        for inserted_id in await self.vector_store.aadd_documents(docs, ids=ids):
            yield inserted_id

    @override
    def similarity_search(
        self,
        query: str,
        k: int = 4,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> list[Document]:
        
        return [
            self._restore_links(doc)
            for doc in self.vector_store.similarity_search(
                query=query,
                k=k,
                filter=filter,
                **kwargs,
            )
        ]

    @override
    async def asimilarity_search(
        self,
        query: str,
        k: int = 4,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> list[Document]:
        
        return [
            self._restore_links(doc)
            for doc in await self.vector_store.asimilarity_search(
                query=query,
                k=k,
                filter=filter,
                **kwargs,
            )
        ]

    @override
    def similarity_search_by_vector(
        self,
        embedding: list[float],
        k: int = 4,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> list[Document]:
        
        return [
            self._restore_links(doc)
            for doc in self.vector_store.similarity_search_by_vector(
                embedding,
                k=k,
                filter=filter,
                **kwargs,
            )
        ]

    @override
    async def asimilarity_search_by_vector(
        self,
        embedding: list[float],
        k: int = 4,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> list[Document]:
        
        return [
            self._restore_links(doc)
            for doc in await self.vector_store.asimilarity_search_by_vector(
                embedding,
                k=k,
                filter=filter,
                **kwargs,
            )
        ]

    def metadata_search(
        self,
        filter: dict[str, Any] | None = None,  
        n: int = 5,
    ) -> Iterable[Document]:
        
        return [
            self._restore_links(doc)
            for doc in self.vector_store.metadata_search(
                filter=filter or {},
                n=n,
            )
        ]

    async def ametadata_search(
        self,
        filter: dict[str, Any] | None = None,  
        n: int = 5,
    ) -> Iterable[Document]:
        
        return [
            self._restore_links(doc)
            for doc in await self.vector_store.ametadata_search(
                filter=filter or {},
                n=n,
            )
        ]

    def get_by_document_id(self, document_id: str) -> Document | None:
        
        doc = self.vector_store.get_by_document_id(document_id=document_id)
        return self._restore_links(doc) if doc is not None else None

    async def aget_by_document_id(self, document_id: str) -> Document | None:
        
        doc = await self.vector_store.aget_by_document_id(document_id=document_id)
        return self._restore_links(doc) if doc is not None else None

    def get_node(self, node_id: str) -> Node | None:
        
        doc = self.vector_store.get_by_document_id(document_id=node_id)
        if doc is None:
            return None
        return _doc_to_node(doc=doc)

    @override
    async def ammr_traversal_search(  
        self,
        query: str,
        *,
        initial_roots: Sequence[str] = (),
        k: int = 4,
        depth: int = 2,
        fetch_k: int = 100,
        adjacent_k: int = 10,
        lambda_mult: float = 0.5,
        score_threshold: float = float("-inf"),
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[Document]:
        
        query_embedding = self.embedding.embed_query(query)
        helper = MmrHelper(
            k=k,
            query_embedding=query_embedding,
            lambda_mult=lambda_mult,
            score_threshold=score_threshold,
        )

        
        outgoing_links_map: dict[str, set[Link]] = {}
        visited_links: set[Link] = set()
        
        retrieved_docs: dict[str, Document] = {}

        async def fetch_neighborhood(neighborhood: Sequence[str]) -> None:
            nonlocal outgoing_links_map, visited_links, retrieved_docs

            
            
            outgoing_links_map.update(
                {content_id: set() for content_id in neighborhood}
            )

            
            
            visited_links = await self._get_outgoing_links(neighborhood)

            
            adjacent_nodes = await self._get_adjacent(
                links=visited_links,
                query_embedding=query_embedding,
                k_per_link=adjacent_k,
                filter=filter,
                retrieved_docs=retrieved_docs,
            )

            new_candidates: dict[str, list[float]] = {}
            for adjacent_node in adjacent_nodes:
                if adjacent_node.id not in outgoing_links_map:
                    outgoing_links_map[adjacent_node.id] = _outgoing_links(
                        node=adjacent_node
                    )
                    new_candidates[adjacent_node.id] = adjacent_node.embedding
            helper.add_candidates(new_candidates)

        async def fetch_initial_candidates() -> None:
            nonlocal outgoing_links_map, visited_links, retrieved_docs

            results = (
                await self.vector_store.asimilarity_search_with_embedding_id_by_vector(
                    embedding=query_embedding,
                    k=fetch_k,
                    filter=filter,
                )
            )

            candidates: dict[str, list[float]] = {}
            for doc, embedding, doc_id in results:
                if doc_id not in retrieved_docs:
                    retrieved_docs[doc_id] = doc

                if doc_id not in outgoing_links_map:
                    node = _doc_to_node(doc)
                    outgoing_links_map[doc_id] = _outgoing_links(node=node)
                    candidates[doc_id] = embedding
            helper.add_candidates(candidates)

        if initial_roots:
            await fetch_neighborhood(initial_roots)
        if fetch_k > 0:
            await fetch_initial_candidates()

        
        depths = {candidate_id: 0 for candidate_id in helper.candidate_ids()}

        
        for _ in range(k):
            selected_id = helper.pop_best()

            if selected_id is None:
                break

            next_depth = depths[selected_id] + 1
            if next_depth < depth:
                
                

                
                selected_outgoing_links = outgoing_links_map.pop(selected_id)

                
                selected_outgoing_links.difference_update(visited_links)

                
                adjacent_nodes = await self._get_adjacent(
                    links=selected_outgoing_links,
                    query_embedding=query_embedding,
                    k_per_link=adjacent_k,
                    filter=filter,
                    retrieved_docs=retrieved_docs,
                )

                
                visited_links.update(selected_outgoing_links)

                new_candidates = {}
                for adjacent_node in adjacent_nodes:
                    if adjacent_node.id not in outgoing_links_map:
                        outgoing_links_map[adjacent_node.id] = _outgoing_links(
                            node=adjacent_node
                        )
                        new_candidates[adjacent_node.id] = adjacent_node.embedding
                        if next_depth < depths.get(adjacent_node.id, depth + 1):
                            
                            
                            
                            
                            
                            
                            
                            
                            
                            
                            depths[adjacent_node.id] = next_depth
                helper.add_candidates(new_candidates)

        for doc_id, similarity_score, mmr_score in zip(
            helper.selected_ids,
            helper.selected_similarity_scores,
            helper.selected_mmr_scores,
        ):
            if doc_id in retrieved_docs:
                doc = self._restore_links(retrieved_docs[doc_id])
                doc.metadata["similarity_score"] = similarity_score
                doc.metadata["mmr_score"] = mmr_score
                yield doc
            else:
                msg = f"retrieved_docs should contain id: {doc_id}"
                raise RuntimeError(msg)

    @override
    def mmr_traversal_search(
        self,
        query: str,
        *,
        initial_roots: Sequence[str] = (),
        k: int = 4,
        depth: int = 2,
        fetch_k: int = 100,
        adjacent_k: int = 10,
        lambda_mult: float = 0.5,
        score_threshold: float = float("-inf"),
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> Iterable[Document]:
        

        async def collect_docs() -> Iterable[Document]:
            async_iter = self.ammr_traversal_search(
                query=query,
                initial_roots=initial_roots,
                k=k,
                depth=depth,
                fetch_k=fetch_k,
                adjacent_k=adjacent_k,
                lambda_mult=lambda_mult,
                score_threshold=score_threshold,
                filter=filter,
                **kwargs,
            )
            return [doc async for doc in async_iter]

        return asyncio.run(collect_docs())

    @override
    async def atraversal_search(  
        self,
        query: str,
        *,
        k: int = 4,
        depth: int = 1,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[Document]:
        
        
        
        
        
        
        
        
        
        
        

        
        visited_ids: dict[str, int] = {}

        
        visited_links: dict[Link, int] = {}

        
        retrieved_docs: dict[str, Document] = {}

        async def visit_nodes(d: int, docs: Iterable[Document]) -> None:
            
            nonlocal visited_ids, visited_links, retrieved_docs

            
            
            
            outgoing_links: set[Link] = set()
            for doc in docs:
                if doc.id is not None:
                    if doc.id not in retrieved_docs:
                        retrieved_docs[doc.id] = doc

                    
                    if d <= visited_ids.get(doc.id, depth):
                        visited_ids[doc.id] = d

                        
                        if d < depth:
                            node = _doc_to_node(doc=doc)
                            
                            
                            for link in _outgoing_links(node=node):
                                if d <= visited_links.get(link, depth):
                                    
                                    
                                    
                                    visited_links[link] = d
                                    outgoing_links.add(link)

            if outgoing_links:
                metadata_search_tasks = []
                for outgoing_link in outgoing_links:
                    metadata_filter = self._get_metadata_filter(
                        metadata=filter,
                        outgoing_link=outgoing_link,
                    )
                    metadata_search_tasks.append(
                        asyncio.create_task(
                            self.vector_store.ametadata_search(
                                filter=metadata_filter, n=1000
                            )
                        )
                    )
                results = await asyncio.gather(*metadata_search_tasks)

                
                visit_target_tasks = [
                    visit_targets(d=d + 1, docs=docs) for docs in results
                ]
                await asyncio.gather(*visit_target_tasks)

        async def visit_targets(d: int, docs: Iterable[Document]) -> None:
            
            nonlocal visited_ids, retrieved_docs

            new_ids_at_next_depth = set()
            for doc in docs:
                if doc.id is not None:
                    if doc.id not in retrieved_docs:
                        retrieved_docs[doc.id] = doc

                    if d <= visited_ids.get(doc.id, depth):
                        new_ids_at_next_depth.add(doc.id)

            if new_ids_at_next_depth:
                visit_node_tasks = [
                    visit_nodes(d=d, docs=[retrieved_docs[doc_id]])
                    for doc_id in new_ids_at_next_depth
                    if doc_id in retrieved_docs
                ]

                fetch_tasks = [
                    asyncio.create_task(
                        self.vector_store.aget_by_document_id(document_id=doc_id)
                    )
                    for doc_id in new_ids_at_next_depth
                    if doc_id not in retrieved_docs
                ]

                new_docs: list[Document | None] = await asyncio.gather(*fetch_tasks)

                visit_node_tasks.extend(
                    visit_nodes(d=d, docs=[new_doc])
                    for new_doc in new_docs
                    if new_doc is not None
                )

                await asyncio.gather(*visit_node_tasks)

        
        initial_docs = self.vector_store.similarity_search(
            query=query,
            k=k,
            filter=filter,
        )
        await visit_nodes(d=0, docs=initial_docs)

        for doc_id in visited_ids:
            if doc_id in retrieved_docs:
                yield self._restore_links(retrieved_docs[doc_id])
            else:
                msg = f"retrieved_docs should contain id: {doc_id}"
                raise RuntimeError(msg)

    @override
    def traversal_search(
        self,
        query: str,
        *,
        k: int = 4,
        depth: int = 1,
        filter: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -> Iterable[Document]:
        

        async def collect_docs() -> Iterable[Document]:
            async_iter = self.atraversal_search(
                query=query,
                k=k,
                depth=depth,
                filter=filter,
                **kwargs,
            )
            return [doc async for doc in async_iter]

        return asyncio.run(collect_docs())

    async def _get_outgoing_links(self, source_ids: Iterable[str]) -> set[Link]:
        
        links = set()

        
        coroutines = [
            self.vector_store.aget_by_document_id(document_id=source_id)
            for source_id in source_ids
        ]

        
        docs = await asyncio.gather(*coroutines)

        for doc in docs:
            if doc is not None:
                node = _doc_to_node(doc=doc)
                links.update(_outgoing_links(node=node))

        return links

    async def _get_adjacent(
        self,
        links: set[Link],
        query_embedding: list[float],
        retrieved_docs: dict[str, Document],
        k_per_link: int | None = None,
        filter: dict[str, Any] | None = None,  
    ) -> Iterable[AdjacentNode]:
        
        targets: dict[str, AdjacentNode] = {}

        tasks = []
        for link in links:
            metadata_filter = self._get_metadata_filter(
                metadata=filter,
                outgoing_link=link,
            )

            tasks.append(
                self.vector_store.asimilarity_search_with_embedding_id_by_vector(
                    embedding=query_embedding,
                    k=k_per_link or 10,
                    filter=metadata_filter,
                )
            )

        results = await asyncio.gather(*tasks)

        for result in results:
            for doc, embedding, doc_id in result:
                if doc_id not in retrieved_docs:
                    retrieved_docs[doc_id] = doc
                if doc_id not in targets:
                    node = _doc_to_node(doc=doc)
                    targets[doc_id] = AdjacentNode(node=node, embedding=embedding)

        
        
        return targets.values()

    @staticmethod
    def _build_docs_from_texts(
        texts: List[str],
        metadatas: Optional[List[dict]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[Document]:
        docs: List[Document] = []
        for i, text in enumerate(texts):
            doc = Document(
                page_content=text,
            )
            if metadatas is not None:
                doc.metadata = metadatas[i]
            if ids is not None:
                doc.id = ids[i]
            docs.append(doc)
        return docs

    @classmethod
    def from_texts(
        cls: Type[CGVST],
        texts: List[str],
        embedding: Embeddings,
        metadatas: Optional[List[dict]] = None,
        *,
        session: Optional[Session] = None,
        keyspace: Optional[str] = None,
        table_name: str = "",
        ids: Optional[List[str]] = None,
        ttl_seconds: Optional[int] = None,
        body_index_options: Optional[List[Tuple[str, Any]]] = None,
        metadata_deny_list: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> CGVST:
        
        docs = cls._build_docs_from_texts(
            texts=texts,
            metadatas=metadatas,
            ids=ids,
        )

        return cls.from_documents(
            documents=docs,
            embedding=embedding,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ttl_seconds=ttl_seconds,
            body_index_options=body_index_options,
            metadata_deny_list=metadata_deny_list,
            **kwargs,
        )

    @classmethod
    async def afrom_texts(
        cls: Type[CGVST],
        texts: List[str],
        embedding: Embeddings,
        metadatas: Optional[List[dict]] = None,
        *,
        session: Optional[Session] = None,
        keyspace: Optional[str] = None,
        table_name: str = "",
        ids: Optional[List[str]] = None,
        ttl_seconds: Optional[int] = None,
        body_index_options: Optional[List[Tuple[str, Any]]] = None,
        metadata_deny_list: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> CGVST:
        
        docs = cls._build_docs_from_texts(
            texts=texts,
            metadatas=metadatas,
            ids=ids,
        )

        return await cls.afrom_documents(
            documents=docs,
            embedding=embedding,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ttl_seconds=ttl_seconds,
            body_index_options=body_index_options,
            metadata_deny_list=metadata_deny_list,
            **kwargs,
        )

    @staticmethod
    def _add_ids_to_docs(
        docs: List[Document],
        ids: Optional[List[str]] = None,
    ) -> List[Document]:
        if ids is not None:
            for doc, doc_id in zip(docs, ids):
                doc.id = doc_id
        return docs

    @classmethod
    def from_documents(
        cls: Type[CGVST],
        documents: List[Document],
        embedding: Embeddings,
        *,
        session: Optional[Session] = None,
        keyspace: Optional[str] = None,
        table_name: str = "",
        ids: Optional[List[str]] = None,
        ttl_seconds: Optional[int] = None,
        body_index_options: Optional[List[Tuple[str, Any]]] = None,
        metadata_deny_list: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> CGVST:
        
        store = cls(
            embedding=embedding,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ttl_seconds=ttl_seconds,
            body_index_options=body_index_options,
            metadata_deny_list=metadata_deny_list,
            **kwargs,
        )
        store.add_documents(documents=cls._add_ids_to_docs(docs=documents, ids=ids))
        return store

    @classmethod
    async def afrom_documents(
        cls: Type[CGVST],
        documents: List[Document],
        embedding: Embeddings,
        *,
        session: Optional[Session] = None,
        keyspace: Optional[str] = None,
        table_name: str = "",
        ids: Optional[List[str]] = None,
        ttl_seconds: Optional[int] = None,
        body_index_options: Optional[List[Tuple[str, Any]]] = None,
        metadata_deny_list: Optional[list[str]] = None,
        **kwargs: Any,
    ) -> CGVST:
        
        store = cls(
            embedding=embedding,
            session=session,
            keyspace=keyspace,
            table_name=table_name,
            ttl_seconds=ttl_seconds,
            setup_mode=SetupMode.ASYNC,
            body_index_options=body_index_options,
            metadata_deny_list=metadata_deny_list,
            **kwargs,
        )
        await store.aadd_documents(
            documents=cls._add_ids_to_docs(docs=documents, ids=ids)
        )
        return store
