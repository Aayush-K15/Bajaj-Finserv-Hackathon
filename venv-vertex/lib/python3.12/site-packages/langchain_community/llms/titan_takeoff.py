from enum import Enum
from typing import Any, Iterator, List, Optional

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from pydantic import BaseModel, ConfigDict

from langchain_community.llms.utils import enforce_stop_tokens


class Device(str, Enum):
    

    cuda = "cuda"
    cpu = "cpu"


class ReaderConfig(BaseModel):
    

    model_config = ConfigDict(
        protected_namespaces=(),
    )

    model_name: str
    

    device: Device = Device.cuda
    

    consumer_group: str = "primary"
    

    tensor_parallel: Optional[int] = None
    

    max_seq_length: int = 512
    

    max_batch_size: int = 4
    


class TitanTakeoff(LLM):
    

    base_url: str = "http://localhost"
    

    port: int = 3000
    

    mgmt_port: int = 3001
    

    streaming: bool = False
    

    client: Any = None
    

    def __init__(
        self,
        base_url: str = "http://localhost",
        port: int = 3000,
        mgmt_port: int = 3001,
        streaming: bool = False,
        models: List[ReaderConfig] = [],
    ):
        
        super().__init__(  
            base_url=base_url, port=port, mgmt_port=mgmt_port, streaming=streaming
        )
        try:
            from takeoff_client import TakeoffClient
        except ImportError:
            raise ImportError(
                "takeoff-client is required for TitanTakeoff. "
                "Please install it with `pip install 'takeoff-client>=0.4.0'`."
            )
        self.client = TakeoffClient(
            self.base_url, port=self.port, mgmt_port=self.mgmt_port
        )
        for model in models:
            self.client.create_reader(model)

    @property
    def _llm_type(self) -> str:
        
        return "titan_takeoff"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        
        if self.streaming:
            text_output = ""
            for chunk in self._stream(
                prompt=prompt,
                stop=stop,
                run_manager=run_manager,
            ):
                text_output += chunk.text
            return text_output

        response = self.client.generate(prompt, **kwargs)
        text = response["text"]

        if stop is not None:
            text = enforce_stop_tokens(text, stop)
        return text

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        
        response = self.client.generate_stream(prompt, **kwargs)
        buffer = ""
        for text in response:
            buffer += text.data
            if "data:" in buffer:
                
                if buffer.startswith("data:"):
                    buffer = ""
                if len(buffer.split("data:", 1)) == 2:
                    content, _ = buffer.split("data:", 1)
                    buffer = content.rstrip("\n")
                
                if buffer:  
                    chunk = GenerationChunk(text=buffer)
                    buffer = ""  
                    if run_manager:
                        run_manager.on_llm_new_token(token=chunk.text)
                    yield chunk

        
        if buffer:
            chunk = GenerationChunk(text=buffer.replace("</s>", ""))
            if run_manager:
                run_manager.on_llm_new_token(token=chunk.text)
            yield chunk
