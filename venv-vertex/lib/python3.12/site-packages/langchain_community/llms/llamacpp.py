from __future__ import annotations

import logging
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Union

from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import GenerationChunk
from langchain_core.utils import get_pydantic_field_names, pre_init
from langchain_core.utils.utils import _build_model_kwargs
from pydantic import Field, model_validator

logger = logging.getLogger(__name__)


class LlamaCpp(LLM):
    

    client: Any = None  
    model_path: str
    

    lora_base: Optional[str] = None
    

    lora_path: Optional[str] = None
    

    n_ctx: int = Field(512, alias="n_ctx")
    

    n_parts: int = Field(-1, alias="n_parts")
    

    seed: int = Field(-1, alias="seed")
    

    f16_kv: bool = Field(True, alias="f16_kv")
    

    logits_all: bool = Field(False, alias="logits_all")
    

    vocab_only: bool = Field(False, alias="vocab_only")
    

    use_mlock: bool = Field(False, alias="use_mlock")
    

    n_threads: Optional[int] = Field(None, alias="n_threads")
    

    n_batch: Optional[int] = Field(8, alias="n_batch")
    

    n_gpu_layers: Optional[int] = Field(None, alias="n_gpu_layers")
    

    suffix: Optional[str] = Field(None)
    

    max_tokens: Optional[int] = 256
    

    temperature: Optional[float] = 0.8
    

    top_p: Optional[float] = 0.95
    

    logprobs: Optional[int] = Field(None)
    

    echo: Optional[bool] = False
    

    stop: Optional[List[str]] = []
    

    repeat_penalty: Optional[float] = 1.1
    

    top_k: Optional[int] = 40
    

    last_n_tokens_size: Optional[int] = 64
    

    use_mmap: Optional[bool] = True
    

    rope_freq_scale: float = 1.0
    

    rope_freq_base: float = 10000.0
    

    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    

    streaming: bool = True
    

    grammar_path: Optional[Union[str, Path]] = None
    
    grammar: Optional[Union[str, Any]] = None
    

    verbose: bool = True
    

    @pre_init
    def validate_environment(cls, values: Dict) -> Dict:
        
        try:
            from llama_cpp import Llama, LlamaGrammar
        except ImportError:
            raise ImportError(
                "Could not import llama-cpp-python library. "
                "Please install the llama-cpp-python library to "
                "use this embedding model: pip install llama-cpp-python"
            )

        model_path = values["model_path"]
        model_param_names = [
            "rope_freq_scale",
            "rope_freq_base",
            "lora_path",
            "lora_base",
            "n_ctx",
            "n_parts",
            "seed",
            "f16_kv",
            "logits_all",
            "vocab_only",
            "use_mlock",
            "n_threads",
            "n_batch",
            "use_mmap",
            "last_n_tokens_size",
            "verbose",
        ]
        model_params = {k: values[k] for k in model_param_names}
        
        if values["n_gpu_layers"] is not None:
            model_params["n_gpu_layers"] = values["n_gpu_layers"]

        model_params.update(values["model_kwargs"])

        try:
            values["client"] = Llama(model_path, **model_params)
        except Exception as e:
            raise ValueError(
                f"Could not load Llama model from path: {model_path}. "
                f"Received error {e}"
            )

        if values["grammar"] and values["grammar_path"]:
            grammar = values["grammar"]
            grammar_path = values["grammar_path"]
            raise ValueError(
                "Can only pass in one of grammar and grammar_path. Received "
                f"{grammar=} and {grammar_path=}."
            )
        elif isinstance(values["grammar"], str):
            values["grammar"] = LlamaGrammar.from_string(values["grammar"])
        elif values["grammar_path"]:
            values["grammar"] = LlamaGrammar.from_file(values["grammar_path"])
        else:
            pass
        return values

    @model_validator(mode="before")
    @classmethod
    def build_model_kwargs(cls, values: Dict[str, Any]) -> Any:
        
        all_required_field_names = get_pydantic_field_names(cls)
        values = _build_model_kwargs(values, all_required_field_names)
        return values

    @property
    def _default_params(self) -> Dict[str, Any]:
        
        params = {
            "suffix": self.suffix,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "logprobs": self.logprobs,
            "echo": self.echo,
            "stop_sequences": self.stop,  
            "repeat_penalty": self.repeat_penalty,
            "top_k": self.top_k,
        }
        if self.grammar:
            params["grammar"] = self.grammar
        return params

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        
        return {**{"model_path": self.model_path}, **self._default_params}

    @property
    def _llm_type(self) -> str:
        
        return "llamacpp"

    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:
        

        
        if self.stop and stop is not None:
            raise ValueError("`stop` found in both the input and default params.")

        params = self._default_params

        
        params.pop("stop_sequences")

        
        params["stop"] = self.stop or stop or []

        return params

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        
        if self.streaming:
            
            
            
            combined_text_output = ""
            for chunk in self._stream(
                prompt=prompt,
                stop=stop,
                run_manager=run_manager,
                **kwargs,
            ):
                combined_text_output += chunk.text
            return combined_text_output
        else:
            params = self._get_parameters(stop)
            params = {**params, **kwargs}
            result = self.client(prompt=prompt, **params)
            return result["choices"][0]["text"]

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        
        params = {**self._get_parameters(stop), **kwargs}
        result = self.client(prompt=prompt, stream=True, **params)
        for part in result:
            logprobs = part["choices"][0].get("logprobs", None)
            chunk = GenerationChunk(
                text=part["choices"][0]["text"],
                generation_info={"logprobs": logprobs},
            )
            if run_manager:
                run_manager.on_llm_new_token(
                    token=chunk.text, verbose=self.verbose, log_probs=logprobs
                )
            yield chunk

    def get_num_tokens(self, text: str) -> int:
        tokenized_text = self.client.tokenize(text.encode("utf-8"))
        return len(tokenized_text)
