























__version__ = '0.54'




import sys
import logging
import struct
import traceback
import os
import zlib






_thismodule_dir = os.path.normpath(os.path.abspath(os.path.dirname(__file__)))

_parent_dir = os.path.normpath(os.path.join(_thismodule_dir, '..'))

if not _parent_dir in sys.path:
    sys.path.insert(0, _parent_dir)

import olefile



def get_logger(name, level=logging.CRITICAL+1):
    
    
    
    if name in logging.Logger.manager.loggerDict:
        
        
        logger = logging.getLogger(name)
        
        logger.setLevel(level)
        return logger
    
    logger = logging.getLogger(name)
    
    
    logger.addHandler(logging.NullHandler())
    logger.setLevel(level)
    return logger





log = get_logger('ppt')


def enable_logging():
    
    log.setLevel(logging.NOTSET)





MAIN_STREAM_NAME = 'PowerPoint Document'


URL_OLEVBA_ISSUES = 'https://bitbucket.org/decalage/oletools/issues'
MSG_OLEVBA_ISSUES = 'Please report this issue on %s' % URL_OLEVBA_ISSUES





class PptUnexpectedData(Exception):
    
    def __init__(self, stream, field_name, found_value, expected_value):
        self.msg = \
            'In stream "{0}" for field "{1}" found value "{2}" but expected {3}!' \
            .format(stream, field_name, found_value, expected_value)
        super(PptUnexpectedData, self).__init__(self.msg)




def read_1(stream):
    
    return struct.unpack('<B', stream.read(1))[0]


def read_2(stream):
    
    return struct.unpack('<H', stream.read(2))[0]


def read_4(stream):
    
    return struct.unpack('<L', stream.read(4))[0]


def check_value(name, value, expected):
    
    if isinstance(expected, (list, tuple)):
        if value not in expected:
            exp_str = '[' + ' OR '.join('{0:04X}'.format(val)
                                        for val in expected) + ']'
            raise PptUnexpectedData(
                'Current User', name,
                '{0:04X}'.format(value), exp_str)
    elif expected != value:
        raise PptUnexpectedData(
            'Current User', name,
            '{0:04X}'.format(value), '{0:04X}'.format(expected))




class RecordHeader(object):
    

    def __init__(self):
        self.rec_ver = None
        self.rec_instance = None
        self.rec_type = None
        self.rec_len = None

    @classmethod
    def extract_from(clz, stream):
        
        
        obj = clz()
        
        version_instance, = struct.unpack('<H', stream.read(2))
        obj.rec_instance, obj.rec_ver = divmod(version_instance, 2**4)
        obj.rec_type, = struct.unpack('<H', stream.read(2))
        obj.rec_len, = struct.unpack('<L', stream.read(4))
        
        
        
        return obj

    @classmethod
    def generate(clz, rec_type, rec_len=None, rec_instance=0, rec_ver=0):
        
        if rec_type is None:
            raise ValueError('RECORD_TYPE not set!')
        version_instance = rec_ver + 2**4 * rec_instance
        if rec_len is None:
            return struct.pack('<HH', version_instance, rec_type)
        else:
            return struct.pack('<HHL', version_instance, rec_type, rec_len)


class PptType(object):
    

    RECORD_TYPE = None      
    RECORD_INSTANCE = 0x0   
    RECORD_VERSION = 0x000  

    @classmethod
    def extract_from(clz, stream):
        raise NotImplementedError('abstract base function!')

    def __init__(self, stream_name=MAIN_STREAM_NAME):
        self.stream_name = stream_name
        self.rec_head = None

    def read_rec_head(self, stream):
        self.rec_head = RecordHeader.extract_from(stream)

    def check_validity(self):
        
        raise NotImplementedError('abstract base function!')

    def check_value(self, name, value, expected):
        
        if isinstance(expected, (list, tuple)):
            if value not in expected:
                clz_name = self.__class__.__name__
                exp_str = '[' + ' OR '.join('{0:04X}'.format(val)
                                            for val in expected) + ']'
                return [PptUnexpectedData(
                    self.stream_name, clz_name + '.' + name,
                    '{0:04X}'.format(value), exp_str), ]
        elif expected != value:
            clz_name = self.__class__.__name__
            return [PptUnexpectedData(
                self.stream_name, clz_name + '.' + name,
                '{0:04X}'.format(value), '{0:04X}'.format(expected)), ]
        return []

    def check_range(self, name, value, expect_lower, expect_upper):
        

        is_err = False
        if expect_upper is None and expect_lower is None:
            raise ValueError('need at least one non-None boundary!')
        if expect_lower is not None:
            if value <= expect_lower:
                is_err = True
        if expect_upper is not None:
            if value >= expect_upper:
                is_err = True

        if is_err:
            clz_name = self.__class__.__name__
            if expect_lower is None:
                expect_str = '< {0:04X}'.format(expect_upper)
            elif expect_upper is None:
                expect_str = '> {0:04X}'.format(expect_lower)
            else:
                expect_str = 'within ({0:04X}, {1:04X})'.format(expect_lower,
                                                                expect_upper)
            return [PptUnexpectedData(self.stream_name, clz_name + '.' + name,
                                      '{0:04X}'.format(value), expect_str), ]
        else:
            return []

    def check_rec_head(self, length=None):
        

        errs = []
        errs.extend(self.check_value('rec_head.recVer', self.rec_head.rec_ver,
                                     self.RECORD_VERSION))
        errs.extend(self.check_value('rec_head.recInstance',
                                     self.rec_head.rec_instance,
                                     self.RECORD_INSTANCE))
        if self.RECORD_TYPE is None:
            raise NotImplementedError('RECORD_TYPE not specified!')
        errs.extend(self.check_value('rec_head.recType',
                                     self.rec_head.rec_type,
                                     self.RECORD_TYPE))
        if length is not None:
            errs.extend(self.check_value('rec_head.recLen',
                                         self.rec_head.rec_len, length))
        return errs

    @classmethod
    def generate_pattern(clz, rec_len=None):
        
        return RecordHeader.generate(clz.RECORD_TYPE, rec_len,
                                     clz.RECORD_INSTANCE, clz.RECORD_VERSION)


class CurrentUserAtom(PptType):
    

    
    HEADER_TOKEN_ENCRYPT = 0xF3D1C4DF
    HEADER_TOKEN_NOCRYPT = 0xE391C05F

    
    REL_VERSION_CAN_USE = 0x00000008
    REL_VERSION_NO_USE = 0x00000009

    
    RECORD_TYPE = 0x0FF6
    SIZE = 0x14
    DOC_FILE_VERSION = 0x03F4
    MAJOR_VERSION = 0x03
    MINOR_VERSION = 0x00

    def __init__(self):
        super(CurrentUserAtom, self).__init__(stream_name='Current User')
        self.rec_head = None
        self.size = None
        self.header_token = None
        self.offset_to_current_edit = None
        self.len_user_name = None
        self.doc_file_version = None
        self.major_version = None
        self.minor_version = None
        self.ansi_user_name = None
        self.unicode_user_name = None
        self.rel_version = None

    def is_encrypted(self):
        return self.header_token == self.HEADER_TOKEN_ENCRYPT

    @classmethod
    def extract_from(clz, stream):
        

        obj = clz()

        
        obj.rec_head = RecordHeader.extract_from(stream)

        obj.size, = struct.unpack('<L', stream.read(4))
        obj.header_token, = struct.unpack('<L', stream.read(4))
        obj.offset_to_current_edit, = struct.unpack('<L', stream.read(4))
        obj.len_user_name, = struct.unpack('<H', stream.read(2))
        obj.doc_file_version, = struct.unpack('<H', stream.read(2))
        obj.major_version, = struct.unpack('<B', stream.read(1))
        obj.minor_version, = struct.unpack('<B', stream.read(1))
        stream.read(2)    
        obj.ansi_user_name = stream.read(obj.len_user_name)
        obj.rel_version, = struct.unpack('<L', stream.read(4))
        obj.unicode_user_name = stream.read(2 * obj.len_user_name)

        return obj

    def check_validity(self):
        errs = self.check_rec_head()
        errs.extend(self.check_value('size', self.size, self.SIZE))
        errs.extend(self.check_value('headerToken', self.header_token,
                                     [self.HEADER_TOKEN_ENCRYPT,
                                      self.HEADER_TOKEN_NOCRYPT]))
        errs.extend(self.check_range('lenUserName', self.len_user_name, None,
                                     256))
        errs.extend(self.check_value('docFileVersion', self.doc_file_version,
                                     self.DOC_FILE_VERSION))
        errs.extend(self.check_value('majorVersion', self.major_version,
                                     self.MAJOR_VERSION))
        errs.extend(self.check_value('minorVersion', self.minor_version,
                                     self.MINOR_VERSION))
        errs.extend(self.check_value('relVersion', self.rel_version,
                                     [self.REL_VERSION_CAN_USE,
                                      self.REL_VERSION_NO_USE]))
        return errs


class UserEditAtom(PptType):
    

    RECORD_TYPE = 0x0FF5
    MINOR_VERSION = 0x00
    MAJOR_VERSION = 0x03

    def __init__(self):
        super(UserEditAtom, self).__init__()
        self.rec_head = None
        self.last_slide_id_ref = None
        self.version = None
        self.minor_version = None
        self.major_version = None
        self.offset_last_edit = None
        self.offset_persist_directory = None
        self.doc_persist_id_ref = None
        self.persist_id_seed = None
        self.last_view = None
        self.encrypt_session_persist_id_ref = None

    @classmethod
    def extract_from(clz, stream, is_encrypted):
        

        log.debug('extract UserEditAtom from stream')

        obj = clz()

        
        obj.rec_head = RecordHeader.extract_from(stream)

        obj.last_slide_id_ref, = struct.unpack('<L', stream.read(4))
        obj.version, = struct.unpack('<H', stream.read(2))
        obj.minor_version, = struct.unpack('<B', stream.read(1))
        obj.major_version, = struct.unpack('<B', stream.read(1))
        obj.offset_last_edit, = struct.unpack('<L', stream.read(4))
        obj.offset_persist_directory, = struct.unpack('<L', stream.read(4))
        obj.doc_persist_id_ref, = struct.unpack('<L', stream.read(4))
        obj.persist_id_seed, = struct.unpack('<L', stream.read(4))
        
        obj.last_view, = struct.unpack('<H', stream.read(2))
        stream.read(2)   
        if is_encrypted:
            obj.encrypt_session_persist_id_ref, = \
                struct.unpack('<L', stream.read(4))
        else:   
            obj.encrypt_session_persist_id_ref = None

        return obj

    def check_validity(self, offset=None):
        errs = self.check_rec_head()
        errs.extend(self.check_value('minorVersion', self.minor_version,
                                     self.MINOR_VERSION))
        errs.extend(self.check_value('majorVersion', self.major_version,
                                     self.MAJOR_VERSION))
        if offset is not None:
            if self.offset_last_edit >= offset:
                errs.append(PptUnexpectedData(
                    'PowerPoint Document', 'UserEditAtom.offsetLastEdit',
                    self.offset_last_edit, '< {0}'.format(offset)))
            if self.offset_persist_directory >= offset or \
                    self.offset_persist_directory <= self.offset_last_edit:
                errs.append(PptUnexpectedData(
                    'PowerPoint Document',
                    'UserEditAtom.offsetPersistDirectory',
                    self.offset_last_edit,
                    'in ({0}, {1})'.format(self.offset_last_edit, offset)))
        errs.extend(self.check_value('docPersistIdRef',
                                     self.doc_persist_id_ref, 1))
        return errs

        


class DummyType(PptType):
    

    def __init__(self, type_name, record_type, rec_ver=0, rec_instance=0,
                 rec_len=None):
        super(DummyType, self).__init__()
        self.type_name = type_name
        self.RECORD_TYPE = record_type
        self.RECORD_VERSION = rec_ver
        self.RECORD_INSTANCE = rec_instance
        self.record_length = rec_len

    def extract_from(self, stream):
        
        self.read_rec_head(stream)
        log.debug('skipping over {0} Byte for type {1}'
                  .format(self.rec_head.rec_len, self.type_name))
        log.debug('start at pos {0}'.format(stream.tell()))
        stream.seek(self.rec_head.rec_len, os.SEEK_CUR)
        log.debug('now at pos {0}'.format(stream.tell()))
        return self

    def check_validity(self):
        return self.check_rec_head(self.record_length)


class PersistDirectoryAtom(PptType):
    

    RECORD_TYPE = 0x1772

    def __init__(self):
        super(PersistDirectoryAtom, self).__init__()
        self.rg_persist_dir_entry = None    
        self.stream_offset = None

    @classmethod
    def extract_from(clz, stream):
        

        log.debug("Extracting a PersistDirectoryAtom from stream")
        obj = clz()

        
        obj.stream_offset = stream.tell()

        
        obj.read_rec_head(stream)

        
        curr_pos = stream.tell()
        stop_pos = curr_pos + obj.rec_head.rec_len
        log.debug('start reading at pos {0}, read until {1}'
                  .format(curr_pos, stop_pos))
        obj.rg_persist_dir_entry = []

        while curr_pos < stop_pos:
            new_entry = PersistDirectoryEntry.extract_from(stream)
            obj.rg_persist_dir_entry.append(new_entry)
            curr_pos = stream.tell()
            log.debug('at pos {0}'.format(curr_pos))
        return obj

    def check_validity(self, user_edit_last_offset=None):
        errs = self.check_rec_head()
        for entry in self.rg_persist_dir_entry:
            errs.extend(entry.check_validity(user_edit_last_offset,
                                             self.stream_offset))
        return errs


class PersistDirectoryEntry(object):
    

    def __init__(self):
        self.persist_id = None
        self.c_persist = None
        self.rg_persist_offset = None

    @classmethod
    def extract_from(clz, stream):
        
        log.debug("Extracting a PersistDirectoryEntry from stream")
        obj = clz()

        
        
        
        
        
        
        

        
        
        temp, = struct.unpack('<L', stream.read(4))
        obj.c_persist, obj.persist_id = divmod(temp, 2**20)
        log.debug('temp is 0x{0:04X} --> id is {1}, reading {2} offsets'
                  .format(temp, obj.persist_id, obj.c_persist))

        
        
        
        
        
        
        
        
        
        obj.rg_persist_offset = [struct.unpack('<L', stream.read(4))[0] \
                                 for _ in range(obj.c_persist)]
        log.debug('offsets are: {0}'.format(obj.rg_persist_offset))
        return obj

    def check_validity(self, user_edit_last_offset=None,
                       persist_obj_dir_offset=None):
        errs = []
        if self.persist_id > 0xFFFFE:  
            errs.append(PptUnexpectedData(
                MAIN_STREAM_NAME, 'PersistDirectoryEntry.persist_id',
                self.persist_id, '< 0xFFFFE (dec: {0})'.format(0xFFFFE)))
        if self.c_persist == 0:
            errs.append(PptUnexpectedData(
                MAIN_STREAM_NAME, 'PersistDirectoryEntry.c_persist',
                self.c_persist, '> 0'))
        if user_edit_last_offset is not None \
                and min(self.rg_persist_offset) < user_edit_last_offset:
            errs.append(PptUnexpectedData(
                MAIN_STREAM_NAME, 'PersistDirectoryEntry.rg_persist_offset',
                min(self.rg_persist_offset),
                '> UserEdit.offsetLastEdit ({0})'
                .format(user_edit_last_offset)))
        if persist_obj_dir_offset is not None \
                and max(self.rg_persist_offset) > persist_obj_dir_offset:
            errs.append(PptUnexpectedData(
                MAIN_STREAM_NAME, 'PersistDirectoryEntry.rg_persist_offset',
                max(self.rg_persist_offset),
                '> PersistObjectDirectory offset ({0})'
                .format(persist_obj_dir_offset)))
        return errs


class DocInfoListSubContainerOrAtom(PptType):
    

    
    VALID_RECORD_TYPES = [0x1388, 
                          0x0414, 
                          0x0413, 
                          0x0407, 
                          0x03FA, 
                          0x0408]  

    def __init__(self):
        super(DocInfoListSubContainerOrAtom, self).__init__()

    @classmethod
    def extract_from(clz, stream):
        

        log.debug('Parsing DocInfoListSubContainerOrAtom from stream')

        obj = clz()
        obj.read_rec_head(stream)
        if obj.rec_head.rec_type == VBAInfoContainer.RECORD_TYPE:
            obj = VBAInfoContainer.extract_from(stream, obj.rec_head)
        else:
            log.debug('skipping over {0} Byte in DocInfoListSubContainerOrAtom'
                      .format(obj.rec_head.rec_len))
            log.debug('start at pos {0}'.format(stream.tell()))
            stream.seek(obj.rec_head.rec_len, os.SEEK_CUR)
            log.debug('now at pos {0}'.format(stream.tell()))
        return obj

    def check_validity(self):
        
        self.check_value('rh.recType', self.rec_head.rec_type,
                         self.VALID_RECORD_TYPES)


class DocInfoListContainer(PptType):
    

    RECORD_VERSION = 0xF
    RECORD_TYPE = 0x07D0

    def __init__(self):
        super(DocInfoListContainer, self).__init__()
        self.rg_child_rec = None

    @classmethod
    def extract_from(clz, stream):
        

        log.debug('Parsing DocInfoListContainer from stream')
        obj = clz()
        obj.read_rec_head(stream)

        
        
        
        
        curr_pos = stream.tell()
        end_pos = curr_pos + obj.rec_head.rec_len
        log.debug('start reading at pos {0}, will read until {1}'
                  .format(curr_pos, end_pos))
        obj.rg_child_rec = []

        while curr_pos < end_pos:
            new_obj = DocInfoListSubContainerOrAtom().extract_from(stream)
            obj.rg_child_rec.append(new_obj)
            curr_pos = stream.tell()
            log.debug('now at pos {0}'.format(curr_pos))

        log.debug('reached end pos {0} ({1}). stop reading DocInfoListContainer'
                  .format(end_pos, curr_pos))

    def check_validity(self):
        errs = self.check_rec_head()
        for obj in self.rg_child_rec:
            errs.extend(obj.check_validity())
        return errs


class DocumentContainer(PptType):
    

    RECORD_TYPE = 0x03E8

    def __init__(self):
        super(DocumentContainer, self).__init__()
        self.document_atom = None
        self.ex_obj_list = None
        self.document_text_info = None
        self.sound_collection = None
        self.drawing_group = None
        self.master_list = None
        self.doc_info_list = None
        self.slide_hf = None
        self.notes_hf = None
        self.slide_list = None
        self.notes_list = None
        self.slide_show_doc_info = None
        self.named_shows = None
        self.summary = None
        self.doc_routing_slip = None
        self.print_options = None
        self.rt_custom_table_styles_1 = None
        self.end_document = None
        self.rt_custom_table_styles_2 = None

    @classmethod
    def extract_from(clz, stream):
        

        log.debug('Parsing DocumentContainer from stream')
        obj = clz()

        
        obj.read_rec_head(stream)
        log.info('validity: {0} errs'.format(len(obj.check_rec_head())))

        
        
        obj.document_atom = DummyType('DocumentAtom', 0x03E9, rec_ver=0x1,
                                      rec_len=0x28).extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.document_atom.check_validity())))

        
        
        obj.ex_obj_list = DummyType('ExObjListContainer', 0x0409, rec_ver=0xF)\
                          .extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.ex_obj_list.check_validity())))

        
        
        
        obj.document_text_info = DummyType('DocumentTextInfoContainer', 0x03F2,
                                           rec_ver=0xF).extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.document_text_info.check_validity())))

        
        
        
        obj.sound_collection = DummyType('SoundCollectionContainer', 0x07E4,
                                         rec_ver=0xF, rec_instance=0x005)\
                               .extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.sound_collection.check_validity())))

        
        
        obj.drawing_group = DummyType('DrawingGroupContainer', 0x040B,
                                      rec_ver=0xF).extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.drawing_group.check_validity())))

        
        
        
        obj.master_list = DummyType('MasterListWithContainer', 0x0FF0,
                                    rec_ver=0xF).extract_from(stream)
        log.info('validity: {0} errs'
                 .format(len(obj.master_list.check_validity())))

        
        
        
        obj.doc_info_list = DocInfoListContainer.extract_from(stream)

        
        
        
        

        
        
        
        

        
        
        

        
        
        

        
        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        
        

        
        
        

        
        
        
        
        

        return obj


    def check_validity(self):
        
        errs = self.check_rec_head()
        errs.extend(self.document_atom.check_validity())
        errs.extend(self.ex_obj_list.check_validity())
        errs.extend(self.document_text_info.check_validity())
        errs.extend(self.sound_collection.check_validity())
        errs.extend(self.drawing_group.check_validity())
        errs.extend(self.master_list.check_validity())
        errs.extend(self.doc_info_list.check_validity())
        return errs


class VBAInfoContainer(PptType):
    

    RECORD_TYPE = 0x03FF
    RECORD_VERSION = 0xF
    RECORD_INSTANCE = 0x001
    RECORD_LENGTH = 0x14

    def __init__(self):
        super(VBAInfoContainer, self).__init__()
        self.vba_info_atom = None

    @classmethod
    def extract_from(clz, stream, rec_head=None):
        
        log.debug('parsing VBAInfoContainer')
        obj = clz()
        if rec_head is None:
            obj.read_rec_head(stream)
        else:
            log.debug('skip parsing of RecordHeader')
            obj.rec_head = rec_head
        obj.vba_info_atom = VBAInfoAtom.extract_from(stream)
        return obj

    def check_validity(self):
        errs = self.check_rec_head(length=self.RECORD_LENGTH)
        errs.extend(self.vba_info_atom.check_validity())
        return errs


class VBAInfoAtom(PptType):
    

    RECORD_TYPE = 0x0400
    RECORD_VERSION = 0x2
    RECORD_LENGTH = 0x0C

    def __init__(self):
        super(VBAInfoAtom, self).__init__()
        self.persist_id_ref = None
        self.f_has_macros = None
        self.version = None

    @classmethod
    def extract_from(clz, stream):
        log.debug('parsing VBAInfoAtom')
        obj = clz()
        obj.read_rec_head(stream)

        
        
        
        obj.persist_id_ref = read_4(stream)

        
        
        
        obj.f_has_macros = read_4(stream)

        
        
        
        obj.version = read_4(stream)

        return obj

    def check_validity(self):

        errs = self.check_rec_head(length=self.RECORD_LENGTH)

        
        errs.extend(self.check_range('fHasMacros', self.f_has_macros, None, 2))
        errs.extend(self.check_value('version', self.version, 2))
        return errs


class ExternalObjectStorage(PptType):
    

    RECORD_TYPE = 0x1011
    RECORD_INSTANCE_COMPRESSED = 1
    RECORD_INSTANCE_UNCOMPRESSED = 0

    def __init__(self, is_compressed=None):
        super(ExternalObjectStorage, self).__init__()
        if is_compressed is None:
            self.RECORD_INSTANCE = None   
        elif is_compressed:
            self.RECORD_INSTANCE = self.RECORD_INSTANCE_COMPRESSED
            self.is_compressed = True
        else:
            self.RECORD_INSTANCE = self.RECORD_INSTANCE_UNCOMPRESSED
            self.is_compressed = False
        self.uncompressed_size = None
        self.data_offset = None
        self.data_size = None

    def extract_from(self, stream):
        
        log.debug('Parsing ExternalObjectStorage (compressed={0}) from stream'
                  .format(self.is_compressed))
        self.read_rec_head(stream)
        self.data_size = self.rec_head.rec_len
        if self.is_compressed:
            self.uncompressed_size = read_4(stream)
            self.data_size -= 4
        self.data_offset = stream.tell()

    def check_validity(self):
        return self.check_rec_head()


class ExternalObjectStorageUncompressed(ExternalObjectStorage):
    
    RECORD_INSTANCE = ExternalObjectStorage.RECORD_INSTANCE_UNCOMPRESSED

    def __init__(self):
        super(ExternalObjectStorageUncompressed, self).__init__(False)

    @classmethod
    def extract_from(clz, stream):
        
        obj = clz()
        super(ExternalObjectStorageUncompressed, obj).extract_from(stream)
        return obj


class ExternalObjectStorageCompressed(ExternalObjectStorage):
    
    RECORD_INSTANCE = ExternalObjectStorage.RECORD_INSTANCE_COMPRESSED

    def __init__(self):
        super(ExternalObjectStorageCompressed, self).__init__(True)

    @classmethod
    def extract_from(clz, stream):
        
        obj = clz()
        super(ExternalObjectStorageCompressed, obj).extract_from(stream)
        return obj




def with_opened_main_stream(func):
    

    def wrapped(self, *args, **kwargs):
        
        stream_opened_by_me = False
        try:
            
            if self._open_main_stream is None:
                log.debug('opening stream {0!r} for {1}'
                          .format(MAIN_STREAM_NAME, func.__name__))
                self._open_main_stream = self.ole.openstream(MAIN_STREAM_NAME)
                stream_opened_by_me = True

            
            return func(self, self._open_main_stream, *args, **kwargs)

        
        except Exception:
            if self.fast_fail:
                raise
            else:
                self._log_exception()
        finally:
            
            if stream_opened_by_me:
                log.debug('closing stream {0!r} after {1}'
                          .format(MAIN_STREAM_NAME, func.__name__))
                self._open_main_stream.close()
                self._open_main_stream = None
    return wrapped


def generator_with_opened_main_stream(func):
    

    def wrapped(self, *args, **kwargs):
        
        stream_opened_by_me = False
        try:
            
            if self._open_main_stream is None:
                log.debug('opening stream {0!r} for {1}'
                          .format(MAIN_STREAM_NAME, func.__name__))
                self._open_main_stream = self.ole.openstream(MAIN_STREAM_NAME)
                stream_opened_by_me = True

            
            for result in func(self, self._open_main_stream, *args, **kwargs):
                yield result

        
        except Exception:
            if self.fast_fail:
                raise
            else:
                self._log_exception()
        finally:
            
            if stream_opened_by_me:
                log.debug('closing stream {0!r} after {1}'
                          .format(MAIN_STREAM_NAME, func.__name__))
                self._open_main_stream.close()
                self._open_main_stream = None
    return wrapped


class PptParser(object):
    

    def __init__(self, ole, fast_fail=False):
        
        if isinstance(ole, olefile.OleFileIO):
            log.debug('using open OleFileIO')
            self.ole = ole
        else:
            log.debug('Opening file {0}'.format(ole))
            self.ole = olefile.OleFileIO(ole)

        self.fast_fail = fast_fail

        self.current_user_atom = None
        self.newest_user_edit = None
        self.document_persist_obj = None
        self.persist_object_directory = None

        
        
        
        
        
        root_streams = self.ole.listdir()
        
        
        if any(len(stream) != 1 for stream in root_streams):
            self._fail('root', 'listdir', root_streams, 'len = 1')
        root_streams = [stream[0].lower() for stream in root_streams]
        if not 'current user' in root_streams:
            self._fail('root', 'listdir', root_streams, 'Current User')
        if not MAIN_STREAM_NAME.lower() in root_streams:
            self._fail('root', 'listdir', root_streams, MAIN_STREAM_NAME)

        self._open_main_stream = None

    def _log_exception(self, msg=None):
        
        if msg is not None:
            stack = traceback.extract_stack()[:-1]
        else:
            _, exc, trace = sys.exc_info()
            stack = traceback.extract_tb(trace)
            msg = str(exc)
        log.error(msg)

        for i_entry, entry in enumerate(traceback.format_list(stack)):
            for line in entry.splitlines():
                log.debug('trace {0}: {1}'.format(i_entry, line))

    def _fail(self, *args):
        
        if self.fast_fail:
            raise PptUnexpectedData(*args)
        else:
            self._log_exception(PptUnexpectedData(*args).msg)

    def parse_current_user(self):
        

        if self.current_user_atom is not None:
            log.warning('re-reading and overwriting '
                        'previously read current_user_atom')

        log.debug('parsing "Current User"')

        stream = None
        try:
            log.debug('opening stream "Current User"')
            stream = self.ole.openstream('Current User')
            self.current_user_atom = CurrentUserAtom.extract_from(stream)
        except Exception:
            if self.fast_fail:
                raise
            else:
                self._log_exception()
        finally:
            if stream is not None:
                log.debug('closing stream "Current User"')
                stream.close()

    @with_opened_main_stream
    def parse_persist_object_directory(self, stream):
        

        if self.persist_object_directory is not None:
            log.warning('re-reading and overwriting '
                        'previously read persist_object_directory')

        
        
        
        if self.current_user_atom is None:
            self.parse_current_user()

        offset = self.current_user_atom.offset_to_current_edit
        is_encrypted = self.current_user_atom.is_encrypted()
        self.persist_object_directory = {}
        self.newest_user_edit = None

        
        while offset != 0:

            
            
            
            stream.seek(offset, os.SEEK_SET)

            
            
            user_edit = UserEditAtom.extract_from(stream, is_encrypted)
            if self.newest_user_edit is None:
                self.newest_user_edit = user_edit

            log.debug('checking validity')
            errs = user_edit.check_validity()
            if errs:
                log.warning('check_validity found {0} issues'
                            .format(len(errs)))
            for err in errs:
                log.warning('UserEditAtom.check_validity: {0}'.format(err))
            if errs and self.fast_fail:
                raise errs[0]

            
            
            
            log.debug('seeking to pos {0}'
                      .format(user_edit.offset_persist_directory))
            stream.seek(user_edit.offset_persist_directory, os.SEEK_SET)

            
            
            persist_dir_atom = PersistDirectoryAtom.extract_from(stream)

            log.debug('checking validity')
            errs = persist_dir_atom.check_validity(offset)
            if errs:
                log.warning('check_validity found {0} issues'
                            .format(len(errs)))
            for err in errs:
                log.warning('PersistDirectoryAtom.check_validity: {0}'
                            .format(err))
            if errs and self.fast_fail:
                raise errs[0]


            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            for entry in persist_dir_atom.rg_persist_dir_entry:
                last_id = entry.persist_id+len(entry.rg_persist_offset)-1
                log.debug('for persist IDs {0}-{1}, save offsets {2}'
                          .format(entry.persist_id, last_id,
                                  entry.rg_persist_offset))
                for count, offset in enumerate(entry.rg_persist_offset):
                    self.persist_object_directory[entry.persist_id+count] \
                        = offset

            
            
            
            offset = user_edit.offset_last_edit

    @with_opened_main_stream
    def parse_document_persist_object(self, stream):
        
        if self.document_persist_obj is not None:
            log.warning('re-reading and overwriting '
                        'previously read document_persist_object')

        
        
        
        if self.persist_object_directory is None:
            self.parse_persist_object_directory()    

        
        
        
        newest_ref = self.newest_user_edit.doc_persist_id_ref
        offset = self.persist_object_directory[newest_ref]
        log.debug('newest user edit ID is {0}, offset is {1}'
                  .format(newest_ref, offset))

        
        log.debug('seek to {0}'.format(offset))
        stream.seek(offset, os.SEEK_SET)

        
        
        self.document_persist_obj = DocumentContainer.extract_from(stream)

        log.debug('checking validity')
        errs = self.document_persist_obj.check_validity()
        if errs:
            log.warning('check_validity found {0} issues'.format(len(errs)))
        for err in errs:
            log.warning('check_validity(document_persist_obj): {0}'
                        .format(err))
        if errs and self.fast_fail:
            raise errs[0]

    
    
    
    

    @generator_with_opened_main_stream
    def search_pattern(self, stream, pattern):
        

        BUF_SIZE = 1024

        pattern_len = len(pattern)
        log.debug('pattern length is {0}'.format(pattern_len))
        if pattern_len > BUF_SIZE:
            raise ValueError('need buf > pattern to search!')

        n_reads = 0
        while True:
            start_pos = stream.tell()
            n_reads += 1
            
            
            buf = stream.read(BUF_SIZE)
            idx = buf.find(pattern)
            while idx != -1:
                log.debug('found pattern at index {0}'.format(start_pos+idx))
                yield start_pos + idx
                idx = buf.find(pattern, idx+1)

            if len(buf) == BUF_SIZE:
                
                stream.seek(start_pos + BUF_SIZE - pattern_len, os.SEEK_SET)
            else:
                log.debug('reached end of buf (read {0}<{1}) after {2} reads'
                          .format(len(buf), BUF_SIZE, n_reads))
                break

    @generator_with_opened_main_stream
    def search_vba_info(self, stream):
        

        log.debug('looking for VBA info containers')

        pattern = VBAInfoContainer.generate_pattern(
                                rec_len=VBAInfoContainer.RECORD_LENGTH) \
                + VBAInfoAtom.generate_pattern(
                                rec_len=VBAInfoAtom.RECORD_LENGTH)

        
        for idx in self.search_pattern(pattern):    
            
            stream.seek(idx)
            log.debug('extracting at idx {0}'.format(idx))
            try:
                container = VBAInfoContainer.extract_from(stream)
            except Exception:
                self._log_exception()
                continue

            errs = container.check_validity()
            if errs:
                log.warning('check_validity found {0} issues'
                            .format(len(errs)))
            else:
                log.debug('container is ok')
                atom = container.vba_info_atom
                log.debug('persist id ref is {0}, has_macros {1}, version {2}'
                          .format(atom.persist_id_ref, atom.f_has_macros,
                                  atom.version))
                yield container
            for err in errs:
                log.warning('check_validity(VBAInfoContainer): {0}'
                            .format(err))
            if errs and self.fast_fail:
                raise errs[0]

    @generator_with_opened_main_stream
    def search_vba_storage(self, stream):
        

        log.debug('looking for VBA storage objects')
        for obj_type in (ExternalObjectStorageUncompressed,
                         ExternalObjectStorageCompressed):
            
            stream.seek(0, os.SEEK_SET)

            pattern = obj_type.generate_pattern()

            
            for idx in self.search_pattern(pattern):    
                
                stream.seek(idx)
                log.debug('extracting at idx {0}'.format(idx))
                try:
                    storage = obj_type.extract_from(stream)
                except Exception:
                    self._log_exception()
                    continue

                errs = storage.check_validity()
                if errs:
                    log.warning('check_validity found {0} issues'
                                .format(len(errs)))
                else:
                    log.debug('storage is ok; compressed={0}, size={1}, '
                              'size_decomp={2}'
                              .format(storage.is_compressed,
                                      storage.rec_head.rec_len,
                                      storage.uncompressed_size))
                    yield storage
                for err in errs:
                    log.warning('check_validity({0}): {1}'
                                .format(obj_type.__name__, err))
                if errs and self.fast_fail:
                    raise errs[0]

    @with_opened_main_stream
    def decompress_vba_storage(self, stream, storage):
        

        log.debug('decompressing storage for VBA OLE data stream ')

        
        
        stream.seek(storage.data_offset, os.SEEK_SET)
        decomp, n_read, err = \
            iterative_decompress(stream, storage.data_size)
        log.debug('decompressed {0} to {1} bytes; found err: {2}'
                  .format(n_read, len(decomp), err))
        if err and self.fast_fail:
            raise err
        

        return decomp

        
        
        
        
        
        
        
        
        
        

    @with_opened_main_stream
    def read_vba_storage_data(self, stream, storage):
        

        log.debug('reading uncompressed VBA OLE data stream: '
                  '{0} bytes starting at {1}'
                  .format(storage.data_size, storage.data_offset))
        stream.seek(storage.data_offset, os.SEEK_SET)
        data = stream.read(storage.data_size)
        return data

    @generator_with_opened_main_stream
    def iter_vba_data(self, stream):
        

        n_infos = 0
        n_macros = 0
        for info in self.search_vba_info():    
            n_infos += 1
            if info.vba_info_atom.f_has_macros > 0:
                n_macros += 1
        
        
        n_storages = 0
        n_compressed = 0
        for storage in self.search_vba_storage():    
            n_storages += 1
            if storage.is_compressed:
                n_compressed += 1
                yield self.decompress_vba_storage(storage)    
            else:
                yield self.read_vba_storage_data(storage)    

        log.info('found {0} infos ({1} with macros) and {2} storages '
                 '({3} compressed)'
                 .format(n_infos, n_macros, n_storages, n_compressed))


def iterative_decompress(stream, size, chunk_size=4096):
    

    decompressor = zlib.decompressobj()
    n_read = 0
    decomp = b''
    return_err = None

    try:
        while n_read < size:
            n_new = min(size-n_read, chunk_size)
            decomp += decompressor.decompress(stream.read(n_new))
            n_read += n_new
    except zlib.error as err:
        return_err = err

    return decomp, n_read, return_err


if __name__ == '__main__':
    print('nothing here to run!')
